%floatsintext can be used in man to have figs appear where they are called
\documentclass[man, noextraspace, apacite, floatsintext]{apa6}
%following looks like a journal printout; need to comment out previous line
%\documentclass[jou,noextraspace,apacite]{apa6}
\usepackage{apacite}
\usepackage{setspace}
\usepackage{listings}
\usepackage{amsmath}
\usepackage[retainorgcmds]{IEEEtrantools}
\lstset{
basicstyle=\small\ttfamily,
columns=flexible,
breaklines=true
}

\title{Use Welch's \textit{t} test to Compare the Means of Independent Groups}
\shorttitle{Welch t Test}

\twoauthors{Joshua D. Wondra}{Richard Gonzalez}
\twoaffiliations{Facebook}{University of Michigan}

\abstract{Researchers typically use Student's t test or ANOVA to test whether the means of independent groups are different from each other.  
These tests assume that the population variances of the groups are equal. If there is 
reason to believe the variances are unequal, then researchers can 
use Welch's t test, which does not assume equal variances, as an alternative. 
We were interested in finding a simple rule to 
decide when to use Student's vs. Welch's t test. We used Monte Carlo 
simulations to compare the false positive rate, power, and coverage probability 
of Student's and Welch's t tests across different variance ratios, sample size ratios, sample sizes, 
and effect sizes. We examined a simple case---the difference in the means of 
two groups---and a more complex case---the test of the interaction in a 
2~$\times$~2 factorial design. We recommend the following rule: 
If the data are normally distributed, always use Welch's t test to 
compare the means of independent groups.}
\keywords{Welch t test, contrasts, hypothesis testing, simulation}

\authornote{Joshua D. Wondra, Facebook.

Richard Gonzalez, Department of Psychology, University of Michigan.

This research was conducted while Josh Wondra was a graduate student at the University of Michigan.

Correspondence concerning this article should be addressed to Josh Wondra, 
Facebook, 770 Broadway, New York, NY, 10003.

Contact: jdwondra@umich.edu}


\begin{document}
\SweaveOpts{concordance=TRUE}
\maketitle
<<setup, echo=FALSE>>=

# Load packages ----
library(formattable)
library(tidyverse)
library(cowplot)


# Import simulation results ----
load('no_effects.R')
load('crossover_sims.R')
load('me_int_sims.R')


# Set ggplot theme ----
theme_set(theme_bw())

@


<<create_plots, echo = FALSE>>=
# Create plots ----

# True effects ====
tf_data <- data.frame(
  effect = factor(rep(c('No effects', 'Crossover interaction', 'Main effects & interaction'), each = 4), levels = c('No effects', 'Crossover interaction', 'Main effects & interaction')),
  y = c(
    6, 6, 6, 6, # no effect
    6.71, 6, 6, 6.71, # crossover
    6.71, 6, 6.28, 6 # main effect + interaction
    ),
  factorA = rep(c('A1', 'A2', 'A1', 'A2'), times = 3),
  factorB = rep(c('B1', 'B1', 'B2', 'B2'), times = 3)
) %>% 
  mutate(
    factorB_label_position = case_when(
      factorA == 'A2' ~ as.numeric(NA),
      factorB == 'B1' ~ y + .1,
      factorB == 'B2' ~ y - .1
      )
  )

tf_plot <- function(data, which_effect) {

  data %>% 
    filter(effect == which_effect) %>% 
    ggplot(aes(y = y, x = factorA, group = factorB, linetype = factorB)) +
    geom_line(show.legend = FALSE) +
    geom_text(
      data = data %>% filter(effect == which_effect, !is.na(factorB_label_position)),
      aes(y = factorB_label_position, label = factorB)
    ) +
    labs(x = 'Factor A', y = NULL) +
    facet_wrap(~ effect) +
    theme_classic() +
    theme(
      panel.grid = element_blank()
    ) +
    coord_cartesian(ylim = c(5.6, 7.4))
}

tf_no_effects <- tf_plot(tf_data, 'No effects')
tf_crossover <- tf_plot(tf_data, 'Crossover interaction')
tf_me_int <- tf_plot(tf_data, 'Main effects & interaction')



# Reject Null - False Positives ====
# false positives (per test)
rn_plot <- function(data, sim, which_contrast, which_test) {
  
  ylims <- if (
    sim == 'No effects' | 
    sim == 'Crossover interaction' & str_detect(which_contrast, 'Main') |
    sim == 'Main effects & interaction' & which_contrast == 'Simple Effect of B at A2'
    ) {
    c(0, .2)
    } else if (
      sim == 'Crossover interaction' & str_detect(which_contrast, 'Int') |
      sim == 'Main effects & interaction'
      ) {
      c(0, 1)
    } else {
      c(0, .5)
    }
  
  ybreaks <- if (
    sim == 'No effects' | 
    sim == 'Crossover interaction' & str_detect(which_contrast, 'Main') |
    sim == 'Main effects & interaction' & which_contrast == 'Simple Effect of B at A2'
    ) {
    seq(0, 1, .05)
    } else if (
      sim == 'Crossover interaction' & str_detect(which_contrast, 'Int') |
      sim == 'Main effects & interaction'
      ) {
      seq(0, 1, .1)
    } else {
      seq(0, 1, .1)
    }
  
  ylabs <- paste0(ybreaks * 100, '%')
  
  data %>% 
    filter(
      which_sim == sim,
      contrast_names == which_contrast,
      test == which_test
      ) %>% 
    ggplot(aes(y = reject_rate, x = var_ratio, group = ns, linetype = ns, shape = ns)) +
    geom_line(size = .5) +
    geom_point(size = 3) + 
    facet_grid(test ~ contrast_names) +
    scale_x_discrete(labels = factor(c('1/5', '1/2', '1', '2', '5'))) +
    #scale_y_continuous(breaks = c(0, .05, .10), minor_breaks = seq(0, .13, .01), limits = c(0, .14)) +
    # labs(
    #   x = bquote(frac(sigma[A[1]*B[1]]^2,{sigma[A[1]*B[2]]^2 == sigma[A[2]*B[1]]^2} == sigma[A[2]*B[2]]^2)),
    #   y = 'False Positive Rate'
    # ) +
    scale_y_continuous(limits = ylims, breaks = ybreaks, labels = ylabs) +
    labs(y = NULL, x = NULL) +
    theme_classic() +
    theme(
      panel.grid = element_blank(),
      panel.grid.major.y = element_line(color = 'gray80')
    ) + 
    scale_shape_manual(name = 'Sample sizes \nGroup A[1]*B[1], A[1]*B[2], A[2]*B[1], A[2]*B[2]',
                     labels = c(
                       '30, 30, 30, 30',
                       '30, 45, 45, 45',
                       '30, 60, 60, 60'
                     ),
                     values = rep(c(15, 16, 17), times = 3)
  ) +
    scale_linetype_manual(name = 'Sample sizes \nGroup A[1]*B[1], A[1]*B[2], A[2]*B[1], A[2]*B[2]',
                        labels = c(
                          '30, 30, 30, 30',
                          '30, 45, 45, 45',
                          '30, 60, 60, 60'
                        ),
                        values = c('solid', 'twodash', 'dotted')
  ) +
    guides(linetype = FALSE, shape = FALSE)
} 

# subset data
rn_data <- mutate(no_effects, which_sim = 'No effects') %>% 
  bind_rows(mutate(crossover_sims, which_sim = 'Crossover interaction')) %>% 
  bind_rows(mutate(me_int_sims, which_sim = 'Main effects & interaction')) %>% 
  ungroup() %>% 
  select(-coverage_student, -coverage_welch, -joint_reject_student, -joint_reject_welch) %>% 
  gather(key = test, value = reject_rate, reject_student, reject_welch) %>% 
  mutate(
    test = case_when(
      test == 'reject_student' ~ "ANOVA",
      test == 'reject_welch' ~ "Welch's t test"
    ),
    contrast_names = case_when(
      contrast_names == 'ME (1 & 2 vs 3 & 4)' ~ 'Main Effect of A',
      contrast_names == 'ME (1 & 3 vs 2 & 4)' ~ 'Main Effect of B',
      contrast_names == 'Interaction' ~ 'Interaction',
      contrast_names == 'SE (1 vs 2)' ~ 'Simple Effect of A at B1',
      contrast_names == 'SE (3 vs 4)' ~ 'Simple Effect of A at B2',
      contrast_names == 'SE (1 vs 3)' ~ 'Simple Effect of B at A1',
      contrast_names == 'SE (2 vs 4)' ~ 'Simple Effect of B at A2'
    ) %>% 
      factor(levels = c('Main Effect of A', 'Main Effect of B', 'Interaction', 'Simple Effect of B at A1', 'Simple Effect of B at A2', 'Simple Effect of A at B1', 'Simple Effect of A at B2')),
    ns = paste(min_sample, min_sample * sample_ratio, min_sample * sample_ratio, min_sample * sample_ratio, sep = ', ') %>% factor(levels = c('30, 30, 30, 30', '30, 45, 45, 45', '30, 60, 60, 60')),
    var_ratio = factor(var_ratio),
    sample_ratio = factor(sample_ratio)
  ) %>% 
  filter(
    min_sample == 30
  )

# get min/max reject rates
min_max_rates <- rn_data %>% 
  group_by(contrast_names, test) %>% 
  summarize(
    min_reject = min(reject_rate),
    max_reject = max(reject_rate)
  )

# Blank plot
blank_plot <- rn_data %>% 
  filter(
    which_sim == 'No effects',
    contrast_names == 'Main Effect of A',
    test == 'ANOVA'
  ) %>% 
  ggplot(aes(y = reject_rate, x = var_ratio)) +
  geom_blank() +
  scale_y_continuous (limits = c(0, .15)) +
  labs(x = NULL, y = NULL) +
  theme(
    rect = element_blank(),
    line = element_blank(),
    axis.text.x = element_blank(),
    axis.text.y = element_blank()
  )


# Main effect of A
## ANOVA
rn_no_effects_meA_anova <- rn_plot(rn_data, sim = 'No effects', which_contrast = 'Main Effect of A', which_test = 'ANOVA')
rn_crossover_meA_anova <- rn_plot(rn_data, sim = 'Crossover interaction', which_contrast = 'Main Effect of A', which_test = 'ANOVA')
rn_me_int_meA_anova <- rn_plot(rn_data, sim = 'Main effects & interaction', which_contrast = 'Main Effect of A', which_test = 'ANOVA')
## Welch
rn_no_effects_meA_welch <- rn_plot(rn_data, sim = 'No effects', which_contrast = 'Main Effect of A', which_test = "Welch's t test")
rn_crossover_meA_welch <- rn_plot(rn_data, sim = 'Crossover interaction', which_contrast = 'Main Effect of A', which_test = "Welch's t test")
rn_me_int_meA_welch <- rn_plot(rn_data, sim = 'Main effects & interaction', which_contrast = 'Main Effect of A', which_test = "Welch's t test")

# Main effect of B
## ANOVA
rn_no_effects_meB_anova <- rn_plot(rn_data, sim = 'No effects', which_contrast = 'Main Effect of B', which_test = 'ANOVA')
rn_crossover_meB_anova <- rn_plot(rn_data, sim = 'Crossover interaction', which_contrast = 'Main Effect of B', which_test = 'ANOVA')
rn_me_int_meB_anova <- rn_plot(rn_data, sim = 'Main effects & interaction', which_contrast = 'Main Effect of B', which_test = 'ANOVA')
## Welch
rn_no_effects_meB_welch <- rn_plot(rn_data, sim = 'No effects', which_contrast = 'Main Effect of B', which_test = "Welch's t test")
rn_crossover_meB_welch <- rn_plot(rn_data, sim = 'Crossover interaction', which_contrast = 'Main Effect of B', which_test = "Welch's t test")
rn_me_int_meB_welch <- rn_plot(rn_data, sim = 'Main effects & interaction', which_contrast = 'Main Effect of B', which_test = "Welch's t test")


# Interaction
## ANOVA
rn_no_effects_int_anova <- rn_plot(rn_data, sim = 'No effects', which_contrast = 'Interaction', which_test = 'ANOVA')
rn_crossover_int_anova <- rn_plot(rn_data, sim = 'Crossover interaction', which_contrast = 'Interaction', which_test = 'ANOVA')
rn_me_int_int_anova <- rn_plot(rn_data, sim = 'Main effects & interaction', which_contrast = 'Interaction', which_test = 'ANOVA')
## Welch
rn_no_effects_int_welch <- rn_plot(rn_data, sim = 'No effects', which_contrast = 'Interaction', which_test = "Welch's t test")
rn_crossover_int_welch <- rn_plot(rn_data, sim = 'Crossover interaction', which_contrast = 'Interaction', which_test = "Welch's t test")
rn_me_int_int_welch <- rn_plot(rn_data, sim = 'Main effects & interaction', which_contrast = 'Interaction', which_test = "Welch's t test")


# Simple Effects
## ANOVA

### no effects
rn_no_effects_A1cont_anova <- rn_plot(rn_data, sim = 'No effects', which_contrast = 'Simple Effect of B at A1', which_test = "ANOVA")
rn_no_effects_A2cont_anova <- rn_plot(rn_data, sim = 'No effects', which_contrast = 'Simple Effect of B at A2', which_test = "ANOVA")
rn_no_effects_B1cont_anova <- rn_plot(rn_data, sim = 'No effects', which_contrast = 'Simple Effect of A at B1', which_test = "ANOVA")
rn_no_effects_B2cont_anova <- rn_plot(rn_data, sim = 'No effects', which_contrast = 'Simple Effect of A at B2', which_test = "ANOVA")

### crossover
rn_crossover_A1cont_anova <- rn_plot(rn_data, sim = 'Crossover interaction', which_contrast = 'Simple Effect of B at A1', which_test = "ANOVA")
rn_crossover_A2cont_anova <- rn_plot(rn_data, sim = 'Crossover interaction', which_contrast = 'Simple Effect of B at A2', which_test = "ANOVA")
rn_crossover_B1cont_anova <- rn_plot(rn_data, sim = 'Crossover interaction', which_contrast = 'Simple Effect of A at B1', which_test = "ANOVA")
rn_crossover_B2cont_anova <- rn_plot(rn_data, sim = 'Crossover interaction', which_contrast = 'Simple Effect of A at B2', which_test = "ANOVA")

### no effects
rn_me_int_A1cont_anova <- rn_plot(rn_data, sim = 'Main effects & interaction', which_contrast = 'Simple Effect of B at A1', which_test = "ANOVA")
rn_me_int_A2cont_anova <- rn_plot(rn_data, sim = 'Main effects & interaction', which_contrast = 'Simple Effect of B at A2', which_test = "ANOVA")
rn_me_int_B1cont_anova <- rn_plot(rn_data, sim = 'Main effects & interaction', which_contrast = 'Simple Effect of A at B1', which_test = "ANOVA")
rn_me_int_B2cont_anova <- rn_plot(rn_data, sim = 'Main effects & interaction', which_contrast = 'Simple Effect of A at B2', which_test = "ANOVA")

## Welch

### no effects
rn_no_effects_A1cont_welch <- rn_plot(rn_data, sim = 'No effects', which_contrast = 'Simple Effect of B at A1', which_test = "Welch's t test")
rn_no_effects_A2cont_welch <- rn_plot(rn_data, sim = 'No effects', which_contrast = 'Simple Effect of B at A2', which_test = "Welch's t test")
rn_no_effects_B1cont_welch <- rn_plot(rn_data, sim = 'No effects', which_contrast = 'Simple Effect of A at B1', which_test = "Welch's t test")
rn_no_effects_B2cont_welch <- rn_plot(rn_data, sim = 'No effects', which_contrast = 'Simple Effect of A at B2', which_test = "Welch's t test")

### crossover
rn_crossover_A1cont_welch <- rn_plot(rn_data, sim = 'Crossover interaction', which_contrast = 'Simple Effect of B at A1', which_test = "Welch's t test")
rn_crossover_A2cont_welch <- rn_plot(rn_data, sim = 'Crossover interaction', which_contrast = 'Simple Effect of B at A2', which_test = "Welch's t test")
rn_crossover_B1cont_welch <- rn_plot(rn_data, sim = 'Crossover interaction', which_contrast = 'Simple Effect of A at B1', which_test = "Welch's t test")
rn_crossover_B2cont_welch <- rn_plot(rn_data, sim = 'Crossover interaction', which_contrast = 'Simple Effect of A at B2', which_test = "Welch's t test")

### no effects
rn_me_int_A1cont_welch <- rn_plot(rn_data, sim = 'Main effects & interaction', which_contrast = 'Simple Effect of B at A1', which_test = "Welch's t test")
rn_me_int_A2cont_welch <- rn_plot(rn_data, sim = 'Main effects & interaction', which_contrast = 'Simple Effect of B at A2', which_test = "Welch's t test")
rn_me_int_B1cont_welch <- rn_plot(rn_data, sim = 'Main effects & interaction', which_contrast = 'Simple Effect of A at B1', which_test = "Welch's t test")
rn_me_int_B2cont_welch <- rn_plot(rn_data, sim = 'Main effects & interaction', which_contrast = 'Simple Effect of A at B2', which_test = "Welch's t test")

plot_legend <- (rn_no_effects_meA_anova + guides(shape = 'legend', linetype = 'legend')) %>% 
  get_legend()



# Reject Null - Joint False Positives ----
joint_reject_data <- no_effects %>% 
  ungroup() %>% 
  filter(min_sample == 30) %>% 
  distinct(sample_ratio, min_sample, effect, var_ratio, joint_reject_student, joint_reject_welch) %>% 
  mutate(
    ns = paste(min_sample, min_sample * sample_ratio, min_sample * sample_ratio, min_sample * sample_ratio, sep = ', ') %>% factor(levels = c('30, 30, 30, 30', '30, 45, 45, 45', '30, 60, 60, 60')),
    var_ratio = factor(var_ratio),
    sample_ratio = factor(sample_ratio)
  )

joint_reject_student <- joint_reject_data %>% 
  mutate(test = 'ANOVA') %>% 
  ggplot(aes(y = joint_reject_student, x = var_ratio, group = ns, linetype = ns, shape = ns)) +
    geom_line(size = .5) +
    geom_point(size = 3) + 
    scale_x_discrete(labels = factor(c('1/5', '1/2', '1', '2', '5'))) +
    scale_y_continuous(limits = c(0, .30), breaks = seq(0, .30, .05), labels = seq(0, 30, 5) %>% paste0('%')) +
    labs(y = NULL, x = NULL) +
    theme_classic() +
    theme(
      panel.grid = element_blank(),
      panel.grid.major.y = element_line(color = 'gray80')
    ) + 
    scale_shape_manual(name = 'Sample sizes \nGroup A[1]*B[1], A[1]*B[2], A[2]*B[1], A[2]*B[2]',
                     labels = c(
                       '30, 30, 30, 30', 
                       '30, 45, 45, 45', 
                       '30, 60, 60, 60'
                     ),
                     values = rep(c(15, 16, 17), times = 3)
  ) +
    scale_linetype_manual(name = 'Sample sizes \nGroup A[1]*B[1], A[1]*B[2], A[2]*B[1], A[2]*B[2]',
                        labels = c(
                          '30, 30, 30, 30', 
                          '30, 45, 45, 45', 
                          '30, 60, 60, 60'
                        ),
                        values = c('solid', 'twodash', 'dotted')
  ) +
  facet_grid(test ~ .) +
    guides(linetype = FALSE, shape = FALSE) +
    theme(strip.text.x = element_blank(), strip.background = element_blank())

joint_reject_welch <- joint_reject_data %>% 
  mutate(test = "Welch's") %>% 
  ggplot(aes(y = joint_reject_welch, x = var_ratio, group = ns, linetype = ns, shape = ns)) +
    geom_line(size = .5) +
    geom_point(size = 3) + 
    scale_x_discrete(labels = factor(c('1/5', '1/2', '1', '2', '5'))) +
    scale_y_continuous(limits = c(0, .30), breaks = seq(0, .30, .05), labels = seq(0, 30, 5) %>% paste0('%')) +
    labs(y = NULL, x = NULL) +
    theme_classic() +
    theme(
      panel.grid = element_blank(),
      panel.grid.major.y = element_line(color = 'gray80')
    ) + 
    scale_shape_manual(name = 'Sample sizes \nGroup A[1]*B[1], A[1]*B[2], A[2]*B[1], A[2]*B[2]',
                     labels = c(
                       '30, 30, 30, 30', 
                       '30, 45, 45, 45', 
                       '30, 60, 60, 60'
                     ),
                     values = rep(c(15, 16, 17), times = 3)
  ) +
    scale_linetype_manual(name = 'Sample sizes \nGroup A[1]*B[1], A[1]*B[2], A[2]*B[1], A[2]*B[2]',
                        labels = c(
                          '30, 30, 30, 30', 
                          '30, 45, 45, 45', 
                          '30, 60, 60, 60'
                        ),
                        values = c('solid', 'twodash', 'dotted')
  ) +
  facet_grid(test ~ .) +
    guides(linetype = FALSE, shape = FALSE) +
    theme(strip.text.x = element_blank(), strip.background = element_blank())




# Power Difference ----

powdiff_plot <- function(data, sim, which_contrast) {
  
  ylims <- c(-.2, .2)
  
  ybreaks <- seq(-1, 1, .05) %>% round(2)
  
  ylabs <- paste0(ybreaks * 100, '%')
  
  data %>% 
    filter(
      which_sim == sim,
      contrast_names == which_contrast
      ) %>% 
    spread(key = test, value = reject_rate) %>% 
    mutate(
      power_diff = `Welch's t test` - ANOVA,
      test = "Welch's - ANOVA"
      ) %>% 
    ggplot(aes(y = power_diff, x = var_ratio, group = ns, linetype = ns, shape = ns)) +
    geom_line(size = .5) +
    geom_point(size = 3) + 
    facet_grid(test ~ contrast_names) +
    scale_x_discrete(labels = factor(c('1/5', '1/2', '1', '2', '5'))) +
    scale_y_continuous(limits = ylims, breaks = ybreaks, labels = ylabs) +
    labs(y = NULL, x = NULL) +
    theme_classic() +
    theme(
      panel.grid = element_blank(),
      panel.grid.major.y = element_line(color = 'gray80')
    ) + 
    scale_shape_manual(name = 'Sample sizes \nGroup A[1]*B[1], A[1]*B[2], A[2]*B[1], A[2]*B[2]',
                     labels = c(
                       '30, 30, 30, 30', 
                       '30, 45, 45, 45', 
                       '30, 60, 60, 60'
                     ),
                     values = rep(c(15, 16, 17), times = 3)
  ) +
    scale_linetype_manual(name = 'Sample sizes \nGroup A[1]*B[1], A[1]*B[2], A[2]*B[1], A[2]*B[2]',
                        labels = c(
                          '30, 30, 30, 30', 
                          '30, 45, 45, 45', 
                          '30, 60, 60, 60'
                        ),
                        values = c('solid', 'twodash', 'dotted')
  ) +
    guides(linetype = FALSE, shape = FALSE)
} 

# Main effect of A
powdiff_me_int_meA <- powdiff_plot(rn_data, sim = 'Main effects & interaction', which_contrast = 'Main Effect of A')

# Main effect of B
powdiff_me_int_meB <- powdiff_plot(rn_data, sim = 'Main effects & interaction', which_contrast = 'Main Effect of B')

# Interaction
powdiff_crossover_int <- powdiff_plot(rn_data, sim = 'Crossover interaction', which_contrast = 'Interaction')
powdiff_me_int_int <- powdiff_plot(rn_data, sim = 'Main effects & interaction', which_contrast = 'Interaction')

# Simple Effects
powdiff_crossover_B1cont <- powdiff_plot(rn_data, sim = 'Crossover interaction', which_contrast = 'Simple Effect of A at B1')
powdiff_crossover_B2cont <- powdiff_plot(rn_data, sim = 'Crossover interaction', which_contrast = 'Simple Effect of A at B2')
powdiff_crossover_A1cont <- powdiff_plot(rn_data, sim = 'Crossover interaction', which_contrast = 'Simple Effect of B at A1')
powdiff_crossover_A2cont <- powdiff_plot(rn_data, sim = 'Crossover interaction', which_contrast = 'Simple Effect of B at A2')
powdiff_me_int_B1cont <- powdiff_plot(rn_data, sim = 'Main effects & interaction', which_contrast = 'Simple Effect of A at B1')
powdiff_me_int_B2cont <- powdiff_plot(rn_data, sim = 'Main effects & interaction', which_contrast = 'Simple Effect of A at B2')
powdiff_me_int_A1cont <- powdiff_plot(rn_data, sim = 'Main effects & interaction', which_contrast = 'Simple Effect of B at A1')
powdiff_me_int_A2cont <- powdiff_plot(rn_data, sim = 'Main effects & interaction', which_contrast = 'Simple Effect of B at A2')






# Coverage Probability ----
# coverage probability (per test)
cov_plot <- function(data, sim, which_contrast, which_test) {
  
  ylims <- c(.85, 1)
  
  ybreaks <- seq(0, 1, .05)
  
  ylabs <- paste0(ybreaks * 100, '%')
  
  data %>% 
    filter(
      which_sim == sim,
      contrast_names == which_contrast,
      test == which_test
      ) %>% 
    ggplot(aes(y = coverage_rate, x = var_ratio, group = ns, linetype = ns, shape = ns)) +
    geom_line(size = .5) +
    geom_point(size = 3) + 
    facet_grid(test ~ contrast_names) +
    scale_x_discrete(labels = factor(c('1/5', '1/2', '1', '2', '5'))) +
    scale_y_continuous(limits = ylims, breaks = ybreaks, labels = ylabs) +
    labs(y = NULL, x = NULL) +
    theme_classic() +
    theme(
      panel.grid = element_blank(),
      panel.grid.major.y = element_line(color = 'gray80')
    ) + 
    scale_shape_manual(name = 'Sample sizes \nGroup A[1]*B[1], A[1]*B[2], A[2]*B[1], A[2]*B[2]',
                     labels = c(
                       '30, 30, 30, 30', 
                       '30, 45, 45, 45', 
                       '30, 60, 60, 60'
                     ),
                     values = rep(c(15, 16, 17), times = 3)
  ) +
    scale_linetype_manual(name = 'Sample sizes \nGroup A[1]*B[1], A[1]*B[2], A[2]*B[1], A[2]*B[2]',
                        labels = c(
                          '30, 30, 30, 30',
                          '30, 45, 45, 45', 
                          '30, 60, 60, 60'
                        ),
                        values = c('solid', 'twodash', 'dotted')
  ) +
    guides(linetype = FALSE, shape = FALSE)
} 


# subset data
cov_data <- mutate(no_effects, which_sim = 'No effects') %>% 
  bind_rows(mutate(crossover_sims, which_sim = 'Crossover interaction')) %>% 
  bind_rows(mutate(me_int_sims, which_sim = 'Main effects & interaction')) %>% 
  ungroup() %>% 
  select(-reject_student, -reject_welch, -joint_reject_student, -joint_reject_welch) %>% 
  gather(key = test, value = coverage_rate, coverage_student, coverage_welch) %>% 
  mutate(
    test = case_when(
      test == 'coverage_student' ~ "ANOVA",
      test == 'coverage_welch' ~ "Welch's t test"
    ),
    contrast_names = case_when(
      contrast_names == 'ME (1 & 2 vs 3 & 4)' ~ 'Main Effect of A',
      contrast_names == 'ME (1 & 3 vs 2 & 4)' ~ 'Main Effect of B',
      contrast_names == 'Interaction' ~ 'Interaction'
    ) %>% 
      factor(levels = c('Main Effect of A', 'Main Effect of B', 'Interaction')),
    ns = paste(min_sample, min_sample * sample_ratio, min_sample * sample_ratio, min_sample * sample_ratio, sep = ', ') %>% factor(levels = c('30, 30, 30, 30', '30, 45, 45, 45', '30, 60, 60, 60')),
    var_ratio = factor(var_ratio),
    sample_ratio = factor(sample_ratio)
  ) %>% 
  filter(
    min_sample == 30,
    str_detect(contrast_names, 'Main|Interaction')
  )

# get min/max reject rates
min_max_rates <- cov_data %>% 
  group_by(contrast_names, test) %>% 
  summarize(
    min_reject = min(coverage_rate),
    max_reject = max(coverage_rate)
  )

# Blank plot
blank_plot_cov <- cov_data %>% 
  filter(
    which_sim == 'No effects',
    contrast_names == 'Main Effect of A',
    test == 'ANOVA'
  ) %>% 
  ggplot(aes(y = coverage_rate, x = var_ratio)) +
  geom_blank() +
  scale_y_continuous (limits = c(0, .15)) +
  labs(x = NULL, y = NULL) +
  theme(
    rect = element_blank(),
    line = element_blank()
  )


# Main effect of A
cov_no_effects_meA_anova <- cov_plot(cov_data, sim = 'No effects', which_contrast = 'Main Effect of A', which_test = 'ANOVA')
cov_crossover_meA_anova <- cov_plot(cov_data, sim = 'Crossover interaction', which_contrast = 'Main Effect of A', which_test = 'ANOVA')
cov_me_int_meA_anova <- cov_plot(cov_data, sim = 'Main effects & interaction', which_contrast = 'Main Effect of A', which_test = 'ANOVA')
cov_no_effects_meA_welch <- cov_plot(cov_data, sim = 'No effects', which_contrast = 'Main Effect of A', which_test = "Welch's t test")
cov_crossover_meA_welch <- cov_plot(cov_data, sim = 'Crossover interaction', which_contrast = 'Main Effect of A', which_test = "Welch's t test")
cov_me_int_meA_welch <- cov_plot(cov_data, sim = 'Main effects & interaction', which_contrast = 'Main Effect of A', which_test = "Welch's t test")

# Main effect of B
cov_no_effects_meB_anova <- cov_plot(cov_data, sim = 'No effects', which_contrast = 'Main Effect of B', which_test = 'ANOVA')
cov_crossover_meB_anova <- cov_plot(cov_data, sim = 'Crossover interaction', which_contrast = 'Main Effect of B', which_test = 'ANOVA')
cov_me_int_meB_anova <- cov_plot(cov_data, sim = 'Main effects & interaction', which_contrast = 'Main Effect of B', which_test = 'ANOVA')
cov_no_effects_meB_welch <- cov_plot(cov_data, sim = 'No effects', which_contrast = 'Main Effect of B', which_test = "Welch's t test")
cov_crossover_meB_welch <- cov_plot(cov_data, sim = 'Crossover interaction', which_contrast = 'Main Effect of B', which_test = "Welch's t test")
cov_me_int_meB_welch <- cov_plot(cov_data, sim = 'Main effects & interaction', which_contrast = 'Main Effect of B', which_test = "Welch's t test")


# Interaction
cov_no_effects_int_anova <- cov_plot(cov_data, sim = 'No effects', which_contrast = 'Interaction', which_test = 'ANOVA')
cov_crossover_int_anova <- cov_plot(cov_data, sim = 'Crossover interaction', which_contrast = 'Interaction', which_test = 'ANOVA')
cov_me_int_int_anova <- cov_plot(cov_data, sim = 'Main effects & interaction', which_contrast = 'Interaction', which_test = 'ANOVA')
cov_no_effects_int_welch <- cov_plot(cov_data, sim = 'No effects', which_contrast = 'Interaction', which_test = "Welch's t test")
cov_crossover_int_welch <- cov_plot(cov_data, sim = 'Crossover interaction', which_contrast = 'Interaction', which_test = "Welch's t test")
cov_me_int_int_welch <- cov_plot(cov_data, sim = 'Main effects & interaction', which_contrast = 'Interaction', which_test = "Welch's t test")
@

    When psychologists compare the means of two groups, they typically use Student's t test \cite{Student1908}, which assumes the group variances are equal. If there is reason to believe variances are unequal, they can use Welch's t test instead \cite{Welch1938, 
Satterthwaite1946}, which does not assume the variances are equal.

    Recently, \citeA{Delacre2017} argued that psychologists should change the typical approach, and that they should use Welch's t test  by default, 
because it does a better job controlling false positives when group variances are unequal, 
and it doesn't lose much power when variances are equal.
Their discussion focused on comparing two groups, but their conclusion can be expanded to cases when there are more than two groups.
In this paper, we compare contrasts based on Welch's t-test to a typical analysis of variance. We show that in designs with more than two groups, Welch's t test does a better job controlling false positives, accurately estimating effect sizes, and simplifying pre-registration, without losing too much power. We illustrate with a 2~$\times$~2 factorial design.


\subsection{Student's and Welch's t tests with Two Groups}

Both Student's and Welch's t tests assume the populations behind the data follow a normal distribution and that the
observations are independent of each other. For both tests, the t-value is the 
difference in group means divided by the standard error of that 
difference:   
    \begin{equation}
    t = \frac{\hat{\mu}_1-\hat{\mu}_2}{\hat{\sigma}_{\mu_1-\mu_2}}
    \end{equation}
The standard error is estimated from the group variances, by summing the group variances divided by their sample sizes, and then then taking the square root of the sum.

\begin{equation}
    \hat{\sigma}_{\overline{x}_1-\overline{x}_2} = \sqrt{\frac{\hat{\sigma}_1^2}{n_1} + \frac{\hat{\sigma}_2^2}{n_2}}
\end{equation}

where $\hat{\sigma}_1^2$ and $\hat{\sigma}_2^2$ are the estimated population variances of the two groups. 

  But Student's t test and Welch's t tests make different assumptions about how to estimate the population variances of the groups. 
  Student's t test assumes the population variances of the groups are equal ($\hat{\sigma}_1^2 = \hat{\sigma}_2^2$), so if the sample variances are different, it's due to sampling error. 
  To correct for this error, Student's t test estimates a common variance
by pooling the sample group variances into a weighted 
average
    \begin{equation}
    s_p^2 = \frac{(n_1 - 1)s_1^2 + (n_2 - 1)s_2^2}{(n_1 - 1) + (n_2 - 1)}
    \end{equation}
     with $s_1^2$ and $s_2^2$ being the two sample group variances. This common variance is used to estimate the population variances of the two groups, and to estimate the standard error of the t test.  
    \begin{equation}
    s_{Student} = \sqrt{\frac{s_p^2}{n_1}  + \frac{s_p^2}{n_2}} %= s_p\sqrt{\frac{1}{n_1}  + \frac{1}{n_2}} Josh W cut this because it's easier to see how the standard error from Student's and Welch's t tests compare by leaving the pooled variance under the radical
    \end{equation}
   
In the pooling equation (EQ 3), if the sample sizes are different, the variance of the group 
with the larger sample size is given more weight. If the larger group has the larger 
variance, then the standard error will be larger; if the larger group has the smaller variance, then the standard error will be smaller.
    
  In contrast Welch's t test does not assume the group variances are equal. The population variances of the groups are estimated with the sample group variances:
    \begin{equation}
    s_{Welch} = \sqrt{\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2}}
    \end{equation}

  Aside from their standard errors, the two tests also differ in how they compute the degrees of 
freedom. Student's t test uses a simple formula to compute the degrees of freedom: $df_{Student}=n_1+n_2-2$. Welch's t test uses a more complicated formula to compute the degrees of freedom: 
    \begin{equation}
    df_{Welch} = \frac{(\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2})^2}{\frac{(s_1^2)^2}{n_1^2(n_1-1)} + \frac{(s_2^2)^2}{n_2^2(n_2-1)}}
    \end{equation}
The important thing to know is the formula decreases the degrees of freedom to the extent that the group
variances differ. It can also produce a degrees of freedom value that is not an integer.





\subsection{Student's and Welch's t tests with More Than Two Groups}

Let's consider the case of a 2~$\times$~2 factorial design with factors A and B. Typically researchers would use a 2~$\times$~2 factorial ANOVA to test for group differences, which would test the main effect of A, the main effect of B, and the interaction of A and B. If the group sample sizes are equal or if Type 3 Sum of Squares is used, then the ANOVA is equivalent to a set of orthogonal contrasts with t tests using the following weights:

\begin{table}[ht]
\begin{tabular}{l c c c c}
\hline
 & \multicolumn{4}{c}{Condition} \\
 & A1, B1 & A1, B2 & A2, B1 & A2, B2 \\
\hline
Main Effect of A & 1 & 1 & -1 & -1 \\
Main Effect of B & 1 & -1 & 1 & -1 \\
Interaction & 1 & -1 & -1 & 1 \\
\hline
\end{tabular}
\end{table}

Each contrast is the sum of the weights, \textit{a}, multiplied by their respective group means.
\begin{equation}
  \hat{I} = \sum a_i \overline{x}_i
\end{equation}

To find the t value to test the contrasts, you need to find the standard error of the contrasts. 
\begin{equation}
  t = \frac{\hat{I}}{\hat{\sigma}_{\hat{I}}}
\end{equation}

To find the estimated standard error of the contrast, you take the sum of each group's estimated variance multiplied by its squared contrast weight and divided by its sample size, then find the square root of the sum. 

\begin{equation}
    \hat{\sigma}_{\hat{I}} = \sqrt{\sum \frac{a_i^2\hat{\sigma_i}^2}{n_i}}
\end{equation}

When Student's t test is used, we assume that the group variances are equal, so we still estimate a common pooled variance by using information from all groups.

\begin{equation}
    s_p^2 = \frac{\sum (n_{i} - 1)s_{i}^2}{\sum (n_{i} - 1)}
\end{equation}

This same pooled variance is used for every estimated group variance in Equation 9. 

\begin{equation}
    s_{\hat{I}_{Student}} = \sqrt{\sum \frac{a_i^2 s_p^2}{n_i}}
\end{equation}

The degrees of freedom are $df = \sum (n_i - 1)$, and the t value is equal to the square root of the F value if ANOVA had been used instead.

However, the benefit of using a t test is that you don't \textit{have} to use Student's t test---you can use Welch's t test instead. When using Welch's t test, instead of finding a common pooled variance, the estimated variance for each group is that group's sample variance because we do not assume variances are equal. 

\begin{equation}
    s_{\hat{I}_{Welch}} = \sqrt{\sum \frac{a_i s_{i}^2}{n_{i}}}
\end{equation}

The degrees of freedom also generalize to use information from all the groups.
\begin{equation}
    df_{Welch} = \frac{(\sum \frac{s_i^2}{n_i})^2}{\sum \frac{(s_i^2)^2}{n_i^2(n_i-1)}}
\end{equation}

Just as Welch's t test controls false positives better than Student's t test without serious downsides, it also controls false positives better than ANOVA, is sometimes more powerful than ANOVA to detect true effects, and does a better job estimating the true effect size. 

We'll illustrate with three examples. First, we'll consider the case where there are no effects. Second, we'll consider a case of a crossover interaction with no main effects. And third, we'll consider a case with two main effects and an interaction. In each case, we ran 10,000 simulations of four independent groups with normally distributed data. We compared how well ANOVAs vs. Welch's t test balances concerns with 
false positives, power, and effect size estimation under 
different variance ratios, sample sizes, sample size ratios, and effect sizes. 
We examined cases where the variances were equal or where the variance of just one group differed from the others.

<<variable_tables, echo = FALSE>>=
n_table <- tribble(
  ~`n ratio`, ~A1B1, ~A1B2, ~A2B1, ~A2B2,
  '1', 30, 30, 30, 30,
  '2/3', 30, 45, 45, 45,
  '1/2', 30, 60, 60, 60
) %>% 
  formattable(
    format = 'latex',
    align = c('l', 'r', 'r', 'r', 'r')
  )

var_table <- tribble(
  ~`Variance ratio`,  ~A1B1, ~A1B2, ~A2B1, ~A2B2,
  '1/5', 2, 10, 10, 10,
  '1/2', 2, 4, 4, 4,
  '1', 2, 2, 2, 2,
  '1/5', 4, 2, 2, 2,
  '1/5', 10, 2, 2, 2
) %>% 
  formattable(
    align = c('l', 'r', 'r', 'r', 'r')
  )

mean_plot <- plot_grid(
    tf_no_effects,
    tf_crossover,
    tf_me_int,
    ncol = 1
  )
@


\section{False Positive Rates}

For false positive rates, we looked at the main effects when there were no effects or only a crossover interaction, and at the interaction effect when there were no effects. 

\begin{figure}[!ht]  
<<false_positives, echo = FALSE, collapse = TRUE, fig = TRUE, width = 12, height = 5>>=
no_effects_fp <- plot_grid(
  
  # first row
  rn_no_effects_meA_anova + theme(strip.text.y = element_blank(), strip.background = element_blank()), 
  rn_no_effects_meB_anova + theme(strip.text.y = element_blank(), strip.background = element_blank()), 
  rn_no_effects_int_anova + theme(strip.background = element_blank()),
  
  # second row
  rn_no_effects_meA_welch + theme(strip.text.x = element_blank(), strip.text.y = element_blank(), strip.background = element_blank()), 
  rn_no_effects_meB_welch + theme(strip.text.x = element_blank(), strip.text.y = element_blank(), strip.background = element_blank()), 
  rn_no_effects_int_welch + theme(strip.text.x = element_blank(), strip.background = element_blank()), 
  ncol = 3
)

crossover_fp <- plot_grid(
  
  # first row
  rn_crossover_meA_anova + theme(strip.text.y = element_blank(), strip.background = element_blank()), 
  rn_crossover_meB_anova + theme(strip.background = element_blank()), 
  blank_plot,
  
  # second row
  rn_crossover_meA_welch + theme(strip.text.x = element_blank(), strip.text.y = element_blank(), strip.background = element_blank()),
  rn_crossover_meB_welch + theme(strip.text.x = element_blank(), strip.background = element_blank()),
  blank_plot,
  
  ncol = 3
)

plot_grid(
  plot_grid(
    tf_no_effects, no_effects_fp,
    tf_crossover, crossover_fp,
    ncol = 2, rel_widths = c(.25, .75)
  ),
  plot_grid(plot_legend),
  ncol = 2,
  rel_widths = c(10, 3),
  labels = list()
) %>% 
add_sub(
  bquote(
    frac(
      sigma[A[1]*B[1]]^2,
      {sigma[A[1]*B[2]]^2 == sigma[A[2]*B[1]]^2} == sigma[A[2]*B[2]]^2
      )
    ), 
  vpadding=grid::unit(0,"lines"),
  y=.1, 
  x=0.5, 
  vjust=0
  ) %>% 
  ggdraw()
@

\textit{Figure 2.} False positives for the simulations when there were no effects or a crossover interaction with no main effects.
\end{figure}

For each comparison, the expected false positive rate is $\alpha = .05$. The observed false positive
rate for ANOVA remained close to the expected rate when either 
the population variances or sample sizes were equal, but it varied widely 
when both the variances and sample sizes were unequal (see Figure 2). 
When the larger group(s) had the larger variance, the false positive rate dropped as low as 
about 2\%, but when it had the smaller 
variance, the false positive rate 
rose as high as 13\%, more than double the expected rate. 
In contrast, the observed false positive rate for Welch's t test 
remained close to the expected rate across all conditions. Overall, Welch's 
t test consistently behaved as expected when it came to false positives, whether there were no effects or a significant interaction but no main effects. 
ANOVA did not.

\subsection{Joint False Positive Rate}

In addition to looking at each comparison separately, we can look at the joint false positive rate
---the probability that at least one of the effects produces a false positive.
We focus on the case where there are no true effects, so the joint false positive rate is the probability that at least one main effect or the interaction produces a false positive. 
The expected joint false positive rate in a 2~$\times$~2 ANOVA, when the three comparisons are independent, is $\alpha = .14$, 
though in practice the rate is typically slightly below .14 \cite{Cramer2016}.


\begin{figure}[!ht]  

<<no_effects_joint_false_positives, echo = FALSE, collapse = TRUE, fig = TRUE, width = 8, height = 4>>=

plot_grid(
  plot_grid(
    joint_reject_student, joint_reject_welch,
    ncol = 1
  ),
  plot_grid(plot_legend),
  ncol = 2,
  rel_widths = c(2, 2),
  labels = list()
) %>% 
add_sub(
  bquote(
    frac(
      sigma[A[1]*B[1]]^2,
      {sigma[A[1]*B[2]]^2 == sigma[A[2]*B[1]]^2} == sigma[A[2]*B[2]]^2
      )
    ), 
  vpadding=grid::unit(0,"lines"),
  y=.1, 
  x=0.25, 
  vjust=0
  ) %>% 
  ggdraw()
@

\textit{Figure 3.} Rates of at least one false positive (at least one main effect or the interaction) for the simulations when there were no effects.
\end{figure}

As with the separate tests of the main effects and interaction, the observed false positive
rate for ANOVA remained close to the expected rate when either 
the population variances or sample sizes were equal, but it varied widely 
when both the variances and sample sizes were unequal (see Figure 3). 
When the larger group(s) had the larger variance, the false positive rate dropped as low as 
about 7\%, but when it had the smaller 
variance, the false positive rate 
rose as high as 26\%.
The observed joint false positive  
rate for Welch's t test remained closer to the expected rate. Welch's 
t test behaved closer to expectations when it came to false positives than 
ANOVA.


\subsection{Power}
To compare the power of ANOVA vs. Welch's t test to detect true effects, we looked at the main effects when there were main effects and an interaction, and at the interaction effect when there was an interaction with or without true main effects. Additionally, in the cases where there is an interaction effect, we can compare the power of Student's t test vs. Welch's t test to detect the simple effects of factor B at each level of factor A.

\begin{figure}[!ht]
<<power_plots, echo = FALSE, collapse = TRUE, fig = TRUE, width = 12, height = 8>>=

crossover_pow <- plot_grid(
  
  # first row
  blank_plot, 
  blank_plot,
  rn_crossover_int_anova + theme(strip.background = element_blank()), 

  # second row
  blank_plot + theme(strip.text.x = element_blank()),
  blank_plot + theme(strip.text.x = element_blank()),
  rn_crossover_int_welch + theme(strip.background = element_blank(), strip.text.x = element_blank()),
  
  # third row
  blank_plot + theme(strip.text.x = element_blank()),
  blank_plot + theme(strip.text.x = element_blank()),
  powdiff_crossover_int + theme(strip.background = element_blank(), strip.text.x = element_blank()),
  
  ncol = 3
)

me_int_pow <- plot_grid(
  
  # first row
  rn_me_int_meA_anova + theme(strip.background = element_blank(), strip.text.y = element_blank()), 
  rn_me_int_meB_anova + theme(strip.background = element_blank(), strip.text.y = element_blank()),
  rn_me_int_int_anova + theme(strip.background = element_blank()), 

  # second row
  rn_me_int_meA_welch + theme(strip.background = element_blank(), strip.text.y = element_blank(), strip.text.x = element_blank()), 
  rn_me_int_meB_welch + theme(strip.background = element_blank(), strip.text.y = element_blank(), strip.text.x = element_blank()),
  rn_me_int_int_welch + theme(strip.background = element_blank(), strip.text.x = element_blank()),
  
  # third row
  powdiff_me_int_meA + theme(strip.background = element_blank(), strip.text.y = element_blank(), strip.text.x = element_blank()), 
  powdiff_me_int_meB + theme(strip.background = element_blank(), strip.text.y = element_blank(), strip.text.x = element_blank()),
  powdiff_me_int_int + theme(strip.background = element_blank(), strip.text.x = element_blank()),
  
  ncol = 3
)

plot_grid(
  plot_grid(
    tf_crossover, crossover_pow,
    tf_me_int, me_int_pow,
    ncol = 2, rel_widths = c(.25, .75)
  ),
  plot_grid(plot_legend),
  ncol = 2,
  rel_widths = c(10, 3),
  labels = list()
) %>% 
add_sub(
  bquote(
    frac(
      sigma[A[1]*B[1]]^2,
      {sigma[A[1]*B[2]]^2 == sigma[A[2]*B[1]]^2} == sigma[A[2]*B[2]]^2
      )
    ), 
  vpadding=grid::unit(0,"lines"),
  y=.1, 
  x=0.5, 
  vjust=0
  ) %>% 
  ggdraw()
@


\textit{Figure 4.} Power for the simulations when there was a crossover interaction or two main effects and an interaction. The third row shows the difference in power between ANOVA and Welch's t test, where higher values mean Welch's t test is more powerful than ANOVA.
\end{figure}

Figure 4 displays the power of ANOVA and Welch's t test to detect true effects, as well as the difference in power between the two tests, where higher values mean Welch's t test is more powerful than ANOVA. As we can see, the power of the two tests to detect the interaction is approximately equal when either the sample sizes or the variances are equal. However, when the sample sizes and the variances are different, the power of the two tests is different. ANOVA has more power when the larger group has the smaller variance--the same condition where it inflates the false positive rate. In contrast, Welch's t test has more power when the larger group has the larger variance, without the risk of inflating the false positive rate. The differences in power are primarily driven by the fact that when the larger group has the smaller variance, ANOVA underestimates the standard error and tends to reject the null hypothesis, regardless of whether or not there is a true effect. So the main benefit of ANOVA is undermined by a higher risk of false positives.


\subsection{Coverage Probability}
Next, we looked at how well ANOVA and Welch's t test estimated the true effect. 
We used 95\% confidence intervals constructed using Student's and Welch's t 
tests to find their relative coverage probability, the proportion of confidence intervals 
that contain the population value of the estimated parameter, which in this case is the 
difference in group means. By 
definition,  the expected coverage probability of 95\% confidence intervals  is .95. 
Additionally, when the null hypothesis is true and $\alpha = 5\%$, the coverage 
probability of 95\% confidence intervals has a simple relationship with the 
false positive rate---it's the complement of the false positive rate. 

Figure 5 displays the coverage probabilities of the two t tests. 
The coverage probability for ANOVA varies, just as the 
false positive rate did, ranging from about 87-98\% when either the sample sizes or variances were unequal. 
Importantly, under the same conditions where Student's t test had the most power to detect true effects, the confidence intervals were least likely to include the true effect size.   
In contrast, Welch's t test performed as expected and the confidence interval 
contains the true effect 95\% of the time regardless of the variance and sample 
size ratios. 

\begin{figure}[!ht]
<<coverage_probability, echo = FALSE, collapse = TRUE, fig = TRUE, width = 12, height = 8>>=

no_effects_cov <- plot_grid(
  
  # first row
  cov_no_effects_meA_anova + theme(strip.text.y = element_blank(), strip.background = element_blank()), 
  cov_no_effects_meB_anova + theme(strip.text.y = element_blank(), strip.background = element_blank()), 
  cov_no_effects_int_anova + theme(strip.background = element_blank()),
  
  # second row
  cov_no_effects_meA_welch + theme(strip.text.x = element_blank(), strip.text.y = element_blank(), strip.background = element_blank()), 
  cov_no_effects_meB_welch + theme(strip.text.x = element_blank(), strip.text.y = element_blank(), strip.background = element_blank()), 
  cov_no_effects_int_welch + theme(strip.text.x = element_blank(), strip.background = element_blank()), 
  ncol = 3
)

crossover_cov <- plot_grid(
  
  # first row
  cov_crossover_meA_anova + theme(strip.text.y = element_blank(), strip.background = element_blank()), 
  cov_crossover_meB_anova + theme(strip.text.y = element_blank(), strip.background = element_blank()), 
  cov_crossover_int_anova + theme(strip.background = element_blank()), 
  
  # second row
  cov_crossover_meA_welch + theme(strip.text.x = element_blank(), strip.text.y = element_blank(), strip.background = element_blank()),
  cov_crossover_meB_welch + theme(strip.text.y = element_blank(), strip.background = element_blank()), 
  cov_crossover_int_welch + theme(strip.background = element_blank()), 
  
  ncol = 3
)

me_int_cov <- plot_grid(
  
  # first row
  cov_me_int_meA_anova + theme(strip.text.y = element_blank(), strip.background = element_blank()), 
  cov_me_int_meB_anova + theme(strip.text.y = element_blank(), strip.background = element_blank()), 
  cov_me_int_int_anova + theme(strip.background = element_blank()), 
  
  # second row
  cov_me_int_meA_welch + theme(strip.text.x = element_blank(), strip.text.y = element_blank(), strip.background = element_blank()),
  cov_me_int_meB_welch + theme(strip.text.y = element_blank(), strip.background = element_blank()), 
  cov_me_int_int_welch + theme(strip.background = element_blank()), 
  
  ncol = 3
)

plot_grid(
  plot_grid(
    tf_no_effects, no_effects_cov,
    tf_crossover, crossover_cov,
    tf_me_int, me_int_cov,
    ncol = 2, rel_widths = c(.25, .75)
  ),
  plot_grid(plot_legend),
  ncol = 2,
  rel_widths = c(10, 3),
  labels = list()
) %>% 
add_sub(
  bquote(
    frac(
      sigma[A[1]*B[1]]^2,
      {sigma[A[1]*B[2]]^2 == sigma[A[2]*B[1]]^2} == sigma[A[2]*B[2]]^2
      )
    ), 
  vpadding=grid::unit(0,"lines"),
  y=.1, 
  x=0.5, 
  vjust=0
  ) %>% 
  ggdraw()
@

\textit{Figure 5.} Coverage probability for ANOVA and Welch's t test
as the variance ratio, sample size ratio, and sample sizes vary.
\end{figure}

\subsection{Summary}
For the tests of main effects and interactions, there are clear benefits to using contrasts based on Welch's t test instead of ANOVA. When the variances are equal, Welch's t test performs about the same as ANOVA, and when the variances are unequal, Welch's t test retains the expected 5\% false positive rate, and its 95\% confidence intervals really do include the true effect size 95\% of the time.

Additionally, there are clear downsides to ANOVA/Student's t test.  When the variances and sample sizes are unequal, it can inflate the false positive rate, and its alleged 95\% confidence intervals can miss the true effect size more often than they should. The only benefit to ANOVA is that it is more powerful than Welch's t test under some conditions, but this benefit is undermined because under those same conditions it inflates the false positive rate and incorrectly estimates the effect size. 

But we still have another topic to cover, which is contrasts that test the simple effects of each factor. These are often run as follow-up tests to a significant interaction, though we support running them first if they are specific tests of the key hypothesis. 

\section{Simple Effects}
In addition to looking at main effects and interactions, we can consider the simple effects of A at each level of B, and the simple effects of B at each level of A. 
The contrast weights are:

\begin{table}[ht]
\begin{tabular}{l c c c c}
\hline
 & \multicolumn{4}{c}{Condition} \\
 & A1, B1 & A1, B2 & A2, B1 & A2, B2 \\
\hline
Simple Effect of A at B1 & 1 & 0 & -1 & 0 \\
Simple Effect of B at A1 & 1 & -1 & 0 & 0 \\
Simple Effect of A at B2 & 0 & 1 & 0 & -1 \\
Simple Effect of B at A2 & 0 & 0 & 1 & -1 \\
\hline
\end{tabular}
\end{table}

In our current simulations, where only group A1/B1 has a variance or sample size that's different from the other groups, what's particularly different from the main effects and interaction is that in some contrasts, that group is excluded, and the groups that are being directly compared have equal sample sizes and variances even though the excluded group doesn't.  

The t tests for the simple effects are constructed using the same formulas as those that we use for main effects and interactions, and the only difference is the contrast weights.

\subsection{False Positives}

\begin{figure}[!ht]
<<se_false_positives, echo = FALSE, collapse = TRUE, fig = TRUE, width = 12, height = 8>>=

no_effects_fp_se <- plot_grid(
  
  # first row
  rn_no_effects_B1cont_anova + theme(strip.text.y = element_blank(), strip.background = element_blank()),
  rn_no_effects_A1cont_anova + theme(strip.text.y = element_blank(), strip.background = element_blank()), 
  rn_no_effects_B2cont_anova + theme(strip.text.y = element_blank(), strip.background = element_blank()),
  rn_no_effects_A2cont_anova + theme(strip.background = element_blank()),

  # second row
  rn_no_effects_B1cont_welch + theme(strip.background = element_blank(), strip.text.y = element_blank(), strip.text.x = element_blank()),
  rn_no_effects_A1cont_welch + theme(strip.background = element_blank(), strip.text.y = element_blank(), strip.text.x = element_blank()), 
  rn_no_effects_B2cont_welch + theme(strip.background = element_blank(), strip.text.y = element_blank(), strip.text.x = element_blank()),
  rn_no_effects_A2cont_welch + theme(strip.background = element_blank(), strip.text.x = element_blank()), 
  
  ncol = 4
)

me_int_fp_se <- plot_grid(
  
  # first row
  blank_plot, 
  blank_plot, 
  blank_plot, 
  rn_me_int_A2cont_anova + theme(strip.background = element_blank()),

  # second row
  blank_plot, 
  blank_plot, 
  blank_plot, 
  rn_me_int_A2cont_welch + theme(strip.background = element_blank(), strip.text.x = element_blank()), 
  
  ncol = 4
)


plot_grid(
  plot_grid(
    tf_no_effects, no_effects_fp_se,
    tf_me_int, me_int_fp_se,
    ncol = 2, rel_widths = c(.25, .75)
  ),
  plot_grid(plot_legend),
  ncol = 2,
  rel_widths = c(10, 3),
  labels = list()
) %>%
add_sub(
  bquote(
    frac(
      sigma[A[1]*B[1]]^2,
      {sigma[A[1]*B[2]]^2 == sigma[A[2]*B[1]]^2} == sigma[A[2]*B[2]]^2
      )
    ),
  vpadding=grid::unit(0,"lines"),
  y=.1,
  x=0.5,
  vjust=0
  ) %>%
  ggdraw()
@

\textit{Figure 5.} False positives for the simple effects.
\end{figure}

As with the main effects and interactions, both ANOVA and Welch's t test were near the expected 5\% false positive rate when both variances and sample sizes were equal. But unlike the main effects and interactions, for the simple effects, when the variance of one group was different from the other groups, having equal sample sizes didn't keep the false positive rate at the expected 5\% rate for Student's t test.

The group with the variance that was different from the other groups is A1B1, which was included in the contrasts testing the simple effect of A at B1 and the simple effect of B at A1 (i.e., had a non-zero weight), and was excluded from the contrasts testing the simple effect of A at B2 and the simple effect of B at A2 (i.e., had a zero weight). When the group with the variance that was different from the other groups was included in the contrasts, Student's t test was too conservative when its variance was smaller, and was too liberal when its variance was larger. But even when the group with the variance that was different from the other groups was excluded from the contrasts, Student's t test was too liberal when its variance was smaller, and was too conservative when its variance was larger, which was the opposite pattern to when it was included in the contrast, though the size of the effect was smaller.

Why, for the simple effects, did Student's t test depart from the expected 5\% rate? And why was the pattern opposite when the group with the different variance was included or excluded from the contrast? With four groups that have equal sample sizes, the pooled variance becomes

\begin{equation}
    s_p^2 = \frac{\sum (n - 1)s_{i}^2}{\sum (n - 1)} = \frac{(n - 1)(s_{1}^2 + s_{2}^2 + s_{3}^2 + s_{4}^2)}{4(n - 1)} = \frac{1}{4}(s_{1}^2 + s_{2}^2 + s_{3}^2 + s_{4}^2)
\end{equation}

When we plug the above into Equation 11, we find that the standard error is the same for any two-group contrast, whether it includes the standard error from the group with unequal variances or not.

\begin{equation}
\begin{split}
s_{Student_{A_1-A_2|B_1}} & = s_{Student_{A_1-A_2|B_2}} \\
& = \sqrt{(1^2\frac{s_p^2}{n} + 0^2\frac{s_p^2}{n} + -1^2\frac{s_p^2}{n} + 0^2\frac{s_p^2}{n})} \\
& = \sqrt{(0^2\frac{s_p^2}{n} + 1^2\frac{s_p^2}{n} + 0^2\frac{s_p^2}{n} + -1^2\frac{s_p^2}{n})} \\
& = \sqrt{2\frac{s_p^2}{n}} \\
& = \sqrt{2\frac{(s_{1}^2 + s_{2}^2 + s_{3}^2 + s_{4}^2)}{4n}}\\
& = \sqrt{\frac{1}{2n}(s_{1}^2 + s_{2}^2 + s_{3}^2 + s_{4}^2)}
\end{split}
\end{equation}

So when Student's t test is used and the sample sizes are equal, each group's variance is weighted equally to estimate the standard error of the contrast, even if that group is excluded from the contrast. However, Welch's t test estimates the standard error from only the groups that are \textit{included} in the contrast.

\begin{equation}
\begin{split}
s_{Welch_{A_1-A_2|B_1}} & = \sqrt{(1^2\frac{s_1^2}{n} + 0^2\frac{s_2^2}{n} + -1^2\frac{s_3^2}{n} + 0^2\frac{s_4^2}{n})} \\
& = \sqrt{\frac{1}{n}(s_1^2 + s_3^2)} \\
& \neq \sqrt{(0^2\frac{s_1^2}{n} + 1^2\frac{s_2^2}{n} + 0^2\frac{s_3^2}{n} + -1^2\frac{s_4^2}{n})} \\
& = \sqrt{\frac{1}{n}(s_2^2 + s_4^2)}
\end{split}
\end{equation}

Note that if all variances are equal, Equations 15 and 16 are equal. It's easiest to interpret the pattern of results from Student's t test by assuming that Welch's t test has the correct standard error, which is implied from the fact that it retains the expected 5\% false positive rate. The difference in the two standard errors is that, under the square root sign, Student's t test weighs each group's variance equally, whether its included or excluded from the specific contrast, whereas Welch's t test weighs the groups that are included in the contrast twice as much as Student's t test, and gives no weight to the groups that are excluded from the contrast. This means that Student's t test is giving too much weight to the variances from groups that are excluded from the contrast--if those groups have higher variance, then Student's t test will be too conservative, and if those groups have lower variance, then Student's t test will be too liberal. So when the group with the different variance is excluded from the contrast, its variance will be over-weighted, causing a lower false positive rate when it has a higher variance and a higher false positive rate when it has the lower variance; when the group is included in the contrast, its variance will be under-wighted, causing a higher false positive rate when it has a higher variance and a lower false positive rate when it has the higher variance.

\subsection{Power}

The power of ANOVA and Welch's t test to detect the simple effects is parallel to the false positive rates. When the variances are equal, the two tests are about equally powerful to detect true effects. But when the variances are unequal, the power of the two tests is different, even when all groups have the same sample size. When the group with the different variance is included in the contrast, Welch's t test is more powerful when the one group has the smaller variance, and Student's t test is more powerful when the one group has the larger variance; but when the group with the different variance is excluded from the contrast, the pattern is reversed (see the explanation for this in the preceding false positive section). Just as with main effects and interactions, Student's t test is more powerful under the same conditions that it inflates the false positive rate, but Welch's t test is more powerful under other conditions while retaining the expected 5\% false positive rate.

\begin{figure}[!ht]
<<se_power_plots, echo = FALSE, collapse = TRUE, fig = TRUE, width = 12, height = 8>>=

crossover_pow_se <- plot_grid(

  # first row
  rn_crossover_B1cont_anova + theme(strip.background = element_blank(), strip.text.y = element_blank()) + scale_y_continuous(limits = c(0, 1)),
  rn_crossover_A1cont_anova + theme(strip.background = element_blank(), strip.text.y = element_blank()) + scale_y_continuous(limits = c(0, 1)),
  rn_crossover_B2cont_anova + theme(strip.background = element_blank(), strip.text.y = element_blank()) + scale_y_continuous(limits = c(0, 1)),
  rn_crossover_A2cont_anova + theme(strip.background = element_blank()) + scale_y_continuous(limits = c(0, 1)),

  # second row
  rn_crossover_B1cont_welch + theme(strip.background = element_blank(), strip.text.y = element_blank(), strip.text.x = element_blank()) + scale_y_continuous(limits = c(0, 1)),
  rn_crossover_A1cont_welch + theme(strip.background = element_blank(), strip.text.y = element_blank(), strip.text.x = element_blank()) + scale_y_continuous(limits = c(0, 1)),
  rn_crossover_B2cont_welch + theme(strip.background = element_blank(), strip.text.y = element_blank(), strip.text.x = element_blank()) + scale_y_continuous(limits = c(0, 1)),
  rn_crossover_A2cont_welch + theme(strip.background = element_blank(), strip.text.x = element_blank()) + scale_y_continuous(limits = c(0, 1)),

  # third row
  powdiff_crossover_B1cont + theme(strip.text.x = element_blank(), strip.text.y = element_blank()) + scale_y_continuous(limits = c(-.25, .25)),
  powdiff_crossover_A1cont + theme(strip.text.x = element_blank(), strip.text.y = element_blank()) + scale_y_continuous(limits = c(-.25, .25)),
  powdiff_crossover_B2cont + theme(strip.text.x = element_blank(), strip.text.y = element_blank()) + scale_y_continuous(limits = c(-.25, .25)),
  powdiff_crossover_A2cont + theme(strip.background = element_blank(), strip.text.x = element_blank()) + scale_y_continuous(limits = c(-.25, .25)),

  ncol = 4
)

me_int_pow_se <- plot_grid(

  # first row
  rn_me_int_B1cont_anova + theme(strip.background = element_blank(), strip.text.y = element_blank()) + scale_y_continuous(limits = c(0, 1)),
  rn_me_int_A1cont_anova + theme(strip.background = element_blank(), strip.text.y = element_blank()) + scale_y_continuous(limits = c(0, 1)),
  rn_me_int_B2cont_anova + theme(strip.background = element_blank()) + scale_y_continuous(limits = c(0, 1)),
  blank_plot,

  # second row
  rn_me_int_B1cont_welch + theme(strip.background = element_blank(), strip.text.y = element_blank(), strip.text.x = element_blank()) + scale_y_continuous(limits = c(0, 1)),
  rn_me_int_A1cont_welch + theme(strip.background = element_blank(), strip.text.y = element_blank(), strip.text.x = element_blank()) + scale_y_continuous(limits = c(0, 1)),
  rn_me_int_B2cont_welch + theme(strip.background = element_blank(), strip.text.x = element_blank()) + scale_y_continuous(limits = c(0, 1)),
  blank_plot,

  # third row
  powdiff_me_int_B1cont + theme(strip.text.x = element_blank(), strip.text.y = element_blank()) + scale_y_continuous(limits = c(-.4, .4)),
  powdiff_me_int_A1cont + theme(strip.text.x = element_blank(), strip.text.y = element_blank()) + scale_y_continuous(limits = c(-.4, .4)),
  powdiff_me_int_B2cont + theme(strip.background = element_blank(), strip.text.x = element_blank()) + scale_y_continuous(limits = c(-.4, .4)),
  blank_plot,

  ncol = 4
)

plot_grid(
  plot_grid(
    tf_crossover, crossover_pow_se,
    tf_me_int, me_int_pow_se,
    ncol = 2, rel_widths = c(.25, .75)
  ),
  plot_grid(plot_legend),
  ncol = 2,
  rel_widths = c(10, 3),
  labels = list()
) %>%
add_sub(
  bquote(
    frac(
      sigma[A[1]*B[1]]^2,
      {sigma[A[1]*B[2]]^2 == sigma[A[2]*B[1]]^2} == sigma[A[2]*B[2]]^2
      )
    ),
  vpadding=grid::unit(0,"lines"),
  y=.1,
  x=0.5,
  vjust=0
  ) %>%
  ggdraw()
@

\textit{Figure 5.} Reject rates for the simple effect of B when there was a crossover interaction or two main effects and an interaction. The third row shows the difference in reject rates between ANOVA and Welch's t test, where higher values mean Welch's t test is more powerful than ANOVA.
\end{figure}


NOTE: Old text below

Next, let's consider the simple effects.
Similar to the interaction effect, ANOVA and Welch's t test had similar power when the variances were equal. But unlike the interaction effect, having equal sample sizes didn't prevent differences in power when the variances were unequal.
The reason for this is that the two tests weigh the group variances differently to find the standard error of the contrast. 
When the sample sizes are equal, we can express the standard error of the contrast with weights on the variance of each group.
\begin{equation}
    s = \sqrt{
    \frac{1}{n}(
    a_{A1B1}s^2_{A1B1} +
    a_{A1B2}s^2_{A1B2} +
    a_{A1B1}s^2_{A2B1} +
    a_{A2B2}s^2_{A2B2}
    )
    }
    \end{equation}
    
For Student's t test, the weights are always equal to 1/2, no matter which groups are involved in the contrast.

\begin{equation}
    s_{Student} = \sqrt{
    \frac{1}{n}(
    \frac{1}{2}s^2_{A1B1} +
    \frac{1}{2}s^2_{A1B2} +
    \frac{1}{2}s^2_{A2B1} +
    \frac{1}{2}s^2_{A2B2}
    )
    }
    \end{equation}
    
But for Welch's t test, the weights are equal to the contrast weights. So for the contrast between groups A1B1 and A1B2, groups A1B1 and A1B2 get weights of 1 and the other groups get weights of 0

\begin{equation}
    s_{Welch} = \sqrt{
    \frac{1}{n}(
    1s^2_{A1B1} +
    1s^2_{A1B2} +
    0s^2_{A2B1} +
    0s^2_{A2B2}
    )
    }
    \end{equation}
    
And for the contrast between groups A2B1 and A2B2, groups A2B1 and A2B2 get weights of 1 and the other groups get weights of 0

\begin{equation}
    s_{Welch} = \sqrt{
    \frac{1}{n}(
    0s^2_{A1B1} +
    0s^2_{A1B2} +
    1s^2_{A2B1} +
    1s^2_{A2B2}
    )
    }
    \end{equation}

Whether Welch's or Student's t test has more power to detect effects depends on which standard error is smaller. Though not shown here, Student's t test is more powerful under the same conditions that it inflates the false positive rate, but Welch's t test is more powerful under other conditions and does not inflate the false positive rate. 



\begin{figure}[!ht]
<<power_crossover_int, echo = FALSE,  collapse = TRUE, fig = TRUE, width = 6, height = 7>>=

current_data <- crossover_sims %>% 
  ungroup() %>% 
  select(-coverage_student, -coverage_welch) %>% 
  mutate(diff = reject_welch - reject_student) %>% 
  gather(key = test, value = reject_rate, reject_student, reject_welch, diff) %>% 
  mutate(
    test = case_when(
      test == 'reject_student' ~ "Student's t test",
      test == 'reject_welch' ~ "Welch's t test",
      test == 'diff' ~ 'Welch - ANOVA'
    ),
    ns = paste(min_sample, min_sample * sample_ratio, min_sample * sample_ratio, min_sample * sample_ratio, sep = ', ') %>% factor(),
    var_ratio = factor(var_ratio),
    sample_ratio = factor(sample_ratio)
  ) %>% 
  filter(
    test == 'Welch - ANOVA',
    min_sample == 30,
    !str_detect(contrast_names, 'ME')
  ) %>% 
  mutate( 
    contrast_names = factor(contrast_names, levels = c('Interaction', 'SE (1 vs 2)', 'SE (3 vs 4)'), labels = c('Interaction', 'Simple Effect (A[1]*B[1] vs. A[1]*B[2])', 'Simple Effect (A[2]*B[1] vs. A[2]*B[2])'))
    )

current_data %>% 
  ggplot(aes(y = reject_rate, x = var_ratio, group = ns, linetype = ns, shape = ns)) +
  geom_line(size = .5) +
  geom_point(size = 4) +
  facet_grid(contrast_names ~ test) +
  geom_hline(aes(yintercept = 0), color = 'gray80') +
  scale_x_discrete(labels = factor(c('1/5', '1/2', '1', '2', '5'))) +
  labs(
    x = bquote(frac(sigma[A[1]*B[1]]^2,{sigma[A[1]*B[2]]^2 == sigma[A[2]*B[1]]^2} == sigma[A[2]*B[2]]^2)),
    y = 'Power'
  ) +
  theme_classic() +
  theme(
    panel.grid = element_blank()
  ) +
  scale_shape_manual(name = 'Sample sizes \nGroup A[1]*B[1], A[1]*B[2], A[2]*B[1], A[2]*B[2]',
                     labels = c(
                       '30, 30, 30, 30', 
                       '30, 45, 45, 45', 
                       '30, 60, 60, 60'
                     ),
                     values = rep(c(15, 16, 17), times = 3)
  ) +
  scale_linetype_manual(name = 'Sample sizes \nGroup A[1]*B[1], A[1]*B[2], A[2]*B[1], A[2]*B[2]',
                        labels = c(
                          '30, 30, 30, 30', 
                          '30, 45, 45, 45', 
                          '30, 60, 60, 60'
                        ),
                        values = rep('solid', each = 9)
  ) 
@

\textit{Figure 5.} Difference in power between Welch's t test and ANOVA to detect true effects with a crossover interaction.
\end{figure}

    Is Welch's t test underpowered compared to Student's t test? When the sample sizes or variances were equal, power was approximately equal. However, when both the sample sizes and 
variances were unequal, there were differences. Student's t 
test was more powerful when the large sample(s) had the smaller variance, whereas 
Welch's t test was more powerful when the small sample(s) had the smaller variance. (NOTE: add an exception for contrasts)
    
    Student's t test had the greatest power over 
Welch's when one sample size was twice the other and the large 
group(s) had the small variance. These are the same conditions where Student's t 
test more than doubled the expected false positive rate. So the advantage of Student's t test
in power is undermined by its inflated false 
positive rate. In contrast, Welch's t test was more 
powerful when the small group had the smaller variance, but it never inflated the false positive rate. 
These joint results, on the false positive rate and power, support the decision to always use Welch's t test. By using 
Welch's t test, you sometimes have greater power to detect true effects, but 
you don't have to worry about sacrificing concerns with false positives.  










\section{Case 2. No main effects, crossover interaction}

Next, let's consider the case where there are no main effects, 
but there is a perfect crossover interaction, such that the effect of one factor has the same magnitude, 
but opposite direction, at each level of the second factor. For our simulations, the difference in 
means corresponded to an effect size of $d = .5$ when variance are equal.
We will compare the traditional ANOVA approach to Welch's t test on the false positive rate for the main effects, power to detect the interaction, and effect size estimation for the interaction.

\subsection{False Positive Rate for Main Effects}

As with the case of no effects, Welch's t test did a better job keeping the false positive rate near the expected value of .05 than ANOVA, which inflated the false positive rate when the group(s) with the smaller sample size had the larger variance.

\begin{figure}[!ht]  

<<no_effects_false_positives_2, echo = FALSE, collapse = TRUE, fig = TRUE, width = 8, height = 6.5>>=

# subset data
current_data <- crossover_sims %>% 
  ungroup() %>% 
  select(-coverage_student, -coverage_welch) %>% 
  gather(key = test, value = reject_rate, reject_student, reject_welch) %>% 
  mutate(
    test = case_when(
      test == 'reject_student' ~ "ANOVA",
      test == 'reject_welch' ~ "Welch's t test"
    ),
    contrast_names = factor(contrast_names, levels = c('ME (1 & 2 vs 3 & 4)', 'ME (1 & 3 vs 2 & 4)')),
    ns = paste(min_sample, min_sample * sample_ratio, min_sample * sample_ratio, min_sample * sample_ratio, sep = ', ') %>% factor(levels = c('50, 50, 50, 50', '50, 75, 75, 75', '50, 100, 100, 100')),
    var_ratio = factor(var_ratio),
    sample_ratio = factor(sample_ratio)
  ) %>% 
  filter(
    min_sample == 50,
    str_detect(contrast_names, 'ME')
  )

# get min/max reject rates
min_max_rates <- current_data %>% 
  group_by(contrast_names, test) %>% 
  summarize(
    min_reject = min(reject_rate),
    max_reject = max(reject_rate)
  )

# false positives (per test)
current_data %>% 
  ggplot(aes(y = reject_rate, x = var_ratio, group = ns, linetype = ns, shape = ns)) +
  geom_line(size = .5) +
  geom_point(size = 4) + 
  facet_grid(contrast_names ~ test) +
  scale_x_discrete(labels = factor(c('1/5', '1/2', '1', '2', '5'))) +
  scale_y_continuous(breaks = c(0, .05, .10), minor_breaks = seq(0, .13, .01)) +
  labs(
    x = bquote(frac(sigma[A[1]*B[1]]^2,{sigma[A[1]*B[2]]^2 == sigma[A[2]*B[1]]^2} == sigma[A[2]*B[2]]^2)),
    y = 'False Positive Rate'
  ) +
  theme_classic() +
  theme(
    panel.grid = element_blank(),
    panel.grid.major.y = element_line(color = 'gray80')
  ) +
  scale_shape_manual(name = 'Sample sizes \nGroup A[1]*B[1], A[1]*B[2], A[2]*B[1], A[2]*B[2]',
                     labels = c(
                       '50, 50, 50, 50',
                       '50, 75, 75, 75',
                       '50, 100, 100, 100'
                     ),
                     values = rep(c(15, 16, 17), times = 3)
  ) +
  scale_linetype_manual(name = 'Sample sizes \nGroup A[1]*B[1], A[1]*B[2], A[2]*B[1], A[2]*B[2]',
                        labels = c(
                          '50, 50, 50, 50',
                          '50, 75, 75, 75',
                          '50, 100, 100, 100'
                        ),
                        values = rep('solid', each = 9)
  ) 
@

\textit{Figure 4.} False positive rates for main effects when there is a significant crossover interaction.
\end{figure}

\subsection{Power}


Figure 5 displays the difference in power between Welch's t test and ANOVA to detect the true effects, with higher values indicating that Welch's is more powerful. 

First, let's consider the interaction effect.
As we can see, the power of the two tests to detect the interaction was approximately equal when either the sample sizes or the variances were equal. However, when the sample sizes and the variances were different, ANOVA had more power when the small group had the larger variance--the same condition when it inflated the false positive rate. Welch's t test had more power when the small group had the smaller variance, without the risk of inflating the false positive rate. This shows that ANOVA/Student's t test are only more powerful under the same conditions where they pose a higher risk of false positives, but Welch's t test can be more powerful without inflating the risk of false positives.

Next, let's consider the simple effects.
Similar to the interaction effect, ANOVA and Welch's t test had similar power when the variances were equal. But unlike the interaction effect, having equal sample sizes didn't prevent differences in power when the variances were unequal.
The reason for this is that the two tests weigh the group variances differently to find the standard error of the contrast. 
When the sample sizes are equal, we can express the standard error of the contrast with weights on the variance of each group.
\begin{equation}
    s = \sqrt{
    \frac{1}{n}(
    a_{A1B1}s^2_{A1B1} +
    a_{A1B2}s^2_{A1B2} +
    a_{A1B1}s^2_{A2B1} +
    a_{A2B2}s^2_{A2B2}
    )
    }
    \end{equation}
    
For Student's t test, the weights are always equal to 1/2, no matter which groups are involved in the contrast.

\begin{equation}
    s_{Student} = \sqrt{
    \frac{1}{n}(
    \frac{1}{2}s^2_{A1B1} +
    \frac{1}{2}s^2_{A1B2} +
    \frac{1}{2}s^2_{A2B1} +
    \frac{1}{2}s^2_{A2B2}
    )
    }
    \end{equation}
    
But for Welch's t test, the weights are equal to the contrast weights. So for the contrast between groups A1B1 and A1B2, groups A1B1 and A1B2 get weights of 1 and the other groups get weights of 0

\begin{equation}
    s_{Welch} = \sqrt{
    \frac{1}{n}(
    1s^2_{A1B1} +
    1s^2_{A1B2} +
    0s^2_{A2B1} +
    0s^2_{A2B2}
    )
    }
    \end{equation}
    
And for the contrast between groups A2B1 and A2B2, groups A2B1 and A2B2 get weights of 1 and the other groups get weights of 0

\begin{equation}
    s_{Welch} = \sqrt{
    \frac{1}{n}(
    0s^2_{A1B1} +
    0s^2_{A1B2} +
    1s^2_{A2B1} +
    1s^2_{A2B2}
    )
    }
    \end{equation}

Whether Welch's or Student's t test has more power to detect effects depends on which standard error is smaller. Though not shown here, Student's t test is more powerful under the same conditions that it inflates the false positive rate, but Welch's t test is more powerful under other conditions and does not inflate the false positive rate. 



\begin{figure}[!ht]
<<power_crossover_int, echo = FALSE,  collapse = TRUE, fig = TRUE, width = 6, height = 7>>=

current_data <- crossover_sims %>% 
  ungroup() %>% 
  select(-coverage_student, -coverage_welch) %>% 
  mutate(diff = reject_welch - reject_student) %>% 
  gather(key = test, value = reject_rate, reject_student, reject_welch, diff) %>% 
  mutate(
    test = case_when(
      test == 'reject_student' ~ "Student's t test",
      test == 'reject_welch' ~ "Welch's t test",
      test == 'diff' ~ 'Welch - ANOVA'
    ),
    ns = paste(min_sample, min_sample * sample_ratio, min_sample * sample_ratio, min_sample * sample_ratio, sep = ', ') %>% factor(),
    var_ratio = factor(var_ratio),
    sample_ratio = factor(sample_ratio)
  ) %>% 
  filter(
    test == 'Welch - ANOVA',
    min_sample == 50,
    !str_detect(contrast_names, 'ME')
  ) %>% 
  mutate( 
    contrast_names = factor(contrast_names, levels = c('Interaction', 'SE (1 vs 2)', 'SE (3 vs 4)'), labels = c('Interaction', 'Simple Effect (A[1]*B[1] vs. A[1]*B[2])', 'Simple Effect (A[2]*B[1] vs. A[2]*B[2])'))
    )

current_data %>% 
  ggplot(aes(y = reject_rate, x = var_ratio, group = ns, linetype = ns, shape = ns)) +
  geom_line(size = .5) +
  geom_point(size = 4) +
  facet_grid(contrast_names ~ test) +
  geom_hline(aes(yintercept = 0), color = 'gray80') +
  scale_x_discrete(labels = factor(c('1/5', '1/2', '1', '2', '5'))) +
  labs(
    x = bquote(frac(sigma[A[1]*B[1]]^2,{sigma[A[1]*B[2]]^2 == sigma[A[2]*B[1]]^2} == sigma[A[2]*B[2]]^2)),
    y = 'Power'
  ) +
  theme_classic() +
  theme(
    panel.grid = element_blank()
  ) +
  scale_shape_manual(name = 'Sample sizes \nGroup A[1]*B[1], A[1]*B[2], A[2]*B[1], A[2]*B[2]',
                     labels = c(
                       '50, 50, 50, 50',
                       '50, 75, 75, 75',
                       '50, 100, 100, 100'
                     ),
                     values = rep(c(15, 16, 17), times = 3)
  ) +
  scale_linetype_manual(name = 'Sample sizes \nGroup A[1]*B[1], A[1]*B[2], A[2]*B[1], A[2]*B[2]',
                        labels = c(
                          '50, 50, 50, 50',
                          '50, 75, 75, 75',
                          '50, 100, 100, 100'
                        ),
                        values = rep('solid', each = 9)
  ) 
@

\textit{Figure 5.} Difference in power between Welch's t test and ANOVA to detect true effects with a crossover interaction.
\end{figure}

    Is Welch's t test underpowered compared to Student's t test? When the sample sizes or variances were equal, power was approximately equal. However, when both the sample sizes and 
variances were unequal, there were differences. Student's t 
test was more powerful when the large sample(s) had the smaller variance, whereas 
Welch's t test was more powerful when the small sample(s) had the smaller variance. (NOTE: add an exception for contrasts)
    
    Student's t test had the greatest power over 
Welch's when one sample size was twice the other and the large 
group(s) had the small variance. These are the same conditions where Student's t 
test more than doubled the expected false positive rate. So the advantage of Student's t test
in power is undermined by its inflated false 
positive rate. In contrast, Welch's t test was more 
powerful when the small group had the smaller variance, but it never inflated the false positive rate. 
These joint results, on the false positive rate and power, support the decision to always use Welch's t test. By using 
Welch's t test, you sometimes have greater power to detect true effects, but 
you don't have to worry about sacrificing concerns with false positives.  


\subsubsection{Coverage Probability}
Next, we looked at how well ANOVA and Welch's t test estimated the true effect. 
We used 95\% confidence intervals constructed using Student's and Welch's t 
tests to find their relative coverage probability, the proportion of confidence intervals 
that contain the population value of the estimated parameter, which in this case is the 
difference in group means. By 
definition,  the expected coverage probability of 95\% confidence intervals  is .95. 
% Because the coverage 
% probability of a confidence interval is not influenced by the effect size, we 
% only show the coverage probabilities when the null hypothesis is true. 
Additionally, when the null hypothesis is true and $\alpha = .05$, the coverage 
probability of 95\% confidence intervals has a simple relationship with the 
false positive rate---it's the complement of the false positive rate. 

Figure 4 displays the coverage probabilities of the two t tests. 
The coverage probability for Student's t test varies dramatically, just as the 
false positive rate did.  When Student's t test is the most 
powerful, it is also the least accurate at estimating the difference in means, and what you would believe is a 95\% 
confidence interval drops to as low as an 87\% confidence interval in reality. 
In contrast, Welch's t test performs as expected and the confidence interval 
contains the true effect 95\% of the time regardless of the variance and sample 
size ratios. 

<<ClassicCoverage, echo=FALSE>>=
#rnorm(50)
@

\begin{figure}[!ht]
<<CoveragePlots, echo = FALSE, fig = TRUE, width = 8, height = 11>>=

# difference in power
crossover_sims %>% 
  ungroup() %>% 
  select(-coverage_student, -coverage_welch) %>% 
  mutate(diff = reject_welch - reject_student) %>% 
  gather(key = test, value = reject_rate, reject_student, reject_welch, diff) %>% 
  mutate(
    test = case_when(
      test == 'reject_student' ~ "Student's t test",
      test == 'reject_welch' ~ "Welch's t test",
      test == 'diff' ~ 'Welch - Student'
    ),
    ns = paste(min_sample, min_sample * sample_ratio, min_sample * sample_ratio, min_sample * sample_ratio, sep = ', ') %>% factor(),
    var_ratio = factor(var_ratio),
    sample_ratio = factor(sample_ratio)
  ) %>% 
  filter(
    test == 'Welch - Student',
    min_sample == 50,
    !str_detect(contrast_names, 'ME')
  ) %>% 
  ggplot(aes(y = reject_rate, x = var_ratio, group = ns)) +
  geom_line() +
  facet_grid(contrast_names ~ test) +
  theme_classic() +
  geom_hline(aes(yintercept = 0), color = 'gray80')

# coverage
crossover_sims %>% 
  ungroup() %>% 
  select(-reject_student, -reject_welch) %>% 
  mutate(diff = coverage_welch - coverage_student) %>% 
  gather(key = test, value = coverage, coverage_welch, coverage_student, diff) %>% 
  mutate(
    test = case_when(
      test == 'coverage_student' ~ "Student's t test",
      test == 'coverage_welch' ~ "Welch's t test",
      test == 'diff' ~ 'Welch - Student'
    ),
    ns = paste(min_sample, min_sample * sample_ratio, min_sample * sample_ratio, min_sample * sample_ratio, sep = ', ') %>% factor(),
    var_ratio = factor(var_ratio),
    sample_ratio = factor(sample_ratio)
  ) %>% 
  filter(
    test != 'Welch - Student',
    min_sample == 50
  ) %>% 
  ggplot(aes(y = coverage, x = var_ratio, group = ns)) +
  geom_hline(aes(yintercept = .95), color = 'gray80') +
  geom_line(aes(group = ns)) +
  facet_grid(contrast_names ~ test) +
  theme_classic()

@

\textit{Figure 6.} Difference in power for a crossover interaction.
\end{figure}




\section{Discussion}

- add that a benefit of using Welch-based contrasts is also that you can go straight to the specific contrasts of interest instead of starting with main effects and interactions, especially in more complex designs


    Data analysis involves decisions about which statistical test answers the research question, whether the test is 
suitable to the data, and whether there is a more appropriate test. Recent discussions of research practices in psychology 
\cite<e.g.,>{Fiedler2012, Simmons2011, 
Wagenmakers2012} highlight the tension between two valued outcomes of these 
decisions. On the one hand, researchers don't want to mistakenly 
claim there is a true effect where none exists, which involves a 
concern with false positives. On the other hand, researchers want to find 
effects where they do exist, which involves a concern with power. In addition,
there is a growing concern with estimating effect sizes 
\cite{Bakker2012, Cumming2014, Ioannidis2008}. 
Researchers must find a way to balance these concerns as they make their decisions. 

The recent push for pre-registration forces researchers to make these decisions prior to data collection by specifying how they plan to test their hypothesis (e.g., an analysis of variance). But they should also address the assumptions of those tests (e.g., equal variances), how they will check if those assumptions are violated (e.g., a significance test of the group variances), and what they will do in that case (e.g., use a different test).  Such planning can be good for science, but it can easily become overwhelming to write a long list of conditional statements: ``if I see this, then I'll do that.''  Preregistration can be simplified if the researcher commits to a statistical procedure that is robust to violations of assumptions, and all the better if the procedure doesn't introduce new costs, such as lower power. We propose that when researchers compare the means of independent groups, they should commit to Welch's t test, which is robust when variances are unequal, instead of ANOVA. 
Throughout the paper we assume that the null hypothesis of for the population mean difference is zero.

Due to these differences in the standard errors and degrees of freedom, Student's and Welch's t tests can disagree about whether the group means are different. Because Welch's t test decreases the degrees of freedom, it might be more conservative. Welch's t test might be the better choice if it finds fewer false positives when the variances are unequal. But it might be the worse choice if it's too conservative and isn't powerful enough to detect true effects. 


        
    However, Welch's t test might not always be more conservative. The power of 
the two tests is based on not only the degrees of freedom, but also the 
standard error.  Welch's t test could be more powerful than 
Student's when its standard error is smaller.
    
    How do you decide which test to use? The typical approach is to use 
Student's t test unless you have evidence that the variances are unequal. The challenge is how to find that evidence. 
Our goal is to find a simple strategy to decide which test to use. We discuss two decision strategies in the main text---using a statistical test of the equal variances assumption and visually examining the data with boxplots---and one decision strategy in the supplemental materials---comparing the degrees of freedom of the two tests. Then we use Monte Carlo simulations to empirically determine when each test performs best.

    Before we proceed, we note that aside from Welch's t test, there have been Bayesian approaches 
to comparing groups with equal or unequal variances \cite{Box1973, Kruschke2013}. Our purpose is to inform decisions about how to use 
frequentist approaches, so we do not discuss
Bayesian approaches further.

\subsection{Testing Assumptions with Another Test}
    One way to decide which test to use
is to run another test of the null hypothesis that the group variances 
are equal. If the test retains the null, the variances are equal and you can use Student's t test. If the test rejects the null, the variances are unequal and you should use Welch's t test.

One example of this approach is Levene's test for homogeneity \cite{Levene1960}, which appears in SPSS by default when you run an independent samples t test.
In its original formulation, the test finds how far each observation deviates from the mean of its group, then it uses ANOVA to test whether the average deviations are different between the groups (later formulations use deviations from the group median or the trimmed mean, which work better when the raw data are not normally distributed, \citeNP{Brown1974}).

Consider the logic of using Levene's test to decide whether to use Student's or Welch's t 
test. You use an ANOVA to test 
whether the deviations from the group means are different.
When there are only two groups, this is equivalent to using a t test, which means you have to make another choice
between Student and Welch, which means you must decide whether the
group \textit{deviations} have equal variances.
This doesn't solve the original problem, it just hides it in a different test. 

Additionally, tests of assumptions,
like other tests, are sensitive 
to sample size \cite{Gonzalez2008}. If your sample is too small, you won't have enough power to 
detect true differences in the variances. If your sample is very 
large, even minute differences will be statistically significant. 

Overall, using tests of equal variances is an ineffective decision strategy \cite{Zimmerman1996,Zimmerman2004}. 

\subsection{Visualizing Data with Boxplots} 

    A second way to decide which test to use is to visualize the data using boxplots and judge 
whether the variances appear to differ.  With smaller samples, you 
can tolerate larger apparent differences and still conclude the variances are equal. You can enhance this strategy by 
simulating data that have the same sample sizes as the real data, changing whether the variances are equal or not, 
and checking if the boxplots from the real data look more like the boxplots of 
simulations with equal variances or unequal variances. 

    Figure 1 displays sample boxplots where two groups have equal 
variances (top) and where one group has five times the variance of the other (bottom), and where each group has $n$ = 20 (first rows) or $n$ = 
100 (second rows). 

  With smaller 
sample sizes it's typical to see 
variability in the spread of the groups. When the population 
variances are equal,
sometimes the spread of the groups appears almost equal (as it should), 
and other times it appears to differ. When the population variances are unequal, the group with the larger variance (to the right) is generally more spread out, but sometimes the spread of the groups looks similar.

In contrast, with larger sample sizes there is more 
consistency in the boxplots. Smaller differences in the spread of the groups 
might be a sign that the population variances differ. Using boxplots
to decide which test to use
is a viable strategy, though it requires some subjective judgment,
especially when sample sizes are small.

   
\begin{figure}[!ht]
<<boxplots, echo = FALSE, fig = TRUE, height = 8>>=

# Boxplots (equal variances) ----

# distributions
x1 <- seq(0, 12, .1)
x2 <- seq(1.13, 13.13, .1)

ve_dist_data <- data.frame(
  x = c(x1, x2),
  y = c(dnorm(x1, mean = 6, sd = sqrt(2)), dnorm(x2, mean = 7.13, sd = sqrt(2))),
  group = factor(rep(c(1, 2), each = length(x1)))
)

ve_distributions <- ggplot(ve_dist_data, aes(x = x, y = y, linetype = group)) +
  geom_line(show.legend = FALSE) +
  labs(y = NULL, x = NULL) +
  theme(
    axis.text = element_blank(),
    axis.ticks = element_blank(),
    panel.grid = element_blank()
  )

# boxplots
set.seed(2184)
ve_bplots_n20 <- lapply(1:4, function(x) {
                 plot_data <- data.frame(
                   group = factor(rep(c(1, 2), each = 20)),
                   y = c(
                     rnorm(n = 20, mean = 6, sd = sqrt(2)),
                     rnorm(n = 20, mean = 7.13, sd = sqrt(2))
                   )
                 )
                 
                 ggplot(plot_data, aes(x = group, y = y, linetype = group)) +
                   geom_boxplot(show.legend = FALSE) +
                   labs(x = NULL, y = NULL) +
                   theme(
                     axis.text = element_blank(),
                     axis.ticks = element_blank(),
                     panel.grid = element_blank()
                   )
})

ve_bplots_n100 <- lapply(1:4, function(x) {
                 plot_data <- data.frame(
                   group = factor(rep(c(1, 2), each = 100)),
                   y = c(
                     rnorm(n = 100, mean = 6, sd = sqrt(2)),
                     rnorm(n = 100, mean = 7.13, sd = sqrt(2))
                   )
                 )
                 
                 ggplot(plot_data, aes(x = group, y = y, linetype = group)) +
                   geom_boxplot(show.legend = FALSE) +
                   labs(x = NULL, y = NULL) +
                   theme(
                     axis.text = element_blank(),
                     axis.ticks = element_blank(),
                     panel.grid = element_blank()
                   )
})



# Boxplots (unequal variances) ----

# distributions
x1 <- seq(0, 12, .1)
x2 <- seq(1.13, 13.13, .1)

vun_dist_data <- data.frame(
  x = c(x1, x2),
  y = c(dnorm(x1, mean = 6, sd = sqrt(2)), dnorm(x2, mean = 7.13, sd = sqrt(10))),
  group = factor(rep(c(1, 2), each = length(x1)))
)

vun_distributions <- ggplot(vun_dist_data, aes(x = x, y = y, linetype = group)) +
  geom_line(show.legend = FALSE) +
  labs(y = NULL, x = NULL) +
  theme(
    axis.text = element_blank(),
    axis.ticks = element_blank(),
    panel.grid = element_blank()
  )

# boxplots
set.seed(2184)
vun_bplots_n20 <- lapply(1:4, function(x) {
                 plot_data <- data.frame(
                   group = factor(rep(c(1, 2), each = 20)),
                   y = c(
                     rnorm(n = 20, mean = 6, sd = sqrt(2)),
                     rnorm(n = 20, mean = 7.13, sd = sqrt(10))
                   ) 
                 )
                 
                 ggplot(plot_data, aes(x = group, y = y, linetype = group)) +
                   geom_boxplot(show.legend = FALSE) +
                   labs(x = NULL, y = NULL) +
                   theme(
                     axis.text = element_blank(),
                     axis.ticks = element_blank(),
                     panel.grid = element_blank()
                   )
})

vun_bplots_n100 <- lapply(1:4, function(x) {
                 plot_data <- data.frame(
                   group = factor(rep(c(1, 2), each = 100)),
                   y = c(
                     rnorm(n = 100, mean = 6, sd = sqrt(2)),
                     rnorm(n = 100, mean = 7.13, sd = sqrt(10))
                   )
                 )
                 
                 ggplot(plot_data, aes(x = group, y = y, linetype = group)) +
                   geom_boxplot(show.legend = FALSE) +
                   labs(x = NULL, y = NULL) +
                   theme(
                     axis.text = element_blank(),
                     axis.ticks = element_blank()
                   )
})

# Print boxplots ---- 

# sample size labels
label_20 <- ggplot() + 
  geom_text(aes(x = 50, y = 50, label = "italic('n')*s == 20"), size = 4, parse = TRUE) + 
  coord_cartesian(ylim = c(40, 60), xlim = c(40, 60)) + 
  theme(
    line = element_blank(), 
    rect = element_blank(), 
    text = element_blank()
    )

label_100 <- ggplot() + 
  geom_text(aes(x = 50, y = 50, label = "italic('n')*s == 100"), size = 4, parse = TRUE) + 
  coord_cartesian(ylim = c(40, 60), xlim = c(40, 60)) + 
  theme(
    line = element_blank(), 
    rect = element_blank(), 
    text = element_blank()
    )

# prep for printing
ve_distributions <- plot_grid(NULL, NULL, ve_distributions, NULL, rel_widths = c(2, 3, 6, 3), nrow = 1)
ve_bplots <- plot_grid(
  plotlist = c(
    append(ve_bplots_n20, list(label_20), after = 0),
    append(ve_bplots_n100, list(label_100), after = 0)
    ),
  nrow = 2,
  rel_widths = c(2, 3, 3, 3, 3)
  )
vun_distributions <- plot_grid(NULL, NULL, vun_distributions, NULL, rel_widths = c(2, 3, 6, 3), nrow = 1)
vun_bplots <- plot_grid(
  plotlist = c(
    append(vun_bplots_n20, list(label_20), after = 0),
    append(vun_bplots_n100, list(label_100), after = 0)
    ),
  nrow = 2,
  rel_widths = c(2, 3, 3, 3, 3)
  )


# combine into one grid
full_bplots <- plot_grid(ve_distributions, ve_bplots, vun_distributions, vun_bplots, nrow = 4, rel_heights = c(2, 3, 2, 3))

full_bplots
@

\textit{Figure 1.} Boxplots from simulations of groups with equal variances (top) and unequal 
variances (bottom).
\end{figure}



Some prior research has examined the 
false positive rate \cite{Boneau1960, Zimmerman1993, 
Zimmerman2004, Zimmerman1996, Zimmerman2009} and
power of the two tests when comparing two groups \cite{Neuhauser2002, 
Zimmerman1993}, though not with the complete configuration of 
conditions that we examined. Nevertheless, it will be informative to display 
the false positives and power of the two tests here. We also discuss 
implications for estimating effect sizes, which follows from the false positive
results but has not, to our knowledge, been discussed explicitly in past 
research.


    We set out to find a simple rule to help researchers decide when to use 
Student's t or Welch's t test. We believe the simplest 
rule is to always use Welch's t test to compare the means of independent groups, whether there are two groups or more.
    
    The simulations demonstrated that when the population 
variances or sample sizes were equal, using Welch's t test instead of Student's 
didn't hurt. Though the degrees of freedom from Welch's t test can 
drop substantially below Student's when only the variances 
or only the sample sizes are unequal (see Figure S1), the simulated false positive rates, 
power, and coverage probabilities were almost identical under 
these conditions. The difference in the degrees of freedom had a 
negligible effect on the outcomes. 
    
    More important was the standard error, which 
affects the t-value. When either the variances or sample sizes are 
equal, the pooled standard error of Student's t test and the separate variances 
standard error of Welch's t test are identical, and the two tests will 
generally agree with each other. However, when both the variances and the 
sample sizes are unequal, the pooled standard error of Student's t test gives 
more weight to the larger group \cite<e.g.,>{Coombs1996,Zimmerman2009}; if that group has the larger 
variance, Student's t test becomes more conservative, but if it
has the smaller variance, Student's t test becomes more liberal. This is why the false positive 
rate and coverage probability varied widely for Student's t test. In 
contrast, Welch's t test was more stable, regardless of which group had the 
larger variance. 
    
    The biggest benefit of Student's t test was that it had more power when the larger sample had the smaller 
variance---yet under these same conditions, it had an inflated false positive
rate and the lowest coverage probability. Far from being underpowered, Welch's 
t test was more powerful when the larger sample had the 
larger variance, but it retained the expected false positive
rate and coverage probability. Overall, Welch's t test did a better job 
of balancing concerns with false positives, power, and estimation.
    
    We believe researchers prefer simple decision rules, 
and so we echo others' recommendations to always use 
Welch's t test \cite{Zimmerman1996,Moser1992,Moser1989}. For researchers who 
insist on using Student's t test, there is some 
good news. If they run experiments, subjects are usually assigned evenly to 
conditions, and when sample sizes are equal the two tests perform equally well. 
However, if their research compares pre-existing groups, it 
might not be possible to have equal sample sizes. Welch's t test is likely to 
outperform Student's unless the variances are 
equal. One could use boxplots to 
determine whether it is reasonable to assume equal variances, which will be easier if the sample sizes are large. 
Nevertheless, researchers should rest assured that they won't suffer from choosing 
Welch's t test as the default.
    
    Prior simulations that compared Student's and Welch's t tests 
focused on null hypothesis significance testing by emphasizing false positives
and power \cite{Boneau1960, Neuhauser2002, Zimmerman1993, Zimmerman2004, 
Zimmerman1996, Zimmerman2009}, but there are important implications of this work
for effect size estimation. Some have called for researchers to report 
effect sizes and confidence intervals to address limitations of 
merely reporting significance tests. It is important to remember 
that effect size estimation also involves assumptions about group variances.
We found that constructing confidence intervals for the difference in group means with Student's t test,
which assumes equal variances, led to 
 unstable estimates. Under some conditions, the confidence intervals were 
less accurate than expected because they were too narrow, and under other 
conditions, they were more accurate than expected because they were too wide. 
Using Welch's t test, which does not assume equal variances, led 
to more stable estimates across conditions.
    
    Cohen's d \cite{Cohen1992} is the most popular effect size for reporting 
the difference in group means, but it assumes equal variances. Cohen's d standardizes the difference in means based on a 
common standard deviation of the population. This common standard deviation
is just the square root of the pooled variance from Student's t test.
But when the group variances are unequal, there is no common standard deviation. 
Cohen's d will suffer from the same problems as Student's t test. Given the same difference in group 
means, if the sample sizes and variances both differ, then Cohen's d will give more 
weight to the larger sample when pooling the variance. If the larger sample has the larger variance, the 
standardized effect size will be smaller than if the larger sample 
has the smaller variance. In reality, either estimate is misleading
because there is no common standard deviation, so there can be no traditional Cohen's d.
    
    Standardized effect sizes such as Cohen's d are often desirable for their 
use in meta-analysis. But the equal variances 
problem applies to meta-analysis as well. Using Cohen's d as the basis for a 
meta-analysis involves an assumption that the group variances across 
the body of research are equal, an assumption which might be untenable. Differences in variances might not be just a nuisance, but rather an interesting part of the effect for meta-analysts to examine. The effect of the independent variable may be on the variances and not merely the means.
    
    The good news is that raw difference in means are also effect sizes
\cite{Cumming2014, Gonzalez2008, Kelley2012}, and you can find confidence intervals around raw differences 
in means without assuming equal variances. In fact, 95\% confidence intervals
based on Welch's t test appear in the 
default output of programs such as SPSS and R. Reporting 
descriptive statistics in their original scale might be a better practice than 
reporting only standardized effect sizes anyway. First, unlike Cohen's d, reporting raw descriptive statistics does not require the researcher to commit to an 
equal variances assumption. Second, it provides all of the necessary 
information for others who want to assume equal variances to find Cohen's d or its alternatives \cite{Peng2013, 
Grissom2001}. Third, it allows other researchers to examine whether differences 
in group variances are a consistent part of an effect, which would be lost by 
just reporting the standardized difference.

    We suspect that most statistics courses in psychology thoroughly teach Student's t 
test and only briefly touch on Welch's t test, if they teach it at 
all. Indeed, we have heard colleagues complain that when they use Welch's t test in a 
manuscript, reviewers are suspicious of the 
degrees of freedom with decimals. These reviewers must not have learned that 
degrees of freedom with decimals are the norm for Welch's t test and related methods such as corrections used for assumptions in repeated measures ANOVA \cite{Gonzalez2008}. The emphasis 
on Student's t test in teaching is consistent with the strategy of assuming equal variances and only using Welch's t test if it appears the assumption has been violated. But why should we spend so much 
time on the equal variances assumption in the first place? Why not teach 
Welch's t test at the outset without imposing 
restrictive assumptions? Student's t test could be taught briefly so  
students understand the existing literature, but we believe it would be 
beneficial to emphasize Welch's t test as the default approach. As demonstrated 
in our simulations, this approach will lead to better decision-making when 
it comes to analyzing data. 

Our discussion began with the relatively simple case of testing the means from two independent groups with normally distributed data.
But our simulations demonstrate that the implications generalize to more complex designs, such as interactions in a 2 x 2 factorial design.
For between-groups contrasts, with two groups or more, we arrived at a simple conclusion: on balance, when considering the false positive rate, power, and effect size estimation, an efficient strategy is to always uses Welch's t test.  In the Appendix, we provide the syntax for an R function that uses Welch's t test for any set of between-groups contrasts, including main effects and interactions. As the study design becomes more complicated, such as with repeated measures, random effect models, or non-normally distributed data, the story will likely be more complicated. As a field we should move toward analysis strategies that have fewer moving parts so they can be described easily in a scientific report or preregistration plan, and  provide robust estimates.

\section{Author Contributions}

JDW wrote the code for the simulations, wrote the first draft of the manuscript. RG advised JDW with coding and contributed independent writing to the manuscript. Both authors revised the manuscript and wrote the function in the Appendix. 



\bibliography{bibliography}
\bibliographystyle{apacite}

\appendix
\section{Welch t test contrasts for R}

In R, you can use the following function to compute group contrasts based on Welch's t test. There is an example of how to use it below.

\subsection{Arguments}
\begin{small}
\begin{singlespace}
\begin{lstlisting}
welch_contrast(data = NULL, dv, groups, ...)

data: The dataframe where the dependent variable and groups are stored (optional)
dv: The dependent variable
groups: The group variable
...: Any number of vectors with numeric contrasts, separated by commas, e.g., c(1, -1, 0, 0), c(-1, 0, 1, 0)
\end{lstlisting}
\end{singlespace}
\end{small}

\subsection{Function (run this syntax in R to access the function)}

\begin{small}
\begin{singlespace}
\begin{lstlisting}
welch_contrast <- function(..., data = NULL, dv, groups) {
  
  # assign dv and groups if they're in a dataframe
  if(!is.null(data)) {
    dv <- eval(substitute(dv), data)
    groups <- eval(substitute(groups), data)
  }
  
  # make sure contrasts are numeric
  if (!is.numeric(c(...))) {
    return('All contrasts must be numeric')
  }
  
  # make sure length of contrasts is correct
    contrast_lengths <- lapply(list(...), length)
    unique_lengths <- unique(contrast_lengths)
  
    # are contrasts the same length as each other?
    if(length(unique_lengths) != 1) { 
      return('All contrasts must be the same length')
    }
    
    # are contrasts the same length as the number of groups?
    if (unique_lengths != length(unique(groups))) { 
      return(
        cat(
          'Contrasts must be the same length as the number of groups: ', 
          length(unique(groups)), 
          '. \n\nIf this number seems too large, you might have missing data (NA) in your groups.', 
          sep = ''
          )
      )
    }
    
  # compute group stats
  means <- by(dv, groups, mean)
  vars <- by(dv, groups, var)
  Ns <- by(dv, groups, length)
  
  # build contrast matrix and compute contrast values
  contrast <- matrix(c(...), nrow = length(list(...)), byrow = TRUE)
  colnames(contrast) <- names(means)
  rownames(contrast) <- paste0('Contrast ', 1:length(list(...)))
  ihat <- contrast %*% means
  
  # t test
  df_welch <- (contrast^2 %*% (vars / Ns))^2 / (contrast^2 %*% (vars^2 / (Ns^2 * (Ns - 1))))
  se_welch <- sqrt(contrast^2 %*% (vars / Ns))
  t_welch <- ihat/se_welch
  p_welch <- 2*(1 - pt(abs(t_welch), df_welch))
  ci_welch <- qt(.025, df = df_welch)
  lb_welch <- ihat - ci_welch * se_welch
  ub_welch <- ihat + ci_welch * se_welch
  
  # store t test results in a data frame
  t_test <- data.frame(t = t_welch,
                       df = df_welch,
                       p = p_welch,
                       lb_95CI = lb_welch,
                       ub_95CI = ub_welch
  )
  t_test <- round(t_test, digits = 3)
  
  # combine it with the contrast matrix
  output <- cbind(contrast, t_test)
  
  return(output)
}
\end{lstlisting}

<<contrast_function, echo = FALSE>>=
welch_contrast <- function(..., data = NULL, dv, groups) {
  
  # assign dv and groups if they're in a dataframe
  if(!is.null(data)) {
    dv <- eval(substitute(dv), data)
    groups <- eval(substitute(groups), data)
  }
  
  # make sure contrasts are numeric
  if (!is.numeric(c(...))) {
    return('All contrasts must be numeric')
  }
  
  # make sure length of contrasts is correct
    contrast_lengths <- lapply(list(...), length)
    unique_lengths <- unique(contrast_lengths)
  
    # are contrasts the same length as each other?
    if(length(unique_lengths) != 1) { 
      return('All contrasts must be the same length')
    }
    
    # are contrasts the same length as the number of groups?
    if (unique_lengths != length(unique(groups))) { 
      return(
        cat(
          'Contrasts must be the same length as the number of groups: ', 
          length(unique(groups)), 
          '. \n\nIf this number seems too large, you might have missing data (NA) in your groups.', 
          sep = ''
          )
      )
    }
    
  # compute group stats
  means <- by(dv, groups, mean)
  vars <- by(dv, groups, var)
  Ns <- by(dv, groups, length)
  
  # build contrast matrix and compute contrast values
  contrast <- matrix(c(...), nrow = length(list(...)), byrow = TRUE)
  colnames(contrast) <- names(means)
  rownames(contrast) <- paste0('Contrast ', 1:length(list(...)))
  ihat <- contrast %*% means
  
  # t test
  df_welch <- (contrast^2 %*% (vars / Ns))^2 / (contrast^2 %*% (vars^2 / (Ns^2 * (Ns - 1))))
  se_welch <- sqrt(contrast^2 %*% (vars / Ns))
  t_welch <- ihat/se_welch
  p_welch <- 2*(1 - pt(abs(t_welch), df_welch))
  ci_welch <- qt(.025, df = df_welch)
  lb_welch <- ihat - ci_welch * se_welch
  ub_welch <- ihat + ci_welch * se_welch
  
  # store t test results in a data frame
  t_test <- data.frame(t = t_welch,
                       df = df_welch,
                       p = p_welch,
                       lb_95CI = lb_welch,
                       ub_95CI = ub_welch
  )
  t_test <- round(t_test, digits = 3)
  
  # combine it with the contrast matrix
  output <- cbind(contrast, t_test)
  
  return(output)
}
@
\end{singlespace}
\end{small}

\subsection{Examples}
\begin{small}
\begin{singlespace}
\begin{lstlisting}
# generate data
set.seed(123)
sample_data <- data.frame(
  y = sample(1:7, size = 200, replace = TRUE),
  groups = rep(c('A', 'B', 'C', 'D'), each = 50)
  )

# Example 1  
# specify data argument
# one contrast
welch_contrast(data = sample_data, 
               dv = y, 
               groups = groups, 
               c(-1, 1, 0, 0)
               )
\end{lstlisting}
<<contrast_example1, echo = FALSE, size = 'tiny'>>=
## Examples
# generate example data
set.seed(123)
sample_data <- data.frame(
  y = sample(1:7, size = 200, replace = TRUE),
  groups = rep(c('A', 'B', 'C', 'D'), each = 50)
  )
  
welch_contrast(data = sample_data, 
               dv = y, 
               groups = groups, 
               c(-1, 1, 0, 0)
               )
@
\begin{lstlisting}
# Example 2
# don't specify data argument
# three contrasts
welch_contrast(dv = sample_data$y, 
               groups = sample_data$groups, 
               c(-1, 1, 0, 0),
               c(-1, 0, 1, 0),
               c(-1, 0, 0, 1)
               )
\end{lstlisting}
<<contrast_example2, echo = FALSE>>=
welch_contrast(dv = sample_data$y, 
               groups = sample_data$groups, 
               c(-1, 1, 0, 0),
               c(-1, 0, 1, 0),
               c(-1, 0, 0, 1)
               )
@
\end{singlespace}
\end{small}


\end{document}