%floatsintext can be used in man to have figs appear where they are called
\documentclass[man, noextraspace, apacite, floatsintext]{apa6}
%following looks like a journal printout; need to comment out previous line
%\documentclass[jou,noextraspace,apacite]{apa6}
\usepackage{apacite, setspace, listings, amsmath}
\usepackage[retainorgcmds]{IEEEtrantools}
\lstset{
basicstyle=\small\ttfamily,
columns=flexible,
breaklines=true
}

\title{Use Welch's \textit{t} test to Compare the Means of Independent Groups}
\shorttitle{Welch t Test}

\twoauthors{Joshua D. Wondra}{Richard Gonzalez}
\twoaffiliations{Facebook}{University of Michigan}

\abstract{Recently, Delacre, Lakens, and Leys (2017) recommended that psychologists should use Welch's t test instead of Student's t test as the default when comparing means between groups.
We agree with this conclusion, but see a need to expand on the evidence to justify it, which we do through Monte Carlo simulations. 
First, past research has focused on research designs with two groups; we show how Welch's t test can be used in designs with more than two groups to test any one degree of freedom contrast.
Second, the justification for using Welch's t test emphasizes control of false positives, and minimizes the concerns researchers have with loss of power; we show that this is unnecessary, because Welch's t test is more powerful than Student's in common situations with no serious downsides.
Third, no attention has been paid to estimates of effect size; we show that 
Welch's t test is more reliable at estimating effect sizes regardless of the false positive rates or power of the two tests. 
}
\keywords{Welch t test, contrasts, hypothesis testing, effect size}

\authornote{Joshua D. Wondra, Facebook.

Richard Gonzalez, Department of Psychology, University of Michigan.

This research was conducted while Josh Wondra was a graduate student at the University of Michigan.

Correspondence concerning this article should be addressed to Josh Wondra, 
Facebook, 770 Broadway, New York, NY, 10003.

Contact: jdwondra@umich.edu}


\begin{document}
\SweaveOpts{concordance=TRUE}
\maketitle
<<setup, echo=FALSE>>=

# Load packages ----
library(formattable)
library(tidyverse)
library(cowplot)

# Import custom functions ----
source('Welch functions.R')

# Import simulation results ----
load('no_effects.R')
load('crossover_sims.R')
load('me_int_sims.R')

# set sample sizes
sims_min_sample <- 30
sample_sizes <- c(
  '30, 30, 30, 30',
  '30, 45, 45, 45',
  '30, 60, 60, 60'
  )

all_data <- mutate(no_effects, dataset = 'no_effects') %>% 
  bind_rows(mutate(crossover_sims, dataset = 'crossover_sims')) %>% 
  bind_rows(mutate(me_int_sims, dataset = 'me_int_sims')) %>% 
  ungroup() %>% 
  filter(
    min_sample == sims_min_sample
  ) %>% 
  mutate(
    which_sim = case_when(
      dataset == 'no_effects' ~ 'No effects',
      dataset == 'crossover_sims' ~ 'Crossover interaction',
      dataset == 'me_int_sims' ~ 'Main effects & interaction'
    ),
    contrast_names = case_when(
      contrast_names == 'ME (1 & 2 vs 3 & 4)' ~ 'Main Effect of A',
      contrast_names == 'ME (1 & 3 vs 2 & 4)' ~ 'Main Effect of B',
      contrast_names == 'Interaction' ~ 'Interaction',
      contrast_names == 'SE (1 vs 2)' ~ 'Simple Effect of A at B1',
      contrast_names == 'SE (3 vs 4)' ~ 'Simple Effect of A at B2',
      contrast_names == 'SE (1 vs 3)' ~ 'Simple Effect of B at A1',
      contrast_names == 'SE (2 vs 4)' ~ 'Simple Effect of B at A2'
    ) %>% 
      factor(levels = c(
        'Main Effect of A', 
        'Main Effect of B', 
        'Interaction', 
        'Simple Effect of B at A1', 
        'Simple Effect of B at A2', 
        'Simple Effect of A at B1', 
        'Simple Effect of A at B2'
        )
        ),
    ns = paste(min_sample, min_sample * sample_ratio, min_sample * sample_ratio, min_sample * sample_ratio, sep = ', ') %>% factor(levels = sample_sizes),
    var_ratio = factor(var_ratio),
    sample_ratio = factor(sample_ratio)
  )

rm(list = c('no_effects', 'crossover_sims', 'me_int_sims'))

# Simulation specifications for plotting ----
sim_specs <- expand.grid(
  sim = c('No effects', 'Crossover interaction', 'Main effects & interaction'),
  which_contrast = c('Main Effect of A', 'Main Effect of B', 'Interaction', 'Simple Effect of B at A1', 'Simple Effect of B at A2', 'Simple Effect of A at B1', 'Simple Effect of A at B2'),
  which_test = c("ANOVA", "Welch's t test")
)

@


<<create_plots, echo = FALSE>>=

# Plot true effects ----

# generate the data
true_effect_data <- data.frame(
  effect = c('No effects', 'Crossover interaction', 'Main effects & interaction') %>% 
    rep(each = 4) %>% 
    factor(levels = c('No effects', 'Crossover interaction', 'Main effects & interaction')),
  y = c(
    6, 6, 6, 6, # no effect
    6.71, 6, 6, 6.71, # crossover
    6.71, 6, 6.28, 6 # main effect + interaction
    ),
  factorA = rep(c('A1', 'A2', 'A1', 'A2'), times = 3),
  factorB = rep(c('B1', 'B1', 'B2', 'B2'), times = 3)
) %>% 
  mutate(
    factorB_label_position = case_when(
      factorA == 'A2' ~ as.numeric(NA),
      factorB == 'B1' ~ y + .1,
      factorB == 'B2' ~ y - .1
      )
  )

# create the plots
no_effects_true_effect_plot <- true_effect_plot(true_effect_data, 'No effects')
crossover_true_effect_plot <- true_effect_plot(true_effect_data, 'Crossover interaction')
me_int_true_effect_plot <- true_effect_plot(true_effect_data, 'Main effects & interaction')





# Plot reject null rates ----

## subset data
reject_null_data <- all_data %>% 
  select(-starts_with('coverage_'), -starts_with('joint_')) %>% 
  gather(key = test, value = reject_rate, reject_student, reject_welch) %>% 
  mutate(
    test = case_when(
      test == 'reject_student' ~ "ANOVA",
      test == 'reject_welch' ~ "Welch's t test"
    )
  )

## plot the rejection rates
apply(
  sim_specs, 
  MARGIN = 1, 
  generate_plots,
  which_plot = 'reject_null_plot'
  ) %>% 
  invisible()








# Plot joint false positives ----

## subset data
joint_reject_data <- all_data %>% 
  filter(dataset == 'no_effects') %>% 
  ungroup() %>% 
  filter(min_sample == 30) %>% 
  distinct(sample_ratio, min_sample, effect, ns, var_ratio, joint_reject_student, joint_reject_welch)

which_test <- c('student', "welch")
for(i in 1:length(which_test)) {
  
  plot_name = paste('joint_reject', which_test[i], sep = '_')
  if (which_test[i] == 'student') {
    joint_reject_data$test = 'ANOVA'
  } else if (which_test[i] == 'welch'){
    joint_reject_data$test = "Welch's t test"
  }
  
  ylims = c(0, .3)
  ybreaks = seq(0, 1, .05)
  ylabs = seq(0, 100, 5) %>% paste0('%')
  
  this_plot = joint_reject_data %>% 
    mutate(
      contrast_names = 'Both Main Effects & Interaction'
    ) %>% 
    ggplot(aes(y = !!sym(plot_name), x = var_ratio, group = ns, linetype = ns, shape = ns)) +
    facet_grid(rows = vars(test))
  
  this_plot = format_plot(this_plot, ylims, ybreaks, ylabs)
  
  assign(plot_name, this_plot, envir = .GlobalEnv)
}


# Plot difference in reject rates ====
apply(
  sim_specs %>% 
    select(-which_test) %>% 
    distinct(), 
  MARGIN = 1, 
  generate_plots,
  which_plot = 'reject_difference_plot'
  ) %>% 
  invisible()





# Plot coverage probability ----

## subset data
cov_data <- all_data %>% 
  select(-reject_student, -reject_welch, -joint_reject_student, -joint_reject_welch) %>% 
  gather(key = test, value = coverage_rate, coverage_student, coverage_welch) %>% 
  mutate(
    test = case_when(
      test == 'coverage_student' ~ "ANOVA",
      test == 'coverage_welch' ~ "Welch's t test"
    )
  )

## generate plots
apply(
  sim_specs, 
  MARGIN = 1, 
  generate_plots,
  which_plot = 'coverage_plot'
  ) %>% 
  invisible()



# Plot legend ----
## even though this is based on a reject null plot, it's the same for all plots
plot_legend <- (rn_no_effects_meA_anova + guides(shape = 'legend')) %>% 
  get_legend() %>% 
  plot_grid()

# Plot x label ----
plot_x_lab <- bquote(
    frac(
      sigma[A[1]*B[1]]^2,
      {sigma[A[1]*B[2]]^2 == sigma[A[2]*B[1]]^2} == sigma[A[2]*B[2]]^2
      )
    )

@

    When psychologists compare the means of two groups, they typically use Student's t test \cite{Student1908}, which assumes the group variances are equal. If there is reason to believe variances are unequal, they can use Welch's t test instead \cite{Welch1938, 
Satterthwaite1946}, which does not assume the variances are equal.

    Recently, \citeA{Delacre2017} argued that psychologists should change the typical approach, and that they should use Welch's t test  by default, 
because it does a better job controlling false positives when group variances are unequal, 
and it doesn't lose much power when variances are equal.
We agree with the conclusion to use Welch's t test by default, but see a need to expand on the evidence to justify it.

First, prior research has focused on research designs with only two groups. This is a fair starting point, because it's the prototypical case where t tests are used. 
However, in designs with more than two groups, t tests are used frequently. 
Most commonly, t tests are used to test differences between specific pairs of groups, or specific combinations of groups--this is commonly referred to as "planned contrasts". 
These contrasts are important because they directly test the hypotheses that psychologists care about most, more so than the omnibus ANOVAs that are commonly used as a first step in the analysis (more on this later).
Aside from contrasts, in factorial designs with two levels per factor, t tests can be used instead of ANOVA to test the main effects and interactions.  We illustrate both contrasts and main effects/interactions in a 2~$\times$~2 factorial design.

Second, the main justification for using Welch's t test has been that it does a better job controlling false positives, and that it has only a little less power to detect true effects. But because researchers typically hope to see significant effects, the loss of power may loom larger than the gain of lower false positive rates, and discourage researchers from using Welch's t test. A stronger justification would more thoroughly address concerns with power. We show that Welch's t test can be much more powerful than Student's t test without serious downsides, and that when Student's t test is more powerful it comes with risks. 

Third, prior research has focused on null hypothesis reject rates, and little attention has been paid to estimates of effect size. This is a different and important topic to consider, because even when one test is more powerful than the other, it can get the effect size wrong. Effect size estimation is related to false positive rates, but deserves discussion on its own. We show that Welch's t test is more reliable at estimating the correct effect size, regardless of false positive rates or power.

Through Monte Carlo simulations of a 2~$\times$~2 factorial design, we show that Welch's t test expands to cases with more than two groups, does a better job controlling false posities than a typical analysis of variance (ANOVA) or Student's t test, is more powerful in some common situations with little downside, and more reliably estimates effect sizes, so that Welch's t test should be used by default to test any one degree of freedom contrast in a variety of research designs.

\subsection{Student's and Welch's t tests with Two Groups}

Both Student's and Welch's t tests assume the populations behind the data follow a normal distribution and that the
observations are independent of each other. For both tests, the t-value is the 
difference in group means divided by the standard error of that 
difference:   
    \begin{equation}
    t = \frac{\hat{\mu}_1-\hat{\mu}_2}{\hat{\sigma}_{\mu_1-\mu_2}}
    \end{equation}
The standard error is estimated from the group variances, by dividing each group's estimated population variance ($\hat{\sigma}_1^2$, $\hat{\sigma}_2^2$) by its sample size ($n_1$, $n_2$), summing them, and then then taking the square root of the sum.

\begin{equation}
    \hat{\sigma}_{\mu_1-\mu_2} = \sqrt{\frac{\hat{\sigma}_1^2}{n_1} + \frac{\hat{\sigma}_2^2}{n_2}}
\end{equation}

  But Student's t test and Welch's t tests make different assumptions about how to find each group's estimated population variance. 
  Student's t test assumes the population variances of the groups are equal ($\hat{\sigma}_1^2 = \hat{\sigma}_2^2$), so if the sample variances are different ($s_1^2 \neq s_2^2$), it's due to sampling error. 
  To correct for this error, Student's t test estimates a common variance
by pooling the sample variances into a weighted 
average
    \begin{equation}
    s_p^2 = \frac{(n_1 - 1)s_1^2 + (n_2 - 1)s_2^2}{(n_1 - 1) + (n_2 - 1)}
    \end{equation}
This common variance is used to estimate the population variances of the two groups, which can then be used to estimate the standard error of the t test.  
    \begin{equation}
    s_{Student} = \sqrt{\frac{s_p^2}{n_1}  + \frac{s_p^2}{n_2}} %= s_p\sqrt{\frac{1}{n_1}  + \frac{1}{n_2}} Josh W cut this because it's easier to see how the standard error from Student's and Welch's t tests compare by leaving the pooled variance under the radical
    \end{equation}
   
In the pooling equation (EQ 3), if the sample sizes are different, the variance of the group 
with the larger sample size is given more weight. If the larger group has the larger 
variance, then the standard error will be larger; if the larger group has the smaller variance, then the standard error will be smaller.
    
  In contrast Welch's t test does not assume the group variances are equal. The sample variances of the groups are used to estimates their population variances, and they are used directly to estimate the standard error of the t test:
    \begin{equation}
    s_{Welch} = \sqrt{\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2}}
    \end{equation}

  In addition to their standard errors, the two tests also differ in how they compute the degrees of 
freedom. Student's t test uses a simple formula to compute the degrees of freedom: $df_{Student}=n_1+n_2-2$. Welch's t test uses a more complicated formula to compute the degrees of freedom: 
    \begin{equation}
    df_{Welch} = \frac{(\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2})^2}{\frac{(\frac{s_1^2}{n_1})^2}{n_1-1} + \frac{(\frac{s_2^2}{n_2})^2}{n_2-1}}
    \end{equation}
The important thing to know is the formula decreases the degrees of freedom to the extent that the group
variances differ. It can also produce a degrees of freedom value that is not an integer.





\subsection{Student's and Welch's t tests with More Than Two Groups}

Our opening discussion focused on the case of two groups, but Welch's t test can generalize to any between-groups design \cite<for a discussion of omnibus tests in the context of one-way ANOVA, see>{Delacre2019}.

Let's consider the case of a 2~$\times$~2 factorial design with factors A and B. Typically researchers would use a 2~$\times$~2 factorial ANOVA to test for group differences, which would test the main effect of A, the main effect of B, and the interaction of A and B. If the group sample sizes are equal or if Type 3 Sum of Squares is used, then the ANOVA is equivalent to a set of orthogonal contrasts with Student's t test using the following weights:

\begin{table}[ht]
\begin{tabular}{l c c c c}
\hline
 & \multicolumn{4}{c}{Condition} \\
 & $A_1 B_1$ & $A_1 B_2$ & $A_2 B_1$ & $A_2 B_2$ \\
\hline
Main Effect of A & 1 & 1 & -1 & -1 \\
Main Effect of B & 1 & -1 & 1 & -1 \\
Interaction & 1 & -1 & -1 & 1 \\
\hline
\end{tabular}
\end{table}

Each contrast is the sum of the weights, \textit{a}, multiplied by their respective group means.
\begin{equation}
  \hat{I} = \sum a_i \overline{x}_i
\end{equation}

To find the t value to test the contrasts, you need to find the standard error of the contrasts. 
\begin{equation}
  t = \frac{\hat{I}}{\hat{\sigma}_{\hat{I}}}
\end{equation}

To find the estimated standard error of the contrast, you follow a procedure that generalizes EQ2. You divide each group's estimated population variance by its sample size, multiply them by their squared contrast weights, sum them, and then take the square root of the sum. 

\begin{equation}
    \hat{\sigma}_{\hat{I}} = \sqrt{\sum a_i^2\frac{\hat{\sigma_i}^2}{n_i}}
\end{equation}

In the case of two groups in EQ2, the contrast weights are always 1 and -1.

When Student's t test is used, we assume that the group variances are equal, so we generalize the pooling equation (EQ3) to estimate a common pooled variance by using information from all groups.

\begin{equation}
    s_p^2 = \frac{\sum (n_{i} - 1)s_{i}^2}{\sum (n_{i} - 1)}
\end{equation}

This same pooled variance is used for every estimated group variance, and these are used to find the standard error of the t test.  

\begin{equation}
    s_{\hat{I}_{Student}} = \sqrt{\sum a_i^2 \frac{s_p^2}{n_i}}
\end{equation}

The degrees of freedom are $df = \sum (n_i - 1)$, and the t value is equal to the square root of the F value if ANOVA had been used instead. Due to this equivalence and the emphasis on cases where researchers would typically run ANOVA, when use the term ANOVA through the rest of the paper, it refers to both ANOVA and Student's t test.  

However, the benefit of using a t test is that you don't \textit{have} to use Student's t test---you can use Welch's t test instead. When using Welch's t test, instead of finding a common pooled variance, the estimated variance for each group is that group's sample variance because we do not assume variances are equal. 

\begin{equation}
    s_{\hat{I}_{Welch}} = \sqrt{\sum a_i^2\frac{s_{i}^2}{n_{i}}}
\end{equation}

The degrees of freedom also generalize to use information from all the groups.

\begin{equation}
    df_{Welch} = \frac{(\sum a_i^2 \frac{s_i^2}{n_i})^2}{\sum \frac{(a_i^2\frac{s_i^2}{n_i})^2}{n_i-1}}
\end{equation}

Just as Welch's t test controls false positives better than Student's t test without serious downsides when there are two groups, it also controls false positives better than ANOVA when there are more than two groups, is sometimes more powerful to detect true effects, and does a better job estimating the true effect size.

We'll illustrate with three examples. First, we'll consider the case where there are no effects. Second, we'll consider the case of a crossover interaction with no main effects. And third, we'll consider the case with two main effects and an interaction. In each case, we ran 10,000 simulations of four independent groups with normally distributed data. We compared how well ANOVA vs. Welch's t test balances concerns with 
false positives, power, and effect size estimation under 
different variance ratios and sample size ratios (FOOTNOTE: We also looked at different effect sizes and sample sizes, but they had no impact on the relative effects of ANOVA and Welch's t test, so we do not show them here).
We examined cases where the variances and sample sizes were equal or where the variance or sample size of just one group differed from the others.

\section{False Positive Rates}

For false positive rates, we looked at the main effects when there were no effects or only a crossover interaction, and at the interaction effect when there were no effects. 

\begin{figure}[!ht]  
<<false_positives, echo = FALSE, fig = TRUE, width = 14, height = 7>>=

no_effects_false_positives <- plot_grid(
  
  # first row
  rn_no_effects_meA_anova %>% first_row(), 
  rn_no_effects_meB_anova %>% first_row(), 
  rn_no_effects_int_anova,
  
  # second row
  rn_no_effects_meA_welch %>% low_row(), 
  rn_no_effects_meB_welch %>% low_row(), 
  rn_no_effects_int_welch %>% low_row_end(), 
  ncol = 3
)

crossover_false_positives <- plot_grid(
  
  # first row
  rn_crossover_meA_anova %>% first_row(), 
  rn_crossover_meB_anova, 
  blank_plot,
  
  # second row
  rn_crossover_meA_welch %>% low_row(),
  rn_crossover_meB_welch %>% low_row_end(),
  blank_plot,
  
  ncol = 3
)

data_plots <- plot_grid(
    no_effects_true_effect_plot, no_effects_false_positives,
    crossover_true_effect_plot, crossover_false_positives,
    ncol = 2, rel_widths = c(.25, .75)
  )

# add legend
plot_grid(
  data_plots, plot_legend,
  ncol = 2,
  rel_widths = c(10, 3)
) %>% 
  add_x_lab(plot_x_lab)
@

\textit{Figure 1.} False positives for the simulations when there were no effects or a crossover interaction with no main effects.
\end{figure}

For each comparison, the expected false positive rate is $\alpha = 5\%$. For ANOVA, the observed false positive
rate was close to the expected rate when either 
the population variances or sample sizes were equal, but it varied widely 
when both the variances and sample sizes were unequal (see Figure 1). 
When the one smaller group had the smaller variance, the false positive rate dropped as low as
about 2\%, but when it had the larger 
variance, the false positive rate 
rose as high as 13\%, more than double the expected rate. 
In contrast, for Welch's t test, the observed false positive rate
was close to the expected rate across all conditions. Overall, Welch's 
t test consistently behaved as expected when it came to false positives, whether there were no effects or a significant interaction but no main effects. 
ANOVA did not.

\subsection{Joint False Positive Rate}

In addition to looking at each test separately, we can look at the joint false positive rate
---the probability that at least one of the tests gives a false positive.
We focus on the case where there are no true effects, so the joint false positive rate is the probability that the test of at least one main effect or the interaction gives a false positive. 
The expected joint false positive rate in a 2~$\times$~2 ANOVA when the three comparisons are independent is $\alpha = 14\%$, 
though in practice the rate is typically slightly below 14\% \cite{Cramer2016}.


\begin{figure}[!ht]  

<<no_effects_joint_false_positives, echo = FALSE, collapse = TRUE, fig = TRUE, width = 8, height = 4>>=
data_plots <- plot_grid(
    joint_reject_student, joint_reject_welch,
    ncol = 1
  )

plot_grid(
  data_plots, plot_legend,
  ncol = 2,
  rel_widths = c(2, 2)
) %>% 
add_x_lab(plot_x_lab)
@

\textit{Figure 2.} Rates of at least one false positive (at least one main effect or the interaction) for the simulations when there were no effects.
\end{figure}

As with the separate tests of the main effects and interaction, for ANOVA, the observed false positive
rate was close to the expected rate when either 
the population variances or sample sizes were equal, but it varied widely 
when both the variances and sample sizes were unequal (see Figure 2). 
When the one smaller group had the smaller variance, the false positive rate dropped as low as 
about 7\%, but when it had the larger 
variance, the false positive rate 
rose as high as 26\%.
For Welch's t test, the observed joint false positive  
rate remained closer to the expected rate. Welch's 
t test behaved closer to expectations when it came to false positives than 
ANOVA.


\subsection{Power}
To compare the power of ANOVA vs. Welch's t test to detect true effects, we looked at the main effects when there were main effects and an interaction, and at the interaction effect when there was an interaction with or without true main effects. 

\begin{figure}[!ht]
<<power_plots, echo = FALSE, collapse = TRUE, fig = TRUE, width = 12, height = 8>>=

crossover_power <- plot_grid(
  
  # first row
  blank_plot, 
  blank_plot,
  rn_crossover_int_anova %>% first_row(), 

  # second row
  blank_plot,
  blank_plot,
  rn_crossover_int_welch %>% low_row_end(),
  
  # third row
  blank_plot,
  blank_plot,
  rd_crossover_int %>% low_row_end(),
  
  ncol = 3
)

me_int_power <- plot_grid(
  
  # first row
  rn_me_int_meA_anova %>% first_row(), 
  rn_me_int_meB_anova %>% first_row(),
  rn_me_int_int_anova, 

  # second row
  rn_me_int_meA_welch %>% low_row(), 
  rn_me_int_meB_welch %>% low_row(),
  rn_me_int_int_welch %>% low_row_end(),
  
  # third row
  rd_me_int_meA %>% low_row(), 
  rd_me_int_meB %>% low_row(),
  rd_me_int_int %>% low_row_end(),
  
  ncol = 3
)

data_plots <- plot_grid(
    crossover_true_effect_plot, crossover_power,
    me_int_true_effect_plot, me_int_power,
    ncol = 2, rel_widths = c(.25, .75)
  )

plot_grid(
  data_plots, plot_legend,
  ncol = 2,
  rel_widths = c(10, 3)
) %>% 
  add_x_lab(plot_x_lab)
@


\textit{Figure 3.} Power for the simulations when there was a crossover interaction or two main effects and an interaction. The third row shows the difference in power between ANOVA and Welch's t test, where higher values mean Welch's t test is more powerful than ANOVA.
\end{figure}

Figure 3 displays the power of ANOVA and Welch's t test to detect true effects, as well as the difference in power between the two tests, where higher values mean Welch's t test is more powerful than ANOVA. As we can see, the power of the two tests to detect the interaction is approximately equal when either the sample sizes or the variances are equal. However, when the sample sizes and the variances are different, the power of the two tests is different. ANOVA has more power when the smaller group has the larger variance--the same condition where it inflates the false positive rate. In contrast, Welch's t test has more power when the smaller group has the smaller variance, without the risk of inflating the false positive rate. The differences in power are primarily driven by the fact that when the smaller group has the larger variance, ANOVA underestimates the standard error and tends to reject the null hypothesis, regardless of whether or not there is a true effect. So the main benefit of ANOVA is undermined by a higher risk of false positives.


\subsection{Coverage Probability}
Next, we looked at how well ANOVA and Welch's t test estimated the true effects. 
We used 95\% confidence intervals constructed using Student's and Welch's t 
tests to find their relative coverage probability, the percentage of confidence intervals 
that contain the population value of the estimated parameter, which in this case is the 
difference in group means. By 
definition,  the expected coverage probability of 95\% confidence intervals  is 95\%. 
Additionally, when the null hypothesis is true and $\alpha = 5\%$, the coverage 
probability of 95\% confidence intervals has a simple relationship with the 
false positive rate---it's the complement of the false positive rate. 

Figure 4 displays the coverage probabilities of the two t tests. 
The coverage probability for ANOVA varied, just as the 
false positive rate did, ranging from about 87-98\% when either the sample sizes and variances were unequal. 
Importantly, under the same conditions where Student's t test had the most power to detect true effects, the probability that it correctly estimated the true effect size was the lowest.   
In contrast, Welch's t test performed as expected and the confidence interval 
contained the true effect 95\% of the time regardless of the variance and sample 
size ratios. 

\begin{figure}[!ht]
<<coverage_probability, echo = FALSE, collapse = TRUE, fig = TRUE, width = 12, height = 8>>=

no_effects_coverage <- plot_grid(

  # first row
  cov_no_effects_meA_anova %>% first_row(),
  cov_no_effects_meB_anova %>% first_row(),
  cov_no_effects_int_anova,

  # second row
  cov_no_effects_meA_welch %>% low_row(),
  cov_no_effects_meB_welch %>% low_row(),
  cov_no_effects_int_welch %>% low_row_end(),
  ncol = 3
)

crossover_coverage <- plot_grid(

  # first row
  cov_crossover_meA_anova %>% first_row(),
  cov_crossover_meB_anova %>% first_row(),
  cov_crossover_int_anova,

  # second row
  cov_crossover_meA_welch %>% low_row(),
  cov_crossover_meB_welch %>% low_row(),
  cov_crossover_int_welch %>% low_row_end(),

  ncol = 3
)

me_int_coverage <- plot_grid(

  # first row
  cov_me_int_meA_anova %>% first_row(),
  cov_me_int_meB_anova %>% first_row(),
  cov_me_int_int_anova,

  # second row
  cov_me_int_meA_welch %>% low_row(),
  cov_me_int_meB_welch %>% low_row(),
  cov_me_int_int_welch %>% low_row_end(),

  ncol = 3
)

data_plots <- plot_grid(
    no_effects_true_effect_plot, no_effects_coverage,
    crossover_true_effect_plot, crossover_coverage,
    me_int_true_effect_plot, me_int_coverage,
    ncol = 2, rel_widths = c(.25, .75)
  )

plot_grid(
  data_plots, plot_legend,
  ncol = 2,
  rel_widths = c(10, 3)
) %>%
add_x_lab(plot_x_lab)
@

\textit{Figure 4.} Coverage probability for ANOVA and Welch's t test
as the variance ratio, sample size ratio, and sample sizes vary.
\end{figure}

\subsection{Summary}
For the tests of main effects and interactions, there are clear benefits to using contrasts based on Welch's t test instead of ANOVA. When the variances are equal, Welch's t test performs about the same as ANOVA, and when the variances are unequal, Welch's t test retains the expected 5\% false positive rate, and its 95\% confidence intervals really do include the true effect size 95\% of the time.

Additionally, there are clear downsides to ANOVA.  When the variances and sample sizes are unequal, it can inflate the false positive rate, and its alleged 95\% confidence intervals can miss the true effect size more often than they should. The only benefit to ANOVA is that it is more powerful than Welch's t test under some conditions, but this benefit is undermined because ANOVA inflates the false positive rate and incorrectly estimates the effect size under those same conditions. 

But we still have another topic to cover, which is contrasts that test the simple effects of each factor. These are often run as follow-up tests to a significant interaction, though we support running them first if they are specific tests of the key hypotheses. 

[NOTE: Expand on this point here] One point to note is that there was little difference in the two tests when sample sizes were equal. Researchers who do experiments might be saying, "So what, I almost always have equal sample sizes?" But as we'll see with simple effects, having equal sample sizes doesn't save Student's t test in the situations that matter most. 

\section{Simple Effects}
[NOTE: Maybe call this contrasts instead of simple effects to be a bit broader]
In addition to looking at main effects and interactions, we can consider the simple effects of A at each level of B, and the simple effects of B at each level of A. 
The contrast weights are:

\begin{table}[!ht]
\begin{tabular}{l c c c c}
\hline
 & \multicolumn{4}{c}{Condition} \\
 & $A_1 B_1$ & $A_1 B_2$ & $A_2 B_1$ & $A_2 B_2$ \\
\hline
Simple Effect of A at B1 & 1 & 0 & -1 & 0 \\
Simple Effect of B at A1 & 1 & -1 & 0 & 0 \\
Simple Effect of A at B2 & 0 & 1 & 0 & -1 \\
Simple Effect of B at A2 & 0 & 0 & 1 & -1 \\
\hline
\end{tabular}
\end{table}

In our current simulations, only group $A_1 B_1$ has a variance or sample size that's different from the other groups under some conditions. 
This group is included in the contrasts testing the simple effect of A at B1 and the simple effect of B at A1, and it is excluded from the contrasts testing the simple effect of A at B2 and the simple effect of B at A2. 
When group $A_1 B_1$ is excluded from the contrast, then even when it has a variance or sample size that's different from the other groups, the groups that are being directly compared always have equal variances and sample sizes. 

The t tests for the simple effects are constructed using the same formulas as those that we use for main effects and interactions, and the only difference is the contrast weights.

\subsection{False Positives}

\begin{figure}[!ht]
<<se_false_positives, echo = FALSE, collapse = TRUE, fig = TRUE, width = 12, height = 8>>=

no_effects_false_positives_simple_effects <- plot_grid(
  
  # first row
  rn_no_effects_B1cont_anova %>% first_row(),
  rn_no_effects_A1cont_anova %>% first_row(), 
  rn_no_effects_B2cont_anova %>% first_row(),
  rn_no_effects_A2cont_anova,

  # second row
  rn_no_effects_B1cont_welch %>% low_row(),
  rn_no_effects_A1cont_welch %>% low_row(), 
  rn_no_effects_B2cont_welch %>% low_row(),
  rn_no_effects_A2cont_welch %>% low_row_end(), 
  
  ncol = 4
)

me_int_false_positives_simple_effects <- plot_grid(
  
  # first row
  blank_plot, 
  blank_plot, 
  blank_plot, 
  rn_me_int_A2cont_anova,

  # second row
  blank_plot, 
  blank_plot, 
  blank_plot, 
  rn_me_int_A2cont_welch %>% low_row_end(), 
  
  ncol = 4
)


data_plots <- plot_grid(
    no_effects_true_effect_plot, no_effects_false_positives_simple_effects,
    me_int_true_effect_plot, me_int_false_positives_simple_effects,
    ncol = 2, rel_widths = c(.25, .75)
  )

plot_grid(
  data_plots, plot_legend,
  ncol = 2,
  rel_widths = c(10, 3)
) %>%
add_x_lab(plot_x_lab)
@

\textit{Figure 5.} False positives for the simple effects.
\end{figure}

As with the main effects and interactions, both ANOVA and Welch's t test were near the expected 5\% false positive rate when both the variances and sample sizes were equal. 
But unlike the main effects and interactions, the false positive rate for ANOVA varied whenever the variance of one group was different from the other groups, even when the sample sizes were equal.

The only group that had a variance that was different from the other groups under some conditions was group $A_1 B_1$. 
When this group was included in the contrasts, ANOVA was too conservative when its variance was smaller than the other groups, and was too liberal when its variance was larger. 
But when the group was excluded from the contrasts, the pattern was opposite (though the size of the effect was smaller)--ANOVA was too liberal when its variance was smaller, and was too conservative when its variance was larger. 
Why, for the simple effects, did ANOVA depart from the expected 5\% rate when the sample sizes were equal? 
And why was the pattern opposite when the group with the different variance was included or excluded from the contrast? 

With four groups that have equal sample sizes, the pooled variance becomes

\begin{equation}
    s_p^2 = \frac{\sum (n - 1)s_{i}^2}{\sum (n - 1)} = \frac{(n - 1)(s_{A_1 B_1}^2 + s_{A_1 B_2}^2 + s_{A_2 B_1}^2 + s_{A_2 B_2}^2)}{4(n - 1)} = \frac{1}{4}(s_{A_1 B_1}^2 + s_{A_1 B_2}^2 + s_{A_2 B_1}^2 + s_{A_2 B_2}^2)
\end{equation}

When we plug the above into Equation 11, we find that the standard error is the same for any two-group contrast, whether the group with unequal variances is included or excluded.

\begin{equation}
\begin{split}
s_{Student_{A_1-A_2|B_1}} & = \sqrt{(1^2\frac{s_p^2}{n} + 0^2\frac{s_p^2}{n} + -1^2\frac{s_p^2}{n} + 0^2\frac{s_p^2}{n})} \\
& = s_{Student_{A_1-A_2|B_2}} \\
& = \sqrt{(0^2\frac{s_p^2}{n} + 1^2\frac{s_p^2}{n} + 0^2\frac{s_p^2}{n} + -1^2\frac{s_p^2}{n})} \\
& = \sqrt{2\frac{s_p^2}{n}} \\
& = \sqrt{2\frac{(s_{A_1 B_1}^2 + s_{A_1 B_2}^2 + s_{A_2 B_1}^2 + s_{A_2 B_2}^2)}{4n}}\\
& = \sqrt{\frac{1}{2n}(s_{A_1 B_1}^2 + s_{A_1 B_2}^2 + s_{A_2 B_1}^2 + s_{A_2 B_2}^2)}
\end{split}
\end{equation}

So when ANOVA is used, when the sample sizes are equal, then even if a group is excluded from the contrast, its variance is weighted just as much as the other groups to estimate the standard error. However, when Welch's t test is used, and when the sample sizes are equal, then only the groups that are \textit{included} in the contrast are used to estimate the standard error.

\begin{equation}
\begin{split}
s_{Welch_{A_1-A_2|B_1}} & = \sqrt{(1^2\frac{s_{A_1 B_1}^2}{n} + 0^2\frac{s_{A_1 B_2}^2}{n} + -1^2\frac{s_{A_2 B_1}^2}{n} + 0^2\frac{s_{A_2 B_2}^2}{n})} \\
& = \sqrt{\frac{1}{n}(s_{A_1 B_1}^2 + s_{A_2 B_1}^2)} \\
& \neq s_{Welch_{A_1-A_2|B_2}} \\
& = \sqrt{(0^2\frac{s_{A_1 B_1}^2}{n} + 1^2\frac{s_{A_1 B_2}^2}{n} + 0^2\frac{s_{A_2 B_1}^2}{n} + -1^2\frac{s_{A_2 B_2}^2}{n})} \\
& = \sqrt{\frac{1}{n}(s_{A_1 B_2}^2 + s_{A_2 B_2}^2)}
\end{split}
\end{equation}

Note that if all variances are equal, Equations 15 and 16 are equal.
Comparing equations 15 and 16, we can see that even though the sample sizes are equal, Student's t test is giving too much weight to the variances from groups that are excluded from the contrasts, and not enough weight to the variances from the groups that are included in the contrasts.
So when the groups that are excluded from the contrast have the smaller variance, the standard error from Student's t test is too small, which will inflate the false positive rate, and when they have the larger variance, the standard error from Student's t test is too large, which will decrease the false positive rate. 

\subsection{Power}

The power of ANOVA and Welch's t test to detect the simple effects is parallel to the false positive rates. When the variances are equal, the power of the two tests is about equal. But when the variances are unequal, the power of each test is different, even when all groups have the same sample size. When the groups that are excluded from the contrast have the smaller variance, the standard error from Student's t test is too small, which makes it more powerful than Welch's t test, and when they have the larger variance, the standard error from Student's t test is too small, which makes it less powerful than Welch's t test. 
Just as with main effects and interactions, Student's t test is more powerful under the same conditions that it inflates the false positive rate, but Welch's t test is more powerful under other conditions while keeping the expected 5\% false positive rate.

\begin{figure}[!ht]
<<se_power_plots, echo = FALSE, collapse = TRUE, fig = TRUE, width = 12, height = 8>>=

crossover_pow_simple_effects <- plot_grid(

  # first row
  rn_crossover_B1cont_anova %>% first_row(),
  rn_crossover_A1cont_anova %>% first_row(),
  rn_crossover_B2cont_anova %>% first_row(),
  rn_crossover_A2cont_anova,

  # second row
  rn_crossover_B1cont_welch %>% low_row(),
  rn_crossover_A1cont_welch %>% low_row(),
  rn_crossover_B2cont_welch %>% low_row(),
  rn_crossover_A2cont_welch %>% low_row_end(),

  # third row
  rd_crossover_B1cont %>% low_row(),
  rd_crossover_A1cont %>% low_row(),
  rd_crossover_B2cont %>% low_row(),
  rd_crossover_A2cont %>% low_row_end(),

  ncol = 4
)

me_int_pow_simple_effects <- plot_grid(

  # first row
  rn_me_int_B1cont_anova %>% first_row(),
  rn_me_int_A1cont_anova %>% first_row(),
  rn_me_int_B2cont_anova,
  blank_plot,

  # second row
  rn_me_int_B1cont_welch %>% low_row(),
  rn_me_int_A1cont_welch %>% low_row(),
  rn_me_int_B2cont_welch %>% low_row_end(),
  blank_plot,

  # third row
  rd_me_int_B1cont %>% low_row(),
  rd_me_int_A1cont %>% low_row(),
  rd_me_int_B2cont %>% low_row_end(),
  blank_plot,

  ncol = 4
)

data_plots <- plot_grid(
    crossover_true_effect_plot, crossover_pow_simple_effects,
    me_int_true_effect_plot, me_int_pow_simple_effects,
    ncol = 2, rel_widths = c(.25, .75)
  )

plot_grid(
  data_plots, plot_legend,
  ncol = 2,
  rel_widths = c(10, 3)
) %>%
add_x_lab(plot_x_lab)
@

\textit{Figure 6.} Power to detect the simple effects of A and B when there was a crossover interaction or two main effects and an interaction. The third row shows the difference in reject rates between ANOVA and Welch's t test, where higher values mean Welch's t test is more powerful than ANOVA.
\end{figure}


\subsection{Coverage Probability}
Figure 7 displays the coverage probabilities of ANOVA and Welch's t test to detect the true simple effects. 
The coverage probability for ANOVA varies, just as the 
false positive rate did, ranging from just under 80\% to over 100\% when the variances were unequal, even if the sample sizes were equal. 
When the group with unequal variances was included in the contrast, when its variance was 5x larger than the other groups and its sample size was half the size of the other groups, 
what you'd think was a 95\% confidence interval really only captured the true effect size 80\% of the time.
Importantly, under the same conditions where Student's t test had the most power to detect true effects, the confidence intervals were least likely to include the true effect size.   
In contrast, Welch's t test performed as expected and the confidence interval 
contains the true effect 95\% of the time regardless of the variance and sample 
size ratios. 

\begin{figure}[!ht]
<<se_coverage_probability, echo = FALSE, collapse = TRUE, fig = TRUE, width = 12, height = 8>>=

no_effects_cov_simple_effects <- plot_grid(

  # first row
  cov_no_effects_B1cont_anova %>% first_row(),
  cov_no_effects_A1cont_anova %>% first_row(),
  cov_no_effects_B2cont_anova %>% first_row(),
  cov_no_effects_A2cont_anova,

  # second row
  cov_no_effects_B1cont_welch %>% low_row(),
  cov_no_effects_A1cont_welch %>% low_row(),
  cov_no_effects_B2cont_welch %>% low_row(),
  cov_no_effects_A2cont_welch %>% low_row_end(),
  ncol = 4
)

crossover_cov_simple_effects <- plot_grid(

  # first row
  cov_crossover_B1cont_anova %>% first_row(),
  cov_crossover_A1cont_anova %>% first_row(),
  cov_crossover_B2cont_anova %>% first_row(),
  cov_crossover_A2cont_anova,

  # second row
  cov_crossover_B1cont_welch %>% low_row(),
  cov_crossover_A1cont_welch %>% low_row(),
  cov_crossover_B2cont_welch %>% low_row(),
  cov_crossover_A2cont_welch %>% low_row_end(),
  ncol = 4
)

me_int_cov_simple_effects <- plot_grid(

  # first row
  cov_me_int_B1cont_anova %>% first_row(),
  cov_me_int_A1cont_anova %>% first_row(),
  cov_me_int_B2cont_anova %>% first_row(),
  cov_me_int_A2cont_anova,

  # second row
  cov_me_int_B1cont_welch %>% low_row(),
  cov_me_int_A1cont_welch %>% low_row(),
  cov_me_int_B2cont_welch %>% low_row(),
  cov_me_int_A2cont_welch %>% low_row_end(),
  ncol = 4
)

data_plots <- plot_grid(
    no_effects_true_effect_plot, no_effects_cov_simple_effects,
    crossover_true_effect_plot, crossover_cov_simple_effects,
    me_int_true_effect_plot, me_int_cov_simple_effects,
    ncol = 2, rel_widths = c(.25, .75)
  )

plot_grid(
  data_plots, plot_legend,
  ncol = 2,
  rel_widths = c(10, 3)
) %>%
  add_x_lab(plot_x_lab)
@

\textit{Figure 6.} Coverage probability for the simple effects using ANOVA and Welch's t test
as the variance ratio, sample size ratio, and sample sizes vary.
\end{figure}


\section{Discussion}

In our simulations we show that Welch's t test can be generalized to any research designs with two or more groups. 
When the variances of the groups are equal, Welch's t test gives almost the same results as ANOVA. 
But when the variances of the groups are unequal, Welch's t test maintains the expected false positive rate, is sometimes more powerful to detect true effects, and is better at correctly estimating the effect size. 
The only advantage to ANOVA is that it has more power to detect true effects when the standard error over-weights groups with small variances,
but under those same conditions it inflates the false positive rate and is worse at estimating the true effect size. 

Beyond these statistical outcomes, using Welch's t test can simplify pre-registration of analyses. The recent push for pre-registration forces researchers to make decisions prior to data collection about how they plan to test their hypothesis (e.g., an analysis of variance). But they should also address the assumptions of those tests (e.g., equal variances), how they will check if those assumptions are violated (e.g., a significance test of the group variances), and what they will do in that case (e.g., use a different test).  Such planning can be good for science, but it can easily become overwhelming to write a long list of conditional statements: ``if I see this, then I'll do that.''  Preregistration can be simplified if the researcher commits to a statistical procedure that is robust to violations of assumptions, and all the better if the procedure doesn't introduce new costs, such as lower power. We propose that when researchers compare the means of independent groups, they should commit to Welch's t test, which is robust when variances are unequal, instead of ANOVA.

One might argue that there are minimal benefits of Welch's t test for researchers who run experiments. The reason for this is that when the sample sizes are equal, then even when the variances are wildly different, ANOVA performs just as well as Welch's t test in the tests of main effects and interactions. However, our simulations highlight that equal sample sizes don't always save ANOVA. Researchers frequently use planned contrasts on a subset of the groups to test their focal hypotheses--if the variances are different, then ANOVA can inflate the false positive rate and get the wrong effect size, even with equal sample sizes. 

Our simulations were based on a 2~$\times$~2 factorial design when only one group had a different sample size and variance from the other groups. But from these results, we can generalize the cases where we'd expect ANOVA and Welch's t test to be different under other designs. When the standard error from ANOVA is smaller than the standard error from Welch's t test, then ANOVA will inflate the false positive rate, be more powerful, and miss the true effect size more than expected. This will happen when ANOVA over-weights groups with small variances, either because they have a larger sample size, or because they're being used in a contrast that they should be excluded from. These principles should generalize to designs with more or fewer groups, with different sample size and variance ratios, and with different numbers of groups that differ in their sample sizes or variances.

We echo others' recommendations to always use 
Welch's t test by default \cite{Zimmerman1996,Moser1992,Moser1989,Delacre2017}, and expand this recommendation to designs with more than two groups.
We also agree that the common approach of using tests of equal variances to decide when to use Welch's t test doesn't work \cite{Delacre2017, Zimmerman1996,Zimmerman2004}.
Perhaps the most popular example of this approach is Levene's test for homogeneity \cite{Levene1960}, which appears in SPSS by default when you run an independent samples t test or ANOVA.
Beyond the points that others have raised (e.g., tests of assumptions are sensitive to sample size), Levene's test hides the equal variances problem in another test.

In its original formulation, the test finds how far each observation deviates from the mean of its group, then it uses ANOVA to test whether the average deviations are different between the groups (later formulations use deviations from the group median or the trimmed mean, which work better when the raw data are not normally distributed, \citeNP{Brown1974}).
Consider the logic of using Levene's test to decide whether to use ANOVA or Welch's t 
test. Before you use an ANOVA to test whether the group means are different, you use an ANOVA to test 
whether the \textit{deviations} from the group means are different, which means you have to make another choice
between ANOVA and Welch, which means you must decide whether the
group \textit{deviations} have equal variances.
This doesn't solve the original problem, it just hides it in a different test.

    A better way to decide to use Welch's t test instead of ANOVA is is to visualize the data using boxplots and judge 
whether the variances appear to differ.  With smaller samples, you 
can tolerate larger apparent differences and still conclude the variances are equal. You can enhance this strategy by 
simulating data that have the same sample sizes as the real data, changing whether the variances are equal or not, 
and checking if the boxplots from the real data look more like the boxplots of
simulations with equal variances or unequal variances. 

    Figure 7 displays sample boxplots where two groups have equal 
variances (top) and where one group has five times the variance of the other (bottom), and where each group has $n$ = 20 (first rows) or $n$ = 
100 (second rows). 

  With smaller 
sample sizes it's typical to see 
variability in the spread of the groups. When the population 
variances are equal,
sometimes the spread of the groups appears almost equal (as it should), 
and other times it appears to differ. When the population variances are unequal, the group with the larger variance (to the right) is generally more spread out, but sometimes the spread of the groups looks similar.

In contrast, with larger sample sizes there is more 
consistency in the boxplots. Smaller differences in the spread of the groups 
might be a sign that the population variances differ. Using boxplots
to decide which test to use
is a viable strategy, though it requires some subjective judgment,
especially when sample sizes are small. 
Though making judgments from boxplots is a reasonable strategy, we believe researchers prefer simple decision rules, 
and so we echo others' recommendations to always use 
Welch's t test \cite{Zimmerman1996,Moser1992,Moser1989}. 
Our simulations suggest there's no harm in doing so.

   
\begin{figure}[!ht]
<<boxplots, echo = FALSE, fig = TRUE, height = 8>>=

# Boxplots (equal variances) ----

# distributions
x1 <- seq(0, 12, .1)
x2 <- seq(1.13, 13.13, .1)

ve_dist_data <- data.frame(
  x = c(x1, x2),
  y = c(dnorm(x1, mean = 6, sd = sqrt(2)), dnorm(x2, mean = 7.13, sd = sqrt(2))),
  group = factor(rep(c(1, 2), each = length(x1)))
)

ve_distributions <- ggplot(ve_dist_data, aes(x = x, y = y, linetype = group)) +
  geom_line(show.legend = FALSE) +
  labs(y = NULL, x = NULL) +
  theme(
    axis.text = element_blank(),
    axis.ticks = element_blank(),
    panel.grid = element_blank()
  )

# boxplots
set.seed(2184)
ve_bplots_n20 <- lapply(1:4, function(x) {
                 plot_data <- data.frame(
                   group = factor(rep(c(1, 2), each = 20)),
                   y = c(
                     rnorm(n = 20, mean = 6, sd = sqrt(2)),
                     rnorm(n = 20, mean = 7.13, sd = sqrt(2))
                   )
                 )
                 
                 ggplot(plot_data, aes(x = group, y = y, linetype = group)) +
                   geom_boxplot(show.legend = FALSE) +
                   labs(x = NULL, y = NULL) +
                   theme(
                     axis.text = element_blank(),
                     axis.ticks = element_blank(),
                     panel.grid = element_blank()
                   )
})

ve_bplots_n100 <- lapply(1:4, function(x) {
                 plot_data <- data.frame(
                   group = factor(rep(c(1, 2), each = 100)),
                   y = c(
                     rnorm(n = 100, mean = 6, sd = sqrt(2)),
                     rnorm(n = 100, mean = 7.13, sd = sqrt(2))
                   )
                 )
                 
                 ggplot(plot_data, aes(x = group, y = y, linetype = group)) +
                   geom_boxplot(show.legend = FALSE) +
                   labs(x = NULL, y = NULL) +
                   theme(
                     axis.text = element_blank(),
                     axis.ticks = element_blank(),
                     panel.grid = element_blank()
                   )
})



# Boxplots (unequal variances) ----

# distributions
x1 <- seq(0, 12, .1)
x2 <- seq(1.13, 13.13, .1)

vun_dist_data <- data.frame(
  x = c(x1, x2),
  y = c(dnorm(x1, mean = 6, sd = sqrt(2)), dnorm(x2, mean = 7.13, sd = sqrt(10))),
  group = factor(rep(c(1, 2), each = length(x1)))
)

vun_distributions <- ggplot(vun_dist_data, aes(x = x, y = y, linetype = group)) +
  geom_line(show.legend = FALSE) +
  labs(y = NULL, x = NULL) +
  theme(
    axis.text = element_blank(),
    axis.ticks = element_blank(),
    panel.grid = element_blank()
  )

# boxplots
set.seed(2184)
vun_bplots_n20 <- lapply(1:4, function(x) {
                 plot_data <- data.frame(
                   group = factor(rep(c(1, 2), each = 20)),
                   y = c(
                     rnorm(n = 20, mean = 6, sd = sqrt(2)),
                     rnorm(n = 20, mean = 7.13, sd = sqrt(10))
                   ) 
                 )
                 
                 ggplot(plot_data, aes(x = group, y = y, linetype = group)) +
                   geom_boxplot(show.legend = FALSE) +
                   labs(x = NULL, y = NULL) +
                   theme(
                     axis.text = element_blank(),
                     axis.ticks = element_blank(),
                     panel.grid = element_blank()
                   )
})

vun_bplots_n100 <- lapply(1:4, function(x) {
                 plot_data <- data.frame(
                   group = factor(rep(c(1, 2), each = 100)),
                   y = c(
                     rnorm(n = 100, mean = 6, sd = sqrt(2)),
                     rnorm(n = 100, mean = 7.13, sd = sqrt(10))
                   )
                 )
                 
                 ggplot(plot_data, aes(x = group, y = y, linetype = group)) +
                   geom_boxplot(show.legend = FALSE) +
                   labs(x = NULL, y = NULL) +
                   theme(
                     axis.text = element_blank(),
                     axis.ticks = element_blank()
                   )
})

# Print boxplots ---- 

# sample size labels
label_20 <- ggplot() + 
  geom_text(aes(x = 50, y = 50, label = "italic('n')*s == 20"), size = 4, parse = TRUE) + 
  coord_cartesian(ylim = c(40, 60), xlim = c(40, 60)) + 
  theme(
    line = element_blank(), 
    rect = element_blank(), 
    text = element_blank()
    )

label_100 <- ggplot() + 
  geom_text(aes(x = 50, y = 50, label = "italic('n')*s == 100"), size = 4, parse = TRUE) + 
  coord_cartesian(ylim = c(40, 60), xlim = c(40, 60)) + 
  theme(
    line = element_blank(), 
    rect = element_blank(), 
    text = element_blank()
    )

# prep for printing
ve_distributions <- plot_grid(NULL, NULL, ve_distributions, NULL, rel_widths = c(2, 3, 6, 3), nrow = 1)
ve_bplots <- plot_grid(
  plotlist = c(
    append(ve_bplots_n20, list(label_20), after = 0),
    append(ve_bplots_n100, list(label_100), after = 0)
    ),
  nrow = 2,
  rel_widths = c(2, 3, 3, 3, 3)
  )
vun_distributions <- plot_grid(NULL, NULL, vun_distributions, NULL, rel_widths = c(2, 3, 6, 3), nrow = 1)
vun_bplots <- plot_grid(
  plotlist = c(
    append(vun_bplots_n20, list(label_20), after = 0),
    append(vun_bplots_n100, list(label_100), after = 0)
    ),
  nrow = 2,
  rel_widths = c(2, 3, 3, 3, 3)
  )


# combine into one grid
full_bplots <- plot_grid(ve_distributions, ve_bplots, vun_distributions, vun_bplots, nrow = 4, rel_heights = c(2, 3, 2, 3))

full_bplots
@

\textit{Figure 7.} Boxplots from simulations of groups with equal variances (top) and unequal 
variances (bottom).
\end{figure}




- add that a benefit of using Welch-based contrasts is also that you can go straight to the specific contrasts of interest instead of starting with main effects and interactions, especially in more complex designs
    
    Prior simulations that compared Student's and Welch's t tests 
focused on null hypothesis significance testing by emphasizing false positives
and power \cite{Boneau1960, Delacre2017, Neuhauser2002, Zimmerman1993, Zimmerman2004, 
Zimmerman1996, Zimmerman2009}, but there are important implications of this work
for effect size estimation. Some have called for researchers to report 
effect sizes and confidence intervals to address limitations of 
merely reporting significance tests. It is important to remember 
that effect size estimation also involves assumptions about group variances.
We found that constructing confidence intervals for the difference in group means with Student's t test,
which assumes equal variances, led to 
 unstable estimates. Under some conditions, the confidence intervals were 
less accurate than expected because they were too narrow, and under other 
conditions, they were more accurate than expected because they were too wide. 
Using Welch's t test, which does not assume equal variances, led 
to more stable estimates across conditions.
    
    Cohen's d \cite{Cohen1992} is the most popular effect size for reporting 
the difference in group means, but it assumes equal variances. Cohen's d standardizes the difference in means based on a 
common standard deviation of the population. This common standard deviation
is just the square root of the pooled variance from Student's t test.
But when the group variances are unequal, there is no common standard deviation. 
Cohen's d will suffer from the same problems as Student's t test. Given the same difference in group 
means, if the sample sizes and variances both differ, then Cohen's d will give more 
weight to the larger sample when pooling the variance. If the larger sample has the larger variance, the 
standardized effect size will be smaller than if the larger sample 
has the smaller variance. In reality, either estimate is misleading
because there is no common standard deviation, so there can be no traditional Cohen's d.
    
    Standardized effect sizes such as Cohen's d are often desirable for their 
use in meta-analysis. But the equal variances 
problem applies to meta-analysis as well. Using Cohen's d as the basis for a 
meta-analysis involves an assumption that the group variances across 
the body of research are equal, an assumption which might be untenable. Differences in variances might not be just a nuisance, but rather an interesting part of the effect for meta-analysts to examine. The effect of the independent variable may be on the variances and not merely the means.
    
    The good news is that raw difference in means are also effect sizes
\cite{Cumming2014, Gonzalez2008, Kelley2012}, and you can find confidence intervals around raw differences 
in means without assuming equal variances. In fact, 95\% confidence intervals
based on Welch's t test appear in the 
default output of programs such as SPSS and R. Reporting 
descriptive statistics in their original scale might be a better practice than 
reporting only standardized effect sizes anyway. First, unlike Cohen's d, reporting raw descriptive statistics does not require the researcher to commit to an 
equal variances assumption. Second, it provides all of the necessary 
information for others who want to assume equal variances to find Cohen's d or its alternatives \cite{Peng2013, 
Grissom2001}. Third, it allows other researchers to examine whether differences 
in group variances are a consistent part of an effect, which would be lost by 
just reporting the standardized difference.

    We suspect that most statistics courses in psychology thoroughly teach Student's t 
test and only briefly touch on Welch's t test, if they teach it at 
all. Indeed, we have heard colleagues complain that when they use Welch's t test in a 
manuscript, reviewers are suspicious of the 
degrees of freedom with decimals. These reviewers must not have learned that 
degrees of freedom with decimals are the norm for Welch's t test and related methods such as corrections used for assumptions in repeated measures ANOVA \cite{Gonzalez2008}. The emphasis 
on Student's t test in teaching is consistent with the strategy of assuming equal variances and only using Welch's t test if it appears the assumption has been violated. But why should we spend so much 
time on the equal variances assumption in the first place? Why not teach 
Welch's t test at the outset without imposing 
restrictive assumptions? Student's t test could be taught briefly so  
students understand the existing literature, but we believe it would be 
beneficial to emphasize Welch's t test as the default approach. As demonstrated 
in our simulations, this approach will lead to better decision-making when 
it comes to analyzing data. 

Our discussion began with the relatively simple case of testing the means from two independent groups with normally distributed data.
But our simulations demonstrate that the implications generalize to more complex designs, such as interactions in a 2 x 2 factorial design.
For between-groups contrasts, with two groups or more, we arrived at a simple conclusion: on balance, when considering the false positive rate, power, and effect size estimation, an efficient strategy is to always uses Welch's t test.  In the Appendix, we provide the syntax for an R function that uses Welch's t test for any set of between-groups contrasts, including main effects and interactions. As the study design becomes more complicated, such as with repeated measures, random effect models, or non-normally distributed data, the story will likely be more complicated. As a field we should move toward analysis strategies that have fewer moving parts so they can be described easily in a scientific report or preregistration plan, and  provide robust estimates.

\section{Author Contributions}

JDW wrote the code for the simulations, wrote the first draft of the manuscript. RG advised JDW with coding and contributed independent writing to the manuscript. Both authors revised the manuscript and wrote the function in the Appendix. 



\bibliography{bibliography}
\bibliographystyle{apacite}

\appendix
\section{Welch t test contrasts for R}

In R, you can use the following function to compute group contrasts based on Welch's t test. There is an example of how to use it below.

\subsection{Arguments}
\begin{small}
\begin{singlespace}
\begin{lstlisting}
welch_contrast(data = NULL, dv, groups, ...)

data: The dataframe where the dependent variable and groups are stored (optional)
dv: The dependent variable
groups: The group variable
...: Any number of vectors with numeric contrasts, separated by commas, e.g., c(1, -1, 0, 0), c(-1, 0, 1, 0)
\end{lstlisting}
\end{singlespace}
\end{small}

\subsection{Function (run this syntax in R to access the function)}

\begin{small}
\begin{singlespace}
\begin{lstlisting}
welch_contrast <- function(..., data = NULL, dv, groups) {
  
  # assign dv and groups if they're in a dataframe
  if(!is.null(data)) {
    dv <- eval(substitute(dv), data)
    groups <- eval(substitute(groups), data)
  }
  
  # make sure contrasts are numeric
  if (!is.numeric(c(...))) {
    return('All contrasts must be numeric')
  }
  
  # make sure length of contrasts is correct
    contrast_lengths <- lapply(list(...), length)
    unique_lengths <- unique(contrast_lengths)
  
    # are contrasts the same length as each other?
    if(length(unique_lengths) != 1) { 
      return('All contrasts must be the same length')
    }
    
    # are contrasts the same length as the number of groups?
    if (unique_lengths != length(unique(groups))) { 
      return(
        cat(
          'Contrasts must be the same length as the number of groups: ', 
          length(unique(groups)), 
          '. \n\nIf this number seems too large, you might have missing data (NA) in your groups.', 
          sep = ''
          )
      )
    }
    
  # compute group stats
  means <- by(dv, groups, mean)
  vars <- by(dv, groups, var)
  Ns <- by(dv, groups, length)
  
  # build contrast matrix and compute contrast values
  contrast <- matrix(c(...), nrow = length(list(...)), byrow = TRUE)
  colnames(contrast) <- names(means)
  rownames(contrast) <- paste0('Contrast ', 1:length(list(...)))
  ihat <- contrast %*% means
  
  # t test
  df_welch <- (contrast^2 %*% (vars / Ns))^2 / (contrast^2 %*% (vars^2 / (Ns^2 * (Ns - 1))))
  se_welch <- sqrt(contrast^2 %*% (vars / Ns))
  t_welch <- ihat/se_welch
  p_welch <- 2*(1 - pt(abs(t_welch), df_welch))
  ci_welch <- qt(.025, df = df_welch)
  lb_welch <- ihat - ci_welch * se_welch
  ub_welch <- ihat + ci_welch * se_welch
  
  # store t test results in a data frame
  t_test <- data.frame(t = t_welch,
                       df = df_welch,
                       p = p_welch,
                       lb_95CI = lb_welch,
                       ub_95CI = ub_welch
  )
  t_test <- round(t_test, digits = 3)
  
  # combine it with the contrast matrix
  output <- cbind(contrast, t_test)
  
  return(output)
}
\end{lstlisting}

<<contrast_function, echo = FALSE>>=
welch_contrast <- function(..., data = NULL, dv, groups) {
  
  # assign dv and groups if they're in a dataframe
  if(!is.null(data)) {
    dv <- eval(substitute(dv), data)
    groups <- eval(substitute(groups), data)
  }
  
  # make sure contrasts are numeric
  if (!is.numeric(c(...))) {
    return('All contrasts must be numeric')
  }
  
  # make sure length of contrasts is correct
    contrast_lengths <- lapply(list(...), length)
    unique_lengths <- unique(contrast_lengths)
  
    # are contrasts the same length as each other?
    if(length(unique_lengths) != 1) { 
      return('All contrasts must be the same length')
    }
    
    # are contrasts the same length as the number of groups?
    if (unique_lengths != length(unique(groups))) { 
      return(
        cat(
          'Contrasts must be the same length as the number of groups: ', 
          length(unique(groups)), 
          '. \n\nIf this number seems too large, you might have missing data (NA) in your groups.', 
          sep = ''
          )
      )
    }
    
  # compute group stats
  means <- by(dv, groups, mean)
  vars <- by(dv, groups, var)
  Ns <- by(dv, groups, length)
  
  # build contrast matrix and compute contrast values
  contrast <- matrix(c(...), nrow = length(list(...)), byrow = TRUE)
  colnames(contrast) <- names(means)
  rownames(contrast) <- paste0('Contrast ', 1:length(list(...)))
  ihat <- contrast %*% means
  
  # t test
  df_welch <- (contrast^2 %*% (vars / Ns))^2 / (contrast^2 %*% (vars^2 / (Ns^2 * (Ns - 1))))
  se_welch <- sqrt(contrast^2 %*% (vars / Ns))
  t_welch <- ihat/se_welch
  p_welch <- 2*(1 - pt(abs(t_welch), df_welch))
  ci_welch <- qt(.025, df = df_welch)
  lb_welch <- ihat - ci_welch * se_welch
  ub_welch <- ihat + ci_welch * se_welch
  
  # store t test results in a data frame
  t_test <- data.frame(t = t_welch,
                       df = df_welch,
                       p = p_welch,
                       lb_95CI = lb_welch,
                       ub_95CI = ub_welch
  )
  t_test <- round(t_test, digits = 3)
  
  # combine it with the contrast matrix
  output <- cbind(contrast, t_test)
  
  return(output)
}
@
\end{singlespace}
\end{small}

\subsection{Examples}
\begin{small}
\begin{singlespace}
\begin{lstlisting}
# generate data
set.seed(123)
sample_data <- data.frame(
  y = sample(1:7, size = 200, replace = TRUE),
  groups = rep(c('A', 'B', 'C', 'D'), each = 50)
  )

# Example 1  
# specify data argument
# one contrast
welch_contrast(data = sample_data, 
               dv = y, 
               groups = groups, 
               c(-1, 1, 0, 0)
               )
\end{lstlisting}
<<contrast_example1, echo = FALSE, size = 'tiny'>>=
## Examples
# generate example data
set.seed(123)
sample_data <- data.frame(
  y = sample(1:7, size = 200, replace = TRUE),
  groups = rep(c('A', 'B', 'C', 'D'), each = 50)
  )
  
welch_contrast(data = sample_data, 
               dv = y, 
               groups = groups, 
               c(-1, 1, 0, 0)
               )
@
\begin{lstlisting}
# Example 2
# don't specify data argument
# three contrasts
welch_contrast(dv = sample_data$y, 
               groups = sample_data$groups, 
               c(-1, 1, 0, 0),
               c(-1, 0, 1, 0),
               c(-1, 0, 0, 1)
               )
\end{lstlisting}
<<contrast_example2, echo = FALSE>>=
welch_contrast(dv = sample_data$y, 
               groups = sample_data$groups, 
               c(-1, 1, 0, 0),
               c(-1, 0, 1, 0),
               c(-1, 0, 0, 1)
               )
@
\end{singlespace}
\end{small}


\end{document}