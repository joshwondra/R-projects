%floatsintext can be used in man to have figs appear where they are called
\documentclass[man,noextraspace,apacite]{apa6}
%following looks like a journal printout; need to comment out previous line
%\documentclass[jou,noextraspace,apacite]{apa6}
\usepackage{apacite}

\title{Always Use Welch's \textit{t} Test to Compare the Means of Two Independent Groups}
\shorttitle{Welch t Test}
\author{Joshua D. Wondra and Richard Gonzalez}
\affiliation{University of Michigan}

\abstract{When testing whether the means of two independent groups are 
different from each other, researchers typically use Student's t test, which 
assumes that the population variances of the two groups are equal. If there is 
reason to believe that the group variances are unequal, then researchers can 
use Welch's t test, which does not assume equal variances, as an alternative. 
We were interested in finding a simple rule that could be used to 
decide when to use Student's vs. Welch's t test. We used Monte Carlo 
simulations to compare the type I error rate, power, and coverage probability 
of Student's and Welch's t tests when testing the difference in the means of 
two groups across different variance ratios, sample size ratios, sample sizes, 
and effect sizes. We recommend the following rule: Always use Welch's t test to 
compare the means of two independent groups.}
\keywords{t test, new statistics, Welch}

\authornote{Joshua D. Wondra, Department of Psychology, University of Michigan.

Richard Gonzalez, Department of Psychology, University of Michigan.

Correspondence concerning this article should be addressed to Josh Wondra, 
Department of Psychology, University of Michigan, 530 Church St., Ann Arbor, MI 
48109-1043.

Contact: jdwondra@umich.edu}


\begin{document}
\SweaveOpts{concordance=TRUE}
\maketitle
<<setup, echo=FALSE>>=

## Retrieve tables with general data
load('veNeSeed2184Tables.Rdata')
load('v2NeSeed2184Tables.Rdata')
load('v5NeSeed2184Tables.Rdata')
load('ve1andhalfnSeed2184Tables.Rdata')
load('ve2nSeed2184Tables.Rdata')
load('v21andhalfnSSVSeed2184Tables.Rdata')
load('v51andhalfnSSVSeed2184Tables.Rdata')
load('v22nSSVSeed2184Tables.Rdata')
load('v52nSSVSeed2184Tables.Rdata')
load('v21andhalfnBSVSeed2184Tables.Rdata')
load('v51andhalfnBSVSeed2184Tables.Rdata')
load('v22nBSVSeed2184Tables.Rdata')
load('v52nBSVSeed2184Tables.Rdata')

## Retrieve tables with 2 x 2 interaction data
load('interaction2x2-veNeSeed2184Tables.Rdata')
load('interaction2x2-v5bigvarNeSeed2184Tables.Rdata')
load('interaction2x2-v5smallvarNeSeed2184Tables.Rdata')
load('interaction2x2-bignveN2Seed2184Tables.Rdata')
load('interaction2x2-smallnveN2Seed2184Tables.Rdata')
load('interaction2x2-biggroupbigvarSeed2184Tables.Rdata')
load('interaction2x2-biggroupsmallvarSeed2184Tables.Rdata')
load('interaction2x2-smallgroupbigvarSeed2184Tables.Rdata')
load('interaction2x2-smallgroupsmallvarSeed2184Tables.Rdata')

## Retrieve data for boxplots
load('veNeSeed2184-sim1.Rdata')
load('v2NeSeed2184-sim1.Rdata')
load('v5NeSeed2184-sim1.Rdata')
load('veN15Seed2184-sim1.Rdata')
load('v2N15ssvSeed2184-sim1.Rdata')
load('v2N15bsvSeed2184-sim1.Rdata')
load('v5N15ssvSeed2184-sim1.Rdata')
load('v5N15bsvSeed2184-sim1.Rdata')
load('veN2Seed2184-sim1.Rdata')
load('v2N2ssvSeed2184-sim1.Rdata')
load('v2N2bsvSeed2184-sim1.Rdata')
load('v5N2ssvSeed2184-sim1.Rdata')
load('v5N2bsvSeed2184-sim1.Rdata')

## Load packages
library(ggplot2)
library(grid)
library(gridExtra)

theme_set(theme_bw())

# Multiple plot function
# retrieved from 
#http://www.cookbook-r.com/Graphs/Multiple_graphs_on_one_page_(ggplot2)/

# ggplot objects can be passed in ..., or to plotlist (as a list of ggplot objects)
# - cols:   Number of columns in layout
# - layout: A matrix specifying the layout. If present, 'cols' is ignored.

# If the layout is something like matrix(c(1,2,3,3), nrow=2, byrow=TRUE),
# then plot 1 will go in the upper left, 2 will go in the upper right, and
# 3 will go all the way across the bottom.

multiplot <- function(..., plotlist=NULL, file, cols=1, layout=NULL) {

  # Make a list from the ... arguments and plotlist
  plots <- c(list(...), plotlist)

  numPlots = length(plots)

  # If layout is NULL, then use 'cols' to determine layout
  if (is.null(layout)) {
    # Make the panel
    # ncol: Number of columns of plots
    # nrow: Number of rows needed, calculated from # of cols
    layout <- matrix(seq(1, cols * ceiling(numPlots/cols)),
                    ncol = cols, nrow = ceiling(numPlots/cols))
  }

 if (numPlots==1) {
    print(plots[[1]])

  } else {
    # Set up the page
    grid.newpage()
    pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout), 
heights=unit(rep(1,nrow(layout)),'null'))))

    # Make each plot, in the correct location
    for (i in 1:numPlots) {
      # Get the i,j matrix positions of the regions that contain this subplot
      matchidx <- as.data.frame(which(layout == i, arr.ind = TRUE))

      print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row, 
                                      layout.pos.col = matchidx$col))
    }
  }

}

@
    Data analysis involves a series of decisions on the part of the researcher 
about which statistical test answers the research question, whether the test is 
suitable to the data, and whether there is an alternative test that is more appropriate. Recent discussions of false positives in psychology research 
\cite<e.g.,>{Fiedler2012, Ioannidis2012, Nosek2012, Simmons2011, 
Wagenmakers2012} highlight the tension between two valued outcomes of the 
decision process. On the one hand, researchers want to avoid mistakenly 
claiming that there is a true effect where none exists, which involves a 
concern with false positives. On the other hand, researchers want to find true 
effects where they do exist, which involves a concern with power. In addition, 
there is a growing concern with estimating and reporting effect sizes 
\cite{Cumming2014}. Some have argued that due to some common research 
practices, such as running underpowered studies, the effects that make it into 
published papers are spuriously large \cite<e.g.,>{Bakker2012, Ioannidis2008}. 
Researchers must find a way to balance the concerns with false positives, 
power, and effect size estimation as they make their decisions. 

The recent push for pre-registration of studies forces researchers to be transparent about these kinds of decision strategies for how they plan to analyze data prior to its collection.  In pre-registration the researcher should detail how they plan to test their hypothesis (e.g., a two-sample t test). But in pre-registration they should also address the assumptions of those statistical tests (e.g., equality of variances), how they will check those assumptions (e.g., visual inspection of boxplots, a test of significance between two variances), what criteria they will use to determine if the assumption is violated, and what remedial measures they will take if those criteria suggest the assumptions are violated (e.g., perform a version of the statistical test that does not make the violated assumption, such as the Welch t test; transform the data; perform a nonparametric test).  Such level of transparency and planning can be good for science in some ways, but it could easily become overwhelming to write a long list of detailed conditional statements of the form ``if I see this, then I'll do that.''  Perhaps preregistration can be simplified if the researcher  commits to a statistical procedure that is robust to violations of assumptions. If a robust procedure could be used without introducing new costs, such as reduction of power or more complicated statistical procedures, then it may lead to easier pre-registration, thus lowering the hurdle to participate in this important practice.

\subsection{The Comparison of Means from Two Independent Groups}
    One of the first decisions that many researchers learn is how to compare 
the means of two independent groups---they run a t test. But even this basic 
comparison involves a choice between the classic Student's t test 
\cite{Student1908} or the alternative Welch-Satterthwaite test \cite{Welch1938, 
Satterthwaite1946}. Most researchers learn about Student's t test in the first 
statistics class that they ever take. When you use Student's t test to compare 
the means of independent groups, you make three assumptions about the 
populations that the data come from: 
\begin{APAenumerate}
    \item Normality: The populations follow a normal distribution.
    \item Independence: Observations are independent of each other, meaning 
that the probability of one observation having a particular value does not 
depend on the probability of another observation having a particular value.
    \item Equal variances: The populations have the same variance.
\end{APAenumerate}
So, even though the research hypothesis deals with the difference between two means, the implementation of the test requires additional conditions that do not directly refer to the means themselves.  
    
If these assumptions hold, then you can find the t-value by taking the 
difference in group means and dividing by the standard error of that 
difference:   
    \begin{equation}
    t = \frac{\hat{\mu}_1-\hat{\mu}_2}{\hat{\sigma}_{\mu_1-\mu_2}}
    \end{equation}
    The p-value for a given value of the t statistic depends on the degrees of 
freedom, $df=n_1+n_2-2$. You are more likely to reject the null hypothesis and 
conclude that there is a difference in the group means as the t-value and the 
degrees of freedom get larger. This means that you are more likely to conclude 
that there is a statistically significant difference between the two means when 
the sample size gets larger (which increases the degrees of freedom), when the 
difference in group means gets larger (which increases the numerator of t), or 
when the standard error gets smaller (which decreases the denominator of t). 
    
    In order to compute the standard error for the Student's t test, you need to find the 
common variance of the groups. Because the population variances of the 
two groups are assumed to be equal, the common variance is 
equal to the population variance of each group. 
In practice, however, you rarely know the population variances of the two groups, and 
instead you estimate them from the sample variances of your data. 
Unfortunately, even if the population variances are identical, the sample 
variances are rarely identical due to sampling error. Student's t test deals with this 
problem by pooling the sample variances of the two groups into a weighted 
average that is used to estimate the common variance. 
    \begin{equation}
    s_p^2 = \frac{(n_1 - 1)s_1^2 + (n_2 - 1)s_2^2}{n_1 + n_2 - 2}
    \end{equation}
    \begin{equation}
    s_{\overline{x}_1-\overline{x}_2} = \sqrt{\frac{s_p^2}{n_1}  + \frac{s_p^2}{n_2}} = s_p\sqrt{\frac{1}{n_1}  + \frac{1}{n_2}}
    \end{equation}
    with $s_1^2$ and $s_2^2$ being the two sample estimates of the population variances.
The group 
with the larger sample size is given more weight than the group with the 
smaller sample size. This means that when the bigger group has the bigger 
variance, the standard error is bigger, but when the bigger group has the 
smaller variance, the standard error is smaller.

    If either the data or the study design suggests that the assumptions of 
normality, independence, and equal variances do not hold, then Student's t test 
is not the right choice and an alternative test should be used. If the group 
variances are unequal, then the Welch-Satterthwaite t test (hereafter called 
Welch's t test for the sake of brevity) is a good alternative choice.  For 
those who use SPSS to analyze their data, Welch's t test is in the ``Equal 
variances not assumed'' row that appears when they run an independent samples t 
test. For those who use R, Welch's t test is the default when they use the 
\texttt{t.test()} function, and they can get the classic Student's t test by setting 
the \texttt{var.equal} argument to \texttt{TRUE}.
    
        As with Student's t test, Welch's t test assumes normality and 
independence; however, it does not assume that the population variances are 
equal. The standard error is based on separate group variances instead of a 
common variance:
    \begin{equation}
    s_{\overline{x}_1-\overline{x}_2} = \sqrt{\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2}}
    \end{equation}
Additionally, Welch's t test uses the following formula to compute the degrees of freedom: 
    \begin{equation}
    df = \frac{(\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2})^2}{\frac{(s_1^2)^2}{n_1^2(n_1-1)} + \frac{(s_2^2)^2}{n_2^2(n_2-1)}}
    \end{equation}
This formula decreases the degrees of freedom to the extent that the group
variances are different from each other. It can also produce a degrees of freedom value  that is not an integer.

Because of the 
differences in the standard errors and degrees of freedom, the two tests can disagree about whether there is a difference in 
group means. Because Welch's t test decreases the degrees of freedom, it might 
be more conservative and less likely to reject the null hypothesis. On the one 
hand, this might make Welch's t test a better choice if Student's t test finds 
more false positives when variances are unequal. On the other hand, this might 
make Welch's t test a worse choice if it is not powerful enough to detect true 
effects. 
        
    However, Welch's t test might not always be more conservative. The power of 
the two tests is not only based on the degrees of freedom, but also on the 
standard error. This means that Welch's t test could be more powerful than 
Student's t test if the separate variances standard error is smaller than the 
pooled variances standard error.
    
    How do you decide which test to use? The typical approach is to use 
Student's t test unless there is evidence that the two groups have unequal 
variances. The challenge is how to find that evidence. 
Our goal is to find a simple decision rule that can help researchers decide when to use Welch's t test instead of Student's t test. We begin by reviewing three decision strategies: testing the equal variances assumption with another statistical test, comparing the degrees of freedom of the two tests, and visually examining the data with boxplots. Then we use Monte Carlo simulations to examine when each test performs best in terms of type I errors, power, and coverage probability.

    Before we proceed, we note that aside from Welch's t test, there have been Bayesian approaches 
to the problem of comparing groups with equal or unequal variances. 
\citeA[p.~104-109]{Box1973} presented analytical Bayesian t tests when equal 
variances is assumed and when equal variances is not assumed. When 
noninformative locally uniform priors are used for the group means and the 
logarithm of the group standard deviations, and equal variances is assumed, 
their analytic solution is identical to Student's t test. When the same priors are used, 
but equal variances are not assumed, their solution is similar to Welch's t 
test, but slightly more conservative because the degrees of freedom are 
generally lower and the standard error is generally larger or equal. More 
recently, \citeA{Kruschke2013} presented a Bayesian approach to comparing group 
means that uses Markov Chain Monte Carlo simulation to estimate the joint distribution of five parameters (two means, two variances and a normality parameter). This approach simulates 
the posterior distribution of means for each group based on the prior 
distributions and the data. This produces a posterior distribution of the 
difference in group means that takes into account the uncertainty in estimating all the parameters in the model. Inferences about the difference in group means are 
based on this posterior distribution.  Our purpose in this paper is to inform decisions about the use of 
frequentist approaches to comparing group means, and so we do not discuss
Bayesian approaches further.

\subsection{Testing Assumptions with Another Test}
    One option for how to decide whether to use Student's or Welch's t test 
is to run another test of the null hypothesis that the group variances 
are equal. If the test rejects the null hypothesis, then the variances are unequal and you should use Welch's t test.
If the test retains the null hypothesis, then the variances can be treated as equal and you can use Student's t test. 

One example of this approach is Levene's test for homogeneity \cite{Levene1960}, which shows up in SPSS by default when you run an independent samples t test.
In Levene's original formulation, the test starts by finding the absolute value of the deviations of each observation from its group mean. 
Then it uses a oneway ANOVA to test whether the mean deviation differs between groups.
Later formulations start by finding the absolute value of the deviations of each observation from the group median or the trimmed group mean, which perform better than deviations from the group mean
when the raw data are not normally distributed \cite{Brown1974}.

Consider the logic of using Levene's test to decide whether to use Student's or Welch's t 
test to find the difference in the means of two groups. You begin by finding the 
deviations of the data from the mean, median, or trimmed mean of each group. Then you use an ANOVA to test 
whether the group deviation means are different.
When there are only two groups, this is equivalent to using a t test to test whether the group deviation means
are different, which means that you have to make another choice
between Student's and Welch's t test, which means that you have to make a decision about whether the
group \textit{deviations} have equal variances. The test of the equal variances assumption
makes its own equal variances assumption, which usually goes unchecked.
This doesn't solve the original problem, it just hides it in a different test. 

Another important point to consider is that statistical tests of assumptions,
like other statistical tests, are sensitive 
to sample size \cite{Gonzalez2008}. If your sample is too small, you won't have enough power to 
detect true differences in the variances of the groups. If your sample is 
large, even minute differences will be statistically significant. 

Overall, running tests of equal variances to decide whether to use Student's or Welch's
t test is an ineffective strategy. This has been confirmed by simulation 
studies \cite{Zimmerman1996,Zimmerman2004}. 

\subsection{Comparing the Degrees of Freedom of the Two Tests}

    A different way to decide which t test to use might be to compare the
degrees of freedom from Welch's t test to the degrees of freedom from
Student's t test. When the group variances are equal, the degrees of freedom of the two tests
are also equal. As the variances become unequal, the degrees of freedom
of Welch's t test decrease but the degrees of freedom of Student's t test
stay the same. Although the degrees of freedom of the two tests typically
differ slightly due to sampling error, 
once the difference in the degrees of freedom reaches a certain point,
it might reliably signal that the group variances
are unequal. For example, perhaps when the Welch's t test degrees of freedom 
are 95-100\% of the Student's t test degrees of freedom, that difference is what typically appears when variances are equal, but 
when the Welch's t test degrees of freedom are 90\% or less of the Student's t test degrees of freedom,
it reliably indicates that the population variances are unequal. 

    We examined whether the ratio of the Welch's t 
test degrees of freedom from to the Student's t test degrees of freedom 
could provide a heuristic for deciding that 
the group variances are unequal and you should use Welch's t test. 
To our knowledge, this possibility has never
been examined. We used the analytical formulas for the  
degrees of freedom from the two tests to see how the degrees of freedom (df) ratio should change as 
the population variances and the sample sizes of the two groups change (Figure 1).

    The top graph of Figure 1 displays the change in the df ratio as the 
variance ratio increases when sample sizes are equal. As expected, the ratio 
decreases as the difference in variances grows larger. When the variances are 
equal, the df ratio is equal to one. When 
the variance of one group is double the variance of the other, the ratio drops 
to .9. A useful heuristic might be to assume that the variances are unequal 
when the ratio falls below some value, such as  96\%.
  
\begin{figure*}
<<dfratiosDiffvars, echo=FALSE, collapse=TRUE, fig=TRUE, height=10>>=

load('dfs.list.RData')

df.ratio.Ns <- function(n1, n2, var1, var2) {
    welch.num <- (var1/n1 + var2/n2)^2
    welch.denom <- var1^2/(n1^2*(n1-1)) + var2^2/(n2^2*(n2-1))
    classic.df <- n1+n2-2
    (welch.num/welch.denom)/classic.df
}

df.ratio.defaults <- list('n1'=50,'n2'=50,'var1'=2,'var2'=2)

partial.df <- function(var = 'a', params=df.ratio.defaults){
    params[[var]]=as.name('x')
    function(x)do.call(df.ratio.Ns, params)
}

dfchange.var <- ggplot(data.frame(x=seq(2,10,.1)), aes(x)) +
    stat_function(fun=partial.df(var='var2', 
params=list('n1'=50,'n2'=50,'var1'=2,'var2'=2))) + #NOTE: the sample sizes 
#don't affect the df ratio
    labs(y=bquote(frac('df'['Welch'],'df'['Student'])), 
x=bquote(frac(sigma[1]^2,sigma[2]^2))) +
    scale_x_continuous(breaks=c(2,4,6,8,10), labels=c(2,4,6,8,10)/2)

dfchange.Ns <- ggplot(data.frame(x=seq(50,100,.1)), aes(x)) + 
    stat_function(fun=partial.df(var='n2', 
params=list('n1'=50,'n2'=50,'var1'=2,'var2'=2))) +
    labs(y=bquote(frac('df'['Welch'],'df'['Student'])), 
x=bquote(frac('n'[1],'n'[2]))) +
    scale_x_continuous(breaks=c(50,60,70,80,90,100), 
labels=c(50,60,70,80,90,100)/50)

dfchange.both <- ggplot(data.frame(x=seq(2,10,.1)), aes(x)) + 
    stat_function(fun=partial.df(var='var2', 
params=list('n1'=75,'n2'=50,'var1'=2,'var2'=2)), aes(linetype = 'n2')) +
    stat_function(fun=partial.df(var='var2', 
params=list('n1'=50,'n2'=75,'var1'=2,'var2'=2)), aes(linetype = 'n1/2')) +
    scale_linetype_manual(name = 'Sample size ratio',
                        values=c('solid', 'dashed'), 
                        labels=c(bquote(frac('n'[1],'n'[2])~'='~frac(3,2)), 
                                 bquote(frac('n'[1],'n'[2])~'='~frac(2,3)))
                        ) +
    labs(y=bquote(frac('df'['Welch'],'df'['Student'])), 
x=bquote(frac(sigma[1]^2,sigma[2]^2))) +
    scale_x_continuous(breaks=c(2,4,6,8,10), labels=c(2,4,6,8,10)/2)

layout <- matrix(c(1,2,3), nrow=3, byrow=TRUE)
multiplot(plotlist=list(dfchange.var, dfchange.Ns, dfchange.both), 
layout=layout)

@

\textit{Figure 1.} Degrees of freedom ratio when sample sizes are equal and 
variances are unequal (top), when variances are equal and sample sizes are 
unequal (middle), and when both variances and sample sizes are unequal (bottom).
\end{figure*}

    But now look at what happens when the variances are equal and the sample 
size ratio changes (middle graph of Figure 1). Even though the variances stay 
the same, the df ratio decreases as the difference in sample sizes grows 
larger. The 96\% heuristic would lead you astray and you would incorrectly 
conclude that the variances are unequal when only the sample sizes are unequal.

%' \begin{figure}
%' <<dfratiosDiffNratios, echo=FALSE,  collapse=TRUE, fig=TRUE>>=
%' 
%' ggplot(data.frame(x=seq(50,100,.1)), aes(x)) + 
%'     stat_function(fun=partial.df(var='n2', 
%params=list('n1'=50,'n2'=50,'var1'=2,'var2'=2))) +
%'     labs(y=bquote(frac('df'['Welch'],'df'['Student'])), 
%x=bquote(frac('n'[1],'n'[2]))) +
%'     scale_x_continuous(breaks=c(50,60,70,80,90,100), 
%labels=c(50,60,70,80,90,100)/50)
%' 
%' @
%' 
%' \textit{Figure 4.} Degrees of freedom ratio when sample sizes are unequal 
%and variances are equal.
%' \end{figure}

    The picture becomes even more complicated when both the sample sizes and 
the variances are unequal (bottom of Figure 1). In this case, the effect of 
different variances on the df ratio depends on whether the larger group has the 
larger variance or the smaller variance. As the variance of the smaller group 
increases, the immediate drop in the df ratio is quite dramatic before it 
begins to level off. However, as the variance of the larger group increases, 
the move from equal to unequal variances actually counteracts the effect of 
unequal sample sizes at first, and the df ratio approaches one before it drops 
again. Due to the difference in sample sizes, a 96\% heuristic could mislead you 
into concluding that variances are equal when they are actually 2-3 times 
different from each other.

%' \begin{figure}
%' <<dfratiosDiffvarsDiffNratios, echo=FALSE,  collapse=TRUE, fig=TRUE>>=
%' ggplot(data.frame(x=seq(2,10,.1)), aes(x)) + 
%'     stat_function(fun=partial.df(var='var2', 
%params=list('n1'=75,'n2'=50,'var1'=2,'var2'=2)), aes(colour='n2')) +
%'     stat_function(fun=partial.df(var='var2', 
%params=list('n1'=50,'n2'=75,'var1'=2,'var2'=2)), aes(colour='n1/2')) +
%'     scale_colour_manual(values=c('blue', 'red'), 
%labels=c(bquote(frac('n'[1],'n'[2])~'='~frac(3,2)), 
%bquote(frac('n'[1],'n'[2])~'= '~frac(2,3)))) +
%'     labs(y=bquote(frac('df'['Welch'],'df'['Student'])), 
%x=bquote(frac(sigma[1]^2,sigma[2]^2))) +
%'     scale_x_continuous(breaks=c(2,4,6,8,10), labels=c(2,4,6,8,10)/2)
%' @
%' 
%' \textit{Figure 5.} Degrees of freedom ratio when sample sizes are unequal 
%and variances are unequal.
%' \end{figure}

In short, a heuristic based on the df ratio would only be useful when the sample sizes are equal. 

\subsection{Visualizing Data with Boxplots} 

    A third way to decide between Student's and Welch's t tests is to visualize the data using boxplots and make a judgment 
about whether the variances appear to differ.  With smaller sample sizes, you 
can tolerate larger apparent differences. This strategy can be enhanced by 
simulating data for two groups that have sample sizes equal to the sample 
sizes in your actual data, changing whether the variances are equal or unequal, 
and seeing if the boxplots of your data look like the boxplots of the 
simulations with equal variances. 

    Figure 2 displays boxplots from simulations of two groups with equal 
variances (top) and from simulations of two groups with unequal variances, where 
the variance of the group to the right is five times as large as the variance 
of the group to the left (bottom). The first row of each set of boxplots 
displays groups with $n$ = 20 and the second row displays groups with $n$ = 
100. The population distributions are displayed above the boxplots. 

When the 
sample sizes are smaller (first rows) there is more variability in the boxplots. Seeing some 
variability in the spread of the groups is normal when $n$ = 20, even when the population 
variances are equal. This can be seen in the first row of boxplots at the top of Figure 2.
Sometimes the variances appear to be approximately equal, 
but other times they appear to differ. There is also some variability in the spread of the 
groups when the population variances are unequal. This can be seen in the first row of the 
boxplots at the bottom of Figure 2. In general, the group to the right is more spread out than the 
group to the left, though sometimes the spread doesn't look very different from the 
boxplots from populations with equal variances. In these unequal variances simulations, 
the variance of one group is five times as large as the variance of the other group. 
When the difference in variances is 
less drastic, such as when the variance of one group is only twice as large
as the variance of the other group, then it may be more difficult
to distinguish cases of equal variances from cases of unequal variances when sample
sizes are small.

In contrast, when sample sizes are larger there is more 
consistency in the boxplots. Smaller differences in the spread of the boxplots 
might be a sign that the population variances differ when $n$ = 100. Using boxplots
to decide when the equal variances assumption has been violated and Welch's t test
should be used is a viable strategy, though it requires some subjective judgment,
especially when sample sizes are small.

   
\begin{figure*}
<<varEqualBoxplots, echo=FALSE, collapse=TRUE, fig=TRUE, height=10>>=

x1 <- seq(0,12,.1)
y1 <- dnorm(x1, mean=6, sd=sqrt(2))
x2 <- seq(1.13,13.13,.1)
y2 <- dnorm(x2, mean=7.13, sd=sqrt(2))

x <- c(x1,x2)
y <- c(y1,y2)
group <- factor(c(rep(1,length(x1)), rep(2,length(x2))))

ve.distributions <- qplot(x=x, y=y, lty=group, geom='line', xlab=NULL, 
ylab=NULL) + 
  geom_vline(xintercept=as.numeric(by(x,group,mean))) + 
  theme(legend.position='none', panel.grid.major.x = element_blank(), panel.grid.major.y = element_blank(), panel.grid.minor.x = element_blank(), panel.grid.minor.y = element_blank())

set.seed(2184)
ve.bplots <- c(list(ve.distributions), 
               lapply(1:4, function(x){
                qplot(x=factor(rep(c(1,2), each=20)), 
                  y=c(rnorm(20,6,sqrt(2)),rnorm(20,7.13,sqrt(2))), 
                  geom='boxplot', 
                  xlab=NULL, 
                  ylab=NULL, 
                  ylim=c(-3,18),
                  lty=factor(rep(c(1,2), each=20))) + 
                theme(legend.position='none', axis.text.x=element_blank(), panel.grid.major.x = element_blank(), panel.grid.major.y = element_blank(), panel.grid.minor.x = element_blank(), panel.grid.minor.y = element_blank())}
              ), 
              lapply(1:4, function(x){
                qplot(x=factor(rep(c(1,2), each=100)), 
                      y=c(rnorm(100,6,sqrt(2)),rnorm(100,7.13,sqrt(2))), 
                      geom='boxplot', 
                      xlab=NULL, 
                      ylab=NULL, 
                      ylim=c(-3,18),
                      lty=factor(rep(c(1,2), each=100))) + 
                  theme(legend.position='none', axis.text.x=element_blank(), panel.grid.major.x = element_blank(), panel.grid.major.y = element_blank(), panel.grid.minor.x = element_blank(), panel.grid.minor.y = element_blank())}
                ))

#layout <- matrix(c(0,1,1,0,2,3,4,5,6,7,8,9), nrow=3, byrow=TRUE)
#multiplot(plotlist=ve.bplots, layout=layout)

a1 <- seq(0,12,.1)
b1 <- dnorm(a1, mean=6, sd=sqrt(2))
a2 <- seq(1.13,13.13,.1)
b2 <- dnorm(a2, mean=7.13, sd=sqrt(10))

a <- c(a1,a2)
b <- c(b1,b2)
group <- factor(c(rep(1,length(a1)), rep(2,length(a2))))

vun.distributions <- qplot(x=a, y=b, lty=group, geom='line', xlab=NULL, 
ylab=NULL) + 
  geom_vline(xintercept=as.numeric(by(a,group,mean))) + 
  theme(legend.position='none', panel.grid.major.x = element_blank(), panel.grid.major.y = element_blank(), panel.grid.minor.x = element_blank(), panel.grid.minor.y = element_blank())

set.seed(2184)
vun.bplots <- c(list(vun.distributions), 
                lapply(1:4, function(x){
                  qplot(x=factor(rep(c(1,2), each=20)), 
                        y=c(rnorm(20,6,sqrt(2)),rnorm(20,7.13,sqrt(10))), 
                        geom='boxplot', 
                        xlab=NULL, 
                        ylab=NULL, 
                        ylim=c(-3,18),
                        lty=factor(rep(c(1,2), each=20))) +
                    theme(legend.position='none', axis.text.x=element_blank(), panel.grid.major.x = element_blank(), panel.grid.major.y = element_blank(), panel.grid.minor.x = element_blank(), panel.grid.minor.y = element_blank())
                  }), 
                lapply(1:4, function(x){
                  qplot(x=factor(rep(c(1,2), each=100)), 
                        y=c(rnorm(100,6,sqrt(2)),rnorm(100,7.13,sqrt(10))), 
                        geom='boxplot', 
                        xlab=NULL, 
                        ylab=NULL, 
                        ylim=c(-3,18),
                        lty=factor(rep(c(1,2), each=100))) + 
                    theme(legend.position='none', axis.text.x=element_blank(), panel.grid.major.x = element_blank(), panel.grid.major.y = element_blank(), panel.grid.minor.x = element_blank(), panel.grid.minor.y = element_blank())
                  }))


full.bplots <- c(ve.bplots, vun.bplots)
layout <- matrix(c(0,1,1,0,2,3,4,5,6,7,8,9,0,10,10,0,11,12,13,14,15,16,17,18), 
nrow=6, byrow=TRUE)
multiplot(plotlist=full.bplots, layout=layout)

@

\textit{Figure 2.} Boxplots for groups with equal variances (top) and unequal 
variances (bottom). For each set of boxplots, the first row displays groups 
with $n$=20 and the second row displays groups with $n$=100.
\end{figure*}

\subsection{When Does Each Test Perform Best?}

    So far, we have been following the standard procedure---use Student's t test 
by default and  switch to Welch's t test if the variances 
of the two groups are unequal. However, this standard procedure might be
unwarranted. When variances and sample sizes are equal, Welch's t test is equivalent to 
Student's t test. If using Welch's t test generally leads to the same decision or better 
decisions than using Student's t test, both when variances and sample sizes are equal and when they are unequal, then 
instead of using Student's t test by default it might be better to always use 
Welch's t test.

    To investigate this possibility on both the false positive rate and power, we conducted Monte Carlo simulations of two independent groups with 
normally distributed data. We examined the type I error rate, power, and 
coverage probability for both Student's t test and Welch's t test under 
different conditions. We varied the ratio of population variances 
($\sigma_{1}^2/\sigma_{2}^2$ = 1/5, 1/2, 1, 2, or 5; for each, the smaller 
$\sigma^2$ = 2), the sample sizes (smallest \textit{n} = 20, 50, or 100), and the ratio 
of sample sizes ($n_{1}/n_{2}$ = 1, 2/3, or 1/2). 
    
    Additionally, we varied the difference in group means based on Cohen's d 
values of 0, .2, .5, and .8 when variances were equal. Importantly, Cohen's d 
assumes that the population variances are equal and pools the group variances 
just like Student's t test. This implies that the traditional
Cohen's d is not well-defined when variances are unequal. Therefore, we used the same differences 
in group means when variances were unequal. Because we changed the variance 
ratio by increasing the variance of one group, the mean differences could be 
considered to represent smaller effects when variances are unequal.
    
    For each condition (i.e., combination of variance ratio, smaller sample size, and sample size ratio), we set the seed of the random number generator to the same value and ran 10,000 simulations. 
When we report conditions with equal sample sizes and variance ratios of 2 and 
5, they are identical to their symmetric conditions with equal sample sizes and variance 
ratios of 1/2 and 1/5.

    We examined how well each test balances concerns with 
false positives, power, and effect size estimation. Some prior research has examined the 
Type I error rate for the two tests \cite{Boneau1960, Zimmerman1993, 
Zimmerman2004, Zimmerman1996, Zimmerman2009} and some has also examined the 
power of the two tests, though not with the complete configuration of 
conditions that we examined in our simulations \cite{Neuhauser2002, 
Zimmerman1993}. Nevertheless, we believe that it will be informative to display 
the false positives and power of the two tests here. We also discuss 
implications for effect size estimation, which follows from the false positive 
results but has not, to our knowledge, been discussed explicitly in past 
research.
    
\subsubsection{Type I Error Rates}
<<Type1Setup, echo=FALSE>>=

##### Student's t test data
## Set up data
nullT.ve.ne <- reject.null.ve.ne[,1,1]
nullT.v2.ne <- reject.null.v2.ne[,1,1]
nullT.v5.ne <- reject.null.v5.ne[,1,1]
nullT.ve.1.5n <- reject.null.ve.1.5n[,1,1]
nullT.ve.2n <- reject.null.ve.2n[,1,1]
nullT.v2.1.5n.ssv <- reject.null.v2.1.5n.ssv[,1,1]
nullT.v2.1.5n.bsv <- reject.null.v2.1.5n.bsv[,1,1]
nullT.v2.2n.ssv <- reject.null.v2.2n.ssv[,1,1]
nullT.v2.2n.bsv <- reject.null.v2.2n.bsv[,1,1]
nullT.v5.1.5n.ssv <- reject.null.v5.1.5n.ssv[,1,1]
nullT.v5.1.5n.bsv <- reject.null.v5.1.5n.bsv[,1,1]
nullT.v5.2n.ssv <- reject.null.v5.2n.ssv[,1,1]
nullT.v5.2n.bsv <- reject.null.v5.2n.bsv[,1,1]

## Put it in a dataframe
type1.classic <- data.frame(
    type1.rate=c(nullT.ve.ne, nullT.v2.ne, nullT.v2.ne, nullT.v5.ne, 
nullT.v5.ne, nullT.ve.1.5n, nullT.ve.2n, nullT.v2.1.5n.ssv, nullT.v2.1.5n.bsv, 
nullT.v2.2n.ssv, nullT.v2.2n.bsv, nullT.v5.1.5n.ssv, nullT.v5.1.5n.bsv, 
nullT.v5.2n.ssv, nullT.v5.2n.bsv), #NOTE: doubled any with equal variance or 
#equal ns because they don't have separate ssv and bsv versions
    
Ns=factor(c('20,20','50,50','100,100','20,20','50,50','100,100','20,20','50,50',
'100,100','20,20','50,50','100,100','20,20','50,50','100,100','20,30','50,75',
'100,150','20,40','50,100','100,200','20,30','50,75','100,150','20,30','50,75',
'100,150','20,40','50,100','100,200','20,40','50,100','100,200','20,30','50,75',
'100,150','20,30','50,75','100,150','20,40','50,100','100,200','20,40','50,100',
'100,200'), 
levels=c('20,20','20,30','20,40','50,50','50,75','50,100','100,100','100,150',
         '100,200')),
    var.ratio=factor(rep(c(0,-1,1,-2,2,0,0,-1,1, -1,1,-2,2,-2,2), each=3), 
levels=c(-2,-1,0,1,2), labels=c(.2,.5,1,2,5)))

## Plot it
T1classic <- ggplot(type1.classic, aes(x=as.numeric(var.ratio), y=type1.rate, ymin=0, ymax=.13)) + 
    geom_point(size=4, aes(shape = Ns)) + 
    scale_shape_manual(name = 'Sample size ratio',
                       labels = c('20, 20', '20, 30', '20, 40',
                                  '50, 50', '50, 75', '50, 100',
                                  '100, 100', '100, 150', '100, 200'),
                       values=rep(c(1,2,0), times = 3)
                       ) +
    geom_line(size=.5, aes(linetype = Ns)) + 
    scale_linetype_manual(name = 'Sample size ratio',
                          labels = c('20, 20', '20, 30', '20, 40',
                                     '50, 50', '50, 75', '50, 100',
                                     '100, 100', '100, 150', '100, 200'),
                          values=rep(c('dotted', 'dashed', 'solid'), each = 3)
                          ) +
#     scale_color_manual(name = 'Sample size ratio',
#                        labels = c('20, 20', '20, 30', '20, 40',
#                                      '50, 50', '50, 75', '50, 100',
#                                      '100, 100', '100, 150', '100, 200'),
#                        values=rep(c('deepskyblue4','darkgoldenrod1', 
# 'grey41'), each=3)
#                        ) + 
    scale_x_continuous(breaks=seq(-2:2), labels=factor(c('1/5', '1/2', '1', '2', '5'))) +
    scale_y_continuous(breaks=seq(from=0, to=.13, by=.01)) +
    xlab(bquote(frac(sigma[1]^2,sigma[2]^2))) + ylab('Type I Error Rate') +
    theme(panel.grid.minor.y = element_blank(), panel.grid.major.x = element_blank(), panel.grid.minor.x = element_blank())



##### Welch's t test data
## Set up data
welch.nullT.ve.ne <- reject.null.ve.ne[,1,2]
welch.nullT.v2.ne <- reject.null.v2.ne[,1,2]
welch.nullT.v5.ne <- reject.null.v5.ne[,1,2]
welch.nullT.ve.1.5n <- reject.null.ve.1.5n[,1,2]
welch.nullT.ve.2n <- reject.null.ve.2n[,1,2]
welch.nullT.v2.1.5n.ssv <- reject.null.v2.1.5n.ssv[,1,2]
welch.nullT.v2.1.5n.bsv <- reject.null.v2.1.5n.bsv[,1,2]
welch.nullT.v2.2n.ssv <- reject.null.v2.2n.ssv[,1,2]
welch.nullT.v2.2n.bsv <- reject.null.v2.2n.bsv[,1,2]
welch.nullT.v5.1.5n.ssv <- reject.null.v5.1.5n.ssv[,1,2]
welch.nullT.v5.1.5n.bsv <- reject.null.v5.1.5n.bsv[,1,2]
welch.nullT.v5.2n.ssv <- reject.null.v5.2n.ssv[,1,2]
welch.nullT.v5.2n.bsv <- reject.null.v5.2n.bsv[,1,2]

## Put it in a dataframe
type1.welch <- data.frame(
    type1.rate=c(welch.nullT.ve.ne, welch.nullT.v2.ne, welch.nullT.v2.ne, 
welch.nullT.v5.ne, welch.nullT.v5.ne, welch.nullT.ve.1.5n, welch.nullT.ve.2n, 
welch.nullT.v2.1.5n.ssv, welch.nullT.v2.1.5n.bsv, welch.nullT.v2.2n.ssv, 
welch.nullT.v2.2n.bsv, welch.nullT.v5.1.5n.ssv, welch.nullT.v5.1.5n.bsv, 
welch.nullT.v5.2n.ssv, welch.nullT.v5.2n.bsv), #NOTE: doubled any with equal 
#variance or equal ns because they don't have separate ssv and bsv versions
    
Ns=factor(c('20,20','50,50','100,100','20,20','50,50','100,100','20,20','50,50',
'100,100','20,20','50,50','100,100','20,20','50,50','100,100','20,30','50,75',
'100,150','20,40','50,100','100,200','20,30','50,75','100,150','20,30','50,75',
'100,150','20,40','50,100','100,200','20,40','50,100','100,200','20,30','50,75',
'100,150','20,30','50,75','100,150','20,40','50,100','100,200','20,40','50,100',
'100,200'), 
levels=c('20,20','20,30','20,40','50,50','50,75','50,100','100,100','100,150',
         '100,200')),                                                                      
                                                                                
                                                                                
                                                                          
    var.ratio=factor(rep(c(0,-1,1,-2,2,0,0,-1,1, -1,1,-2,2,-2,2), each=3), 
levels=c(-2,-1,0,1,2), labels=c(.2,.5,1,2,5)))
type1.welch <- type1.welch[order(type1.welch$Ns, type1.welch$var.ratio),]

## Plot it

T1welch <- ggplot(type1.welch, aes(x=as.numeric(var.ratio), y=type1.rate, ymin=0, ymax=.13)) + 
    geom_point(size=4, aes(shape = Ns)) + 
    scale_shape_manual(name = 'Sample size ratio',
                       labels = c('20, 20', '20, 30', '20, 40',
                                  '50, 50', '50, 75', '50, 100',
                                  '100, 100', '100, 150', '100, 200'),
                       values=rep(c(1,2,0), times = 3)
                       ) +
    geom_line(size=.5, aes(linetype = Ns)) + 
    scale_linetype_manual(name = 'Sample size ratio',
                          labels = c('20, 20', '20, 30', '20, 40',
                                     '50, 50', '50, 75', '50, 100',
                                     '100, 100', '100, 150', '100, 200'),
                          values=rep(c('dotted', 'dashed', 'solid'), each = 3)
                          ) +
#     scale_color_manual(name = 'Sample size ratio',
#                        labels = c('20, 20', '20, 30', '20, 40',
#                                      '50, 50', '50, 75', '50, 100',
#                                      '100, 100', '100, 150', '100, 200'),
#                        values=rep(c('deepskyblue4','darkgoldenrod1', 
# 'grey41'), each=3)
#                        ) + 
    scale_x_continuous(breaks=seq(-2:2), labels=factor(c('1/5', '1/2', '1', '2', '5'))) +
    scale_y_continuous(breaks=seq(from=0, to=.13, by=.01)) +
    xlab(bquote(frac(sigma[1]^2,sigma[2]^2))) + ylab('Type I Error Rate') +
    theme(panel.grid.minor.y = element_blank(), panel.grid.major.x = element_blank(), panel.grid.minor.x = element_blank())

@

    The expected type I error (false positive) rate is $\alpha = .05$. The observed type I error 
rate for Student's t test remained close to the expected .05 rate when either 
the sample size or the population variances were equal, but it varied widely 
when both population variances and sample sizes were unequal (see Figure 3). 
When the group with the larger sample size had the larger variance, the type I error rate dropped as low as 
about .01, but when the group with the larger sample size had the smaller 
variance, the type I error rate 
rose as high as .12, which is more than double the normally accepted false 
positive rate. In contrast, the observed type I error rate for Welch's t test 
remained close to the expected .05 rate across all conditions. Overall, Welch's 
t test consistently behaved as expected when it came to false positives. 
Student's t test did not.

\begin{figure*}    
<<Type1ClassicPlot, echo=FALSE, fig=TRUE, height=10>>=
multiplot(T1classic+ggtitle("Student's t test"), T1welch+ggtitle("Welch's t test"))
@

\textit{Figure 3.} Type I error rates for Student's t test and Welch's t test 
as the variance ratio, sample size ratio, and sample sizes vary.
\end{figure*}


%' \begin{figure}
%' <<Type1WelchPlot, echo=FALSE, fig=TRUE>>=
%' T1welch
%' @
%' 
%' \textit{Figure 8.} Type I error rates for Welch's t test.
%' \end{figure}

\subsubsection{Power}
<<PowerSetup, echo=FALSE>>=

## Common variables

power.Ns <- 
factor(c('20,20','50,50','100,100','20,20','50,50','100,100','20,20','50,50',
'100,100','20,20','50,50','100,100','20,20','50,50','100,100','20,30','50,75',
'100,150','20,40','50,100','100,200','20,30','50,75','100,150','20,30','50,75',
'100,150','20,40','50,100','100,200','20,40','50,100','100,200','20,30','50,75',
'100,150','20,30','50,75','100,150','20,40','50,100','100,200','20,40','50,100',
'100,200'), 
levels=c('20,20','20,30','20,40','50,50','50,75','50,100','100,100','100,150','100,200'))

power.var.ratio <- factor(rep(c(0,-1,1,-2,2,0,0,-1,1,-1,1,-2,2,-2,2), each=3), 
levels=c(-2,-1,0,1,2), labels=c(.2,.5,1,2,5))

## Set up data for observed power

# d=.2
smalld.ve.ne <- reject.null.ve.ne[,2,1]
smalld.v2.ne <- reject.null.v2.ne[,2,1]
smalld.v5.ne <- reject.null.v5.ne[,2,1]
smalld.ve.1.5n <- reject.null.ve.1.5n[,2,1]
smalld.ve.2n <- reject.null.ve.2n[,2,1]
smalld.v2.1.5n.ssv <- reject.null.v2.1.5n.ssv[,2,1]
smalld.v2.1.5n.bsv <- reject.null.v2.1.5n.bsv[,2,1]
smalld.v2.2n.ssv <- reject.null.v2.2n.ssv[,2,1]
smalld.v2.2n.bsv <- reject.null.v2.2n.bsv[,2,1]
smalld.v5.1.5n.ssv <- reject.null.v5.1.5n.ssv[,2,1]
smalld.v5.1.5n.bsv <- reject.null.v5.1.5n.bsv[,2,1]
smalld.v5.2n.ssv <- reject.null.v5.2n.ssv[,2,1]
smalld.v5.2n.bsv <- reject.null.v5.2n.bsv[,2,1]
power.smalld.student <- c(smalld.ve.ne, smalld.v2.ne, smalld.v2.ne, 
smalld.v5.ne, smalld.v5.ne, smalld.ve.1.5n, smalld.ve.2n, smalld.v2.1.5n.ssv, 
smalld.v2.1.5n.bsv, smalld.v2.2n.ssv, smalld.v2.2n.bsv, smalld.v5.1.5n.ssv, 
smalld.v5.1.5n.bsv, smalld.v5.2n.ssv, smalld.v5.2n.bsv) #NOTE: doubled any with 
#equal variance or equal ns because they don't have separate ssv and bsv versions

smalld.student <- data.frame(power.smalld.student, power.Ns, power.var.ratio)
smalld.student <- smalld.student[order(smalld.student$power.Ns, 
smalld.student$power.var.ratio),]


# d=.5
midd.ve.ne <- reject.null.ve.ne[,3,1]
midd.v2.ne <- reject.null.v2.ne[,3,1]
midd.v5.ne <- reject.null.v5.ne[,3,1]
midd.ve.1.5n <- reject.null.ve.1.5n[,3,1]
midd.ve.2n <- reject.null.ve.2n[,3,1]
midd.v2.1.5n.ssv <- reject.null.v2.1.5n.ssv[,3,1]
midd.v2.1.5n.bsv <- reject.null.v2.1.5n.bsv[,3,1]
midd.v2.2n.ssv <- reject.null.v2.2n.ssv[,3,1]
midd.v2.2n.bsv <- reject.null.v2.2n.bsv[,3,1]
midd.v5.1.5n.ssv <- reject.null.v5.1.5n.ssv[,3,1]
midd.v5.1.5n.bsv <- reject.null.v5.1.5n.bsv[,3,1]
midd.v5.2n.ssv <- reject.null.v5.2n.ssv[,3,1]
midd.v5.2n.bsv <- reject.null.v5.2n.bsv[,3,1]
power.midd.student <- c(midd.ve.ne, midd.v2.ne, midd.v2.ne, midd.v5.ne, 
midd.v5.ne, midd.ve.1.5n, midd.ve.2n, midd.v2.1.5n.ssv, midd.v2.1.5n.bsv, 
midd.v2.2n.ssv, midd.v2.2n.bsv, midd.v5.1.5n.ssv, midd.v5.1.5n.bsv, 
midd.v5.2n.ssv, midd.v5.2n.bsv) #NOTE: doubled any with equal variance or equal 
#ns because they don't have separate ssv and bsv versions

midd.student <- data.frame(power.midd.student, power.Ns, power.var.ratio)       
                                                                                
                                                                                
                                                                                
                                                         
midd.student <- midd.student[order(midd.student$power.Ns, 
midd.student$power.var.ratio),]

# d=.8
bigd.ve.ne <- reject.null.ve.ne[,4,1]
bigd.v2.ne <- reject.null.v2.ne[,4,1]
bigd.v5.ne <- reject.null.v5.ne[,4,1]
bigd.ve.1.5n <- reject.null.ve.1.5n[,4,1]
bigd.ve.2n <- reject.null.ve.2n[,4,1]
bigd.v2.1.5n.ssv <- reject.null.v2.1.5n.ssv[,4,1]
bigd.v2.1.5n.bsv <- reject.null.v2.1.5n.bsv[,4,1]
bigd.v2.2n.ssv <- reject.null.v2.2n.ssv[,4,1]
bigd.v2.2n.bsv <- reject.null.v2.2n.bsv[,4,1]
bigd.v5.1.5n.ssv <- reject.null.v5.1.5n.ssv[,4,1]
bigd.v5.1.5n.bsv <- reject.null.v5.1.5n.bsv[,4,1]
bigd.v5.2n.ssv <- reject.null.v5.2n.ssv[,4,1]
bigd.v5.2n.bsv <- reject.null.v5.2n.bsv[,4,1]
power.bigd.student <- c(bigd.ve.ne, bigd.v2.ne, bigd.v2.ne, bigd.v5.ne, 
bigd.v5.ne, bigd.ve.1.5n, bigd.ve.2n, bigd.v2.1.5n.ssv, bigd.v2.1.5n.bsv, 
bigd.v2.2n.ssv, bigd.v2.2n.bsv, bigd.v5.1.5n.ssv, bigd.v5.1.5n.bsv, 
bigd.v5.2n.ssv, bigd.v5.2n.bsv) #NOTE: doubled any with equal variance or equal 
#ns because they don't have separate ssv and bsv versions

bigd.student <- data.frame(power.bigd.student, power.Ns, power.var.ratio)
bigd.student <- bigd.student[order(bigd.student$power.Ns, 
bigd.student$power.var.ratio),]

## Student t, d=.2, power
## Plot using ggplot2

smalld.student.plot <- ggplot(smalld.student, aes(x=as.numeric(power.var.ratio), y=power.smalld.student, ymin=0, ymax=1)) + 
    geom_point(size=3, aes(shape = power.Ns)) + 
    scale_shape_manual(name = 'Sample size ratio',
                       labels = c('20, 20', '20, 30', '20, 40',
                                  '50, 50', '50, 75', '50, 100',
                                  '100, 100', '100, 150', '100, 200'),
                       values=rep(c(1,2,0), times = 3)
                       ) +
    geom_line(size=.5, aes(linetype = power.Ns)) + 
    scale_linetype_manual(name = 'Sample size ratio',
                          labels = c('20, 20', '20, 30', '20, 40',
                                     '50, 50', '50, 75', '50, 100',
                                     '100, 100', '100, 150', '100, 200'),
                          values=rep(c('dotted', 'dashed', 'solid'), each = 3)
                          ) +
#     scale_color_manual(name = 'Sample size ratio',
#                        labels = c('20, 20', '20, 30', '20, 40',
#                                      '50, 50', '50, 75', '50, 100',
#                                      '100, 100', '100, 150', '100, 200'),
#                        values=rep(c('deepskyblue4','darkgoldenrod1', 
# 'grey41'), each=3)
#                        ) + 
    scale_x_continuous(breaks=seq(-2:2), labels=factor(c('1/5', '1/2', '1', '2', '5'))) +
    scale_y_continuous(breaks=seq(from=0, to=1, by=.1)) +
    xlab(bquote(sigma[1]^2~'/'~sigma[2]^2)) + ylab('Small Effect') +
    ggtitle("Student's t test") +
    theme(panel.grid.minor.y = element_blank(), panel.grid.major.x = element_blank(), panel.grid.minor.x = element_blank())



## Student t, d=.5, power
## Set up Student t test plot of deviation from expected power

midd.student.plot <- ggplot(midd.student, aes(x=as.numeric(power.var.ratio), y=power.midd.student, ymin=0, ymax=1)) + 
    geom_point(size=3, aes(shape = power.Ns)) + 
    scale_shape_manual(name = 'Sample size ratio',
                       labels = c('20, 20', '20, 30', '20, 40',
                                  '50, 50', '50, 75', '50, 100',
                                  '100, 100', '100, 150', '100, 200'),
                       values=rep(c(1,2,0), times = 3)
                       ) +
    geom_line(size=.5, aes(linetype = power.Ns)) + 
    scale_linetype_manual(name = 'Sample size ratio',
                          labels = c('20, 20', '20, 30', '20, 40',
                                     '50, 50', '50, 75', '50, 100',
                                     '100, 100', '100, 150', '100, 200'),
                          values=rep(c('dotted', 'dashed', 'solid'), each = 3)
                          ) +
#     scale_color_manual(name = 'Sample size ratio',
#                        labels = c('20, 20', '20, 30', '20, 40',
#                                      '50, 50', '50, 75', '50, 100',
#                                      '100, 100', '100, 150', '100, 200'),
#                        values=rep(c('deepskyblue4','darkgoldenrod1', 
# 'grey41'), each=3)
#                        ) + 
    scale_x_continuous(breaks=seq(-2:2), labels=factor(c('1/5', '1/2', '1', '2', '5'))) +
    scale_y_continuous(breaks=seq(from=0, to=1, by=.1)) +
    xlab(bquote(sigma[1]^2~'/'~sigma[2]^2)) + ylab('Medium Effect') +
    theme(panel.grid.minor.y = element_blank(), panel.grid.major.x = element_blank(), panel.grid.minor.x = element_blank())



## Student t, d=.8, power

bigd.student.plot <- ggplot(bigd.student, aes(x=as.numeric(power.var.ratio), y=power.bigd.student, ymin=0, ymax=1)) + 
    geom_point(size=3, aes(shape = power.Ns)) + 
    scale_shape_manual(name = 'Sample size ratio',
                       labels = c('20, 20', '20, 30', '20, 40',
                                  '50, 50', '50, 75', '50, 100',
                                  '100, 100', '100, 150', '100, 200'),
                       values=rep(c(1,2,0), times = 3)
                       ) +
    geom_line(size=.5, aes(linetype = power.Ns)) + 
    scale_linetype_manual(name = 'Sample size ratio',
                          labels = c('20, 20', '20, 30', '20, 40',
                                     '50, 50', '50, 75', '50, 100',
                                     '100, 100', '100, 150', '100, 200'),
                          values=rep(c('dotted', 'dashed', 'solid'), each = 3)
                          ) +
#     scale_color_manual(name = 'Sample size ratio',
#                        labels = c('20, 20', '20, 30', '20, 40',
#                                      '50, 50', '50, 75', '50, 100',
#                                      '100, 100', '100, 150', '100, 200'),
#                        values=rep(c('deepskyblue4','darkgoldenrod1', 
# 'grey41'), each=3)
#                        ) + 
    scale_x_continuous(breaks=seq(-2:2), labels=factor(c('1/5', '1/2', '1', '2', '5'))) +
    scale_y_continuous(breaks=seq(from=0, to=1, by=.1)) +
    xlab(bquote(sigma[1]^2~'/'~sigma[2]^2)) + ylab('Large Effect') +
    theme(panel.grid.minor.y = element_blank(), panel.grid.major.x = element_blank(), panel.grid.minor.x = element_blank())

@

<<WelchPowerSetup, echo=FALSE>>=

## Set up data for observed power

# d=.2
smalld.ve.ne <- reject.null.ve.ne[,2,2]
smalld.v2.ne <- reject.null.v2.ne[,2,2]
smalld.v5.ne <- reject.null.v5.ne[,2,2]
smalld.ve.1.5n <- reject.null.ve.1.5n[,2,2]
smalld.ve.2n <- reject.null.ve.2n[,2,2]
smalld.v2.1.5n.ssv <- reject.null.v2.1.5n.ssv[,2,2]
smalld.v2.1.5n.bsv <- reject.null.v2.1.5n.bsv[,2,2]
smalld.v2.2n.ssv <- reject.null.v2.2n.ssv[,2,2]
smalld.v2.2n.bsv <- reject.null.v2.2n.bsv[,2,2]
smalld.v5.1.5n.ssv <- reject.null.v5.1.5n.ssv[,2,2]
smalld.v5.1.5n.bsv <- reject.null.v5.1.5n.bsv[,2,2]
smalld.v5.2n.ssv <- reject.null.v5.2n.ssv[,2,2]
smalld.v5.2n.bsv <- reject.null.v5.2n.bsv[,2,2]
power.smalld.welch <- c(smalld.ve.ne, smalld.v2.ne, smalld.v2.ne, smalld.v5.ne, 
smalld.v5.ne, smalld.ve.1.5n, smalld.ve.2n, smalld.v2.1.5n.ssv, 
smalld.v2.1.5n.bsv, smalld.v2.2n.ssv, smalld.v2.2n.bsv, smalld.v5.1.5n.ssv, 
smalld.v5.1.5n.bsv, smalld.v5.2n.ssv, smalld.v5.2n.bsv) #NOTE: doubled any with 
#equal variance or equal ns because they don't have separate ssv and bsv versions

smalld.welch <- data.frame(power.smalld.welch, power.Ns, power.var.ratio)
smalld.welch <- smalld.welch[order(smalld.welch$power.Ns, 
smalld.welch$power.var.ratio),]

# d=.5
midd.ve.ne <- reject.null.ve.ne[,3,2]
midd.v2.ne <- reject.null.v2.ne[,3,2]
midd.v5.ne <- reject.null.v5.ne[,3,2]
midd.ve.1.5n <- reject.null.ve.1.5n[,3,2]
midd.ve.2n <- reject.null.ve.2n[,3,2]
midd.v2.1.5n.ssv <- reject.null.v2.1.5n.ssv[,3,2]
midd.v2.1.5n.bsv <- reject.null.v2.1.5n.bsv[,3,2]
midd.v2.2n.ssv <- reject.null.v2.2n.ssv[,3,2]
midd.v2.2n.bsv <- reject.null.v2.2n.bsv[,3,2]
midd.v5.1.5n.ssv <- reject.null.v5.1.5n.ssv[,3,2]
midd.v5.1.5n.bsv <- reject.null.v5.1.5n.bsv[,3,2]
midd.v5.2n.ssv <- reject.null.v5.2n.ssv[,3,2]
midd.v5.2n.bsv <- reject.null.v5.2n.bsv[,3,2]
power.midd.welch <- c(midd.ve.ne, midd.v2.ne, midd.v2.ne, midd.v5.ne, 
midd.v5.ne, midd.ve.1.5n, midd.ve.2n, midd.v2.1.5n.ssv, midd.v2.1.5n.bsv, 
midd.v2.2n.ssv, midd.v2.2n.bsv, midd.v5.1.5n.ssv, midd.v5.1.5n.bsv, 
midd.v5.2n.ssv, midd.v5.2n.bsv) #NOTE: doubled any with equal variance or equal 
#ns because they don't have separate ssv and bsv versions

midd.welch <- data.frame(power.midd.welch, power.Ns, power.var.ratio)
midd.welch <- midd.welch[order(midd.welch$power.Ns, 
midd.welch$power.var.ratio),]

# d=.8
bigd.ve.ne <- reject.null.ve.ne[,4,2]
bigd.v2.ne <- reject.null.v2.ne[,4,2]
bigd.v5.ne <- reject.null.v5.ne[,4,2]
bigd.ve.1.5n <- reject.null.ve.1.5n[,4,2]
bigd.ve.2n <- reject.null.ve.2n[,4,2]
bigd.v2.1.5n.ssv <- reject.null.v2.1.5n.ssv[,4,2]
bigd.v2.1.5n.bsv <- reject.null.v2.1.5n.bsv[,4,2]
bigd.v2.2n.ssv <- reject.null.v2.2n.ssv[,4,2]
bigd.v2.2n.bsv <- reject.null.v2.2n.bsv[,4,2]
bigd.v5.1.5n.ssv <- reject.null.v5.1.5n.ssv[,4,2]
bigd.v5.1.5n.bsv <- reject.null.v5.1.5n.bsv[,4,2]
bigd.v5.2n.ssv <- reject.null.v5.2n.ssv[,4,2]
bigd.v5.2n.bsv <- reject.null.v5.2n.bsv[,4,2]
power.bigd.welch <- c(bigd.ve.ne, bigd.v2.ne, bigd.v2.ne, bigd.v5.ne, 
bigd.v5.ne, bigd.ve.1.5n, bigd.ve.2n, bigd.v2.1.5n.ssv, bigd.v2.1.5n.bsv, 
bigd.v2.2n.ssv, bigd.v2.2n.bsv, bigd.v5.1.5n.ssv, bigd.v5.1.5n.bsv, 
bigd.v5.2n.ssv, bigd.v5.2n.bsv) #NOTE: doubled any with equal variance or equal 
#ns because they don't have separate ssv and bsv versions

bigd.welch <- data.frame(power.bigd.welch, power.Ns, power.var.ratio)
bigd.welch <- bigd.welch[order(bigd.welch$power.Ns, 
bigd.welch$power.var.ratio),]


## Welch t, d=.2, power
smalld.welch.plot <- ggplot(smalld.welch, aes(x=as.numeric(power.var.ratio), y=power.smalld.welch, ymin=0, ymax=1)) + 
    geom_point(size=3, aes(shape = power.Ns)) + 
    scale_shape_manual(name = 'Sample size ratio',
                       labels = c('20, 20', '20, 30', '20, 40',
                                  '50, 50', '50, 75', '50, 100',
                                  '100, 100', '100, 150', '100, 200'),
                       values=rep(c(1,2,0), times = 3)
                       ) +
    geom_line(size=.5, aes(linetype = power.Ns)) + 
    scale_linetype_manual(name = 'Sample size ratio',
                          labels = c('20, 20', '20, 30', '20, 40',
                                     '50, 50', '50, 75', '50, 100',
                                     '100, 100', '100, 150', '100, 200'),
                          values=rep(c('dotted', 'dashed', 'solid'), each = 3)
                          ) +
#     scale_color_manual(name = 'Sample size ratio',
#                        labels = c('20, 20', '20, 30', '20, 40',
#                                      '50, 50', '50, 75', '50, 100',
#                                      '100, 100', '100, 150', '100, 200'),
#                        values=rep(c('deepskyblue4','darkgoldenrod1', 
# 'grey41'), each=3)
#                        ) + 
    scale_x_continuous(breaks=seq(-2:2), labels=factor(c('1/5', '1/2', '1', '2', '5'))) +
    scale_y_continuous(breaks=seq(from=0, to=1, by=.1)) +
    xlab(bquote(sigma[1]^2~'/'~sigma[2]^2)) + ylab('Small Effect') +
    ggtitle("Welch's t test") +
    theme(panel.grid.minor.y = element_blank(), panel.grid.major.x = element_blank(), panel.grid.minor.x = element_blank())


## Welch t, d=.5, power
midd.welch.plot <- ggplot(midd.welch, aes(x=as.numeric(power.var.ratio), y=power.midd.welch, ymin=0, ymax=1)) + 
    geom_point(size=3, aes(shape = power.Ns)) + 
    scale_shape_manual(name = 'Sample size ratio',
                       labels = c('20, 20', '20, 30', '20, 40',
                                  '50, 50', '50, 75', '50, 100',
                                  '100, 100', '100, 150', '100, 200'),
                       values=rep(c(1,2,0), times = 3)
                       ) +
    geom_line(size=.5, aes(linetype = power.Ns)) + 
    scale_linetype_manual(name = 'Sample size ratio',
                          labels = c('20, 20', '20, 30', '20, 40',
                                     '50, 50', '50, 75', '50, 100',
                                     '100, 100', '100, 150', '100, 200'),
                          values=rep(c('dotted', 'dashed', 'solid'), each = 3)
                          ) +
#     scale_color_manual(name = 'Sample size ratio',
#                        labels = c('20, 20', '20, 30', '20, 40',
#                                      '50, 50', '50, 75', '50, 100',
#                                      '100, 100', '100, 150', '100, 200'),
#                        values=rep(c('deepskyblue4','darkgoldenrod1', 
# 'grey41'), each=3)
#                        ) + 
    scale_x_continuous(breaks=seq(-2:2), labels=factor(c('1/5', '1/2', '1', '2', '5'))) +
    scale_y_continuous(breaks=seq(from=0, to=1, by=.1)) +
    xlab(bquote(sigma[1]^2~'/'~sigma[2]^2)) + ylab('Medium Effect') +
    theme(panel.grid.minor.y = element_blank(), panel.grid.major.x = element_blank(), panel.grid.minor.x = element_blank())


## Welch t, d=.8, power
bigd.welch.plot <- ggplot(bigd.welch, aes(x=as.numeric(power.var.ratio), y=power.bigd.welch, ymin=0, ymax=1)) + 
    geom_point(size=3, aes(shape = power.Ns)) + 
    scale_shape_manual(name = 'Sample size ratio',
                       labels = c('20, 20', '20, 30', '20, 40',
                                  '50, 50', '50, 75', '50, 100',
                                  '100, 100', '100, 150', '100, 200'),
                       values=rep(c(1,2,0), times = 3)
                       ) +
    geom_line(size=.5, aes(linetype = power.Ns)) + 
    scale_linetype_manual(name = 'Sample size ratio',
                          labels = c('20, 20', '20, 30', '20, 40',
                                     '50, 50', '50, 75', '50, 100',
                                     '100, 100', '100, 150', '100, 200'),
                          values=rep(c('dotted', 'dashed', 'solid'), each = 3)
                          ) +
#     scale_color_manual(name = 'Sample size ratio',
#                        labels = c('20, 20', '20, 30', '20, 40',
#                                      '50, 50', '50, 75', '50, 100',
#                                      '100, 100', '100, 150', '100, 200'),
#                        values=rep(c('deepskyblue4','darkgoldenrod1', 
# 'grey41'), each=3)
#                        ) + 
    scale_x_continuous(breaks=seq(-2:2), labels=factor(c('1/5', '1/2', '1', '2', '5'))) +
    scale_y_continuous(breaks=seq(from=0, to=1, by=.1)) +
    xlab(bquote(sigma[1]^2~'/'~sigma[2]^2)) + ylab('Large Effect') +
    theme(panel.grid.minor.y = element_blank(), panel.grid.major.x = element_blank(), panel.grid.minor.x = element_blank())


@

<<diffPowerSetup, echo=FALSE>>=

power.smalld.diff <- power.smalld.student-power.smalld.welch
smalld.diff <- data.frame(power.smalld.diff, power.Ns, power.var.ratio)
smalld.diff <- smalld.diff[order(smalld.diff$power.Ns, 
smalld.diff$power.var.ratio),]

power.midd.diff <- power.midd.student-power.midd.welch
midd.diff <- data.frame(power.midd.diff, power.Ns, power.var.ratio)
midd.diff <- midd.diff[order(midd.diff$power.Ns, midd.diff$power.var.ratio),]

power.bigd.diff <- power.bigd.student-power.bigd.welch
bigd.diff <- data.frame(power.bigd.diff, power.Ns, power.var.ratio)
bigd.diff <- bigd.diff[order(bigd.diff$power.Ns, bigd.diff$power.var.ratio),]

smalld.diff.plot <- ggplot(smalld.diff, aes(x=as.numeric(power.var.ratio), y=power.smalld.diff, ymin=-.3, ymax=.3)) + 
    geom_point(size=3, aes(shape = power.Ns)) + 
    scale_shape_manual(name = 'Sample size ratio',
                       labels = c('20, 20', '20, 30', '20, 40',
                                  '50, 50', '50, 75', '50, 100',
                                  '100, 100', '100, 150', '100, 200'),
                       values=rep(c(1,2,0), times = 3)
                       ) +
    geom_line(size=.5, aes(linetype = power.Ns)) + 
    scale_linetype_manual(name = 'Sample size ratio',
                          labels = c('20, 20', '20, 30', '20, 40',
                                     '50, 50', '50, 75', '50, 100',
                                     '100, 100', '100, 150', '100, 200'),
                          values=rep(c('dotted', 'dashed', 'solid'), each = 3)
                          ) +
#     scale_color_manual(name = 'Sample size ratio',
#                        labels = c('20, 20', '20, 30', '20, 40',
#                                      '50, 50', '50, 75', '50, 100',
#                                      '100, 100', '100, 150', '100, 200'),
#                        values=rep(c('deepskyblue4','darkgoldenrod1', 
# 'grey41'), each=3)
#                        ) + 
    scale_x_continuous(breaks=seq(-2:2), labels=factor(c('1/5', '1/2', '1', '2', '5'))) +
    scale_y_continuous(breaks=seq(from=-.3, to=.3, by=.1), labels = c(-.3, -.2, -.1, 0, .1, .2, .3)) +
    xlab(bquote(sigma[1]^2~'/'~sigma[2]^2)) + ylab('Small Effect') +
    theme(panel.grid.minor.y = element_blank(), panel.grid.major.x = element_blank(), panel.grid.minor.x = element_blank())

midd.diff.plot <- ggplot(midd.diff, aes(x=as.numeric(power.var.ratio), y=power.midd.diff, ymin=-.3, ymax=.3)) + 
    geom_point(size=3, aes(shape = power.Ns)) + 
    scale_shape_manual(name = 'Sample size ratio',
                       labels = c('20, 20', '20, 30', '20, 40',
                                  '50, 50', '50, 75', '50, 100',
                                  '100, 100', '100, 150', '100, 200'),
                       values=rep(c(1,2,0), times = 3)
                       ) +
    geom_line(size=.5, aes(linetype = power.Ns)) + 
    scale_linetype_manual(name = 'Sample size ratio',
                          labels = c('20, 20', '20, 30', '20, 40',
                                     '50, 50', '50, 75', '50, 100',
                                     '100, 100', '100, 150', '100, 200'),
                          values=rep(c('dotted', 'dashed', 'solid'), each = 3)
                          ) +
#     scale_color_manual(name = 'Sample size ratio',
#                        labels = c('20, 20', '20, 30', '20, 40',
#                                      '50, 50', '50, 75', '50, 100',
#                                      '100, 100', '100, 150', '100, 200'),
#                        values=rep(c('deepskyblue4','darkgoldenrod1', 
# 'grey41'), each=3)
#                       ) + 
    scale_x_continuous(breaks=seq(-2:2), labels=factor(c('1/5', '1/2', '1', '2', '5'))) +
    scale_y_continuous(breaks=seq(from=-.3, to=.3, by=.1), labels = c(-.3, -.2, -.1, 0, .1, .2, .3)) +
    xlab(bquote(sigma[1]^2~'/'~sigma[2]^2)) + ylab('Medium Effect') +
    theme(panel.grid.minor.y = element_blank(), panel.grid.major.x = element_blank(), panel.grid.minor.x = element_blank())

bigd.diff.plot <- ggplot(bigd.diff, aes(x=as.numeric(power.var.ratio), y=power.bigd.diff, ymin=-.3, ymax=.3)) + 
    geom_point(size=3, aes(shape = power.Ns)) + 
    scale_shape_manual(name = 'Sample size ratio',
                       labels = c('20, 20', '20, 30', '20, 40',
                                  '50, 50', '50, 75', '50, 100',
                                  '100, 100', '100, 150', '100, 200'),
                       values=rep(c(1,2,0), times = 3)
                       ) +
    geom_line(size=.5, aes(linetype = power.Ns)) + 
    scale_linetype_manual(name = 'Sample size ratio',
                          labels = c('20, 20', '20, 30', '20, 40',
                                     '50, 50', '50, 75', '50, 100',
                                     '100, 100', '100, 150', '100, 200'),
                          values=rep(c('dotted', 'dashed', 'solid'), each = 3)
                          ) +
#     scale_color_manual(name = 'Sample size ratio',
#                        labels = c('20, 20', '20, 30', '20, 40',
#                                      '50, 50', '50, 75', '50, 100',
#                                      '100, 100', '100, 150', '100, 200'),
#                        values=rep(c('deepskyblue4','darkgoldenrod1', 
# 'grey41'), each=3)
#                        ) + 
    scale_x_continuous(breaks=seq(-2:2), labels=factor(c('1/5', '1/2', '1', '2', '5'))) +
    scale_y_continuous(breaks=seq(from=-.3, to=.3, by=.1), labels = c(-.3, -.2, -.1, 0, .1, .2, .3)) +
    xlab(bquote(sigma[1]^2~'/'~sigma[2]^2)) + ylab('Large Effect') +
    theme(panel.grid.minor.y = element_blank(), panel.grid.major.x = element_blank(), panel.grid.minor.x = element_blank())
@

    Is Welch's t test underpowered compared to Student's t test? Figure 4 
displays the power of the two tests to detect small, medium, and large effects 
under the different conditions. For both tests, power decreased as variances 
became unequal because we increased one group's variance; however, the power of 
each test decreased at a different rate depending on the sample size and 
variance ratios. Figure 5 displays the difference in power between Student's t 
test and Welch's t test, with higher numbers indicating that Student's t test 
is more powerful. When the sample sizes or variances were equal, the power of 
the two tests was approximately equal. However, when both the sample sizes and 
variances were unequal, there were differences in power. Overall, Student's t 
test was more powerful when the large sample had the smaller variance, whereas 
Welch's t test was more powerful when the small sample had the smaller variance. 
These differences are the most dramatic when one sample was twice the size of 
the other. 
    
    The conditions in which Student's t test had the greatest power over 
Welch's t test, when one sample was twice the size of the other and the large 
sample had the small variance, were the same conditions in which Student's t 
test had a risk of doubling the false positive rate. Whatever advantage 
Student's t test has in terms of power is undermined by the inflated false 
positive rate. In contrast, in the conditions where Welch's t test was more 
powerful, it never inflated the type I error rate beyond the expected rate. 
These joint results, on the false positive rate and power, support the decision to always use Welch's t test. By using 
Welch's t test, you sometimes have greater power to detect true effects, but 
you don't have to worry about sacrificing concerns with false positives.  

\begin{figure*}
<<plotPower, echo=FALSE, fig=TRUE, width=12, height=16>>=

layout <- matrix(c(1,2,3,4,5,6), nrow=3, byrow=TRUE)
multiplot(smalld.student.plot, smalld.welch.plot, midd.student.plot, 
midd.welch.plot, bigd.student.plot, bigd.welch.plot, layout=layout)

#multiplot(smalld.classic.power, smalld.classic.deviation, smalld.welch.power, 
#smalld.welch.deviation, cols=2)

#multiplot(midd.classic.power, midd.classic.deviation, midd.welch.power, 
#midd.welch.deviation, cols=2)

#multiplot(bigd.classic.power, bigd.classic.deviation, bigd.welch.power, 
#bigd.welch.deviation, cols=2)

@

\textit{Figure 4.} Power of Student's and Welch's t tests.
\end{figure*}

\begin{figure*}
<<plotPowerDiff, echo=FALSE, fig=TRUE, width=12, height=16>>=

layout <- matrix(c(0,1,1,0,0,2,2,0,0,3,3,0), nrow=3, byrow=TRUE)
multiplot(smalld.diff.plot, midd.diff.plot, bigd.diff.plot, layout=layout)

@

\textit{Figure 5.} Difference in the Power of Student's and Welch's t tests 
(Student-Welch).
\end{figure*}

\subsubsection{Coverage Probability}
In addition to the type I error rate and power, we examined the coverage probability 
of 95\% confidence intervals constructed using Student's t test and Welch's t 
test. Coverage probability refers to the proportion of confidence intervals 
that contain the true population value of the estimated parameter, which is the 
difference in group means in this case. For 95\% confidence intervals, by 
definition the expected coverage probability is .95. 
% Because the coverage 
% probability of a confidence interval is not influenced by the effect size, we 
% only show the coverage probabilities when the null hypothesis is true. 
Additionally, when the null hypothesis is true and $\alpha = .05$, the coverage 
probability of 95\% confidence intervals has a simple relationship with the 
type I error rate---the coverage probability is the complement of the type I 
error rate. 

Figure 6 displays the coverage probabilities of the two t tests. 
The coverage probability for Student's t test varies dramatically, just as the 
type I error rate did. When Student's t test is the least powerful, it is the 
most accurate at estimating the difference in means. When it is the most 
powerful, it is also the least accurate, and what you would believe is a 95\% 
confidence interval drops to as low as an 88\% confidence interval in reality. 
In contrast, Welch's t test performs as expected and the confidence interval 
contains the true effect 95\% of the time regardless of the variance and sample 
size ratios. 

<<ClassicCoverage, echo=FALSE>>=

##### Plot classic results only

## Set up data
nullT.ve.ne <- obs.coverage.ve.ne[,1,1]
nullT.v2.ne <- obs.coverage.v2.ne[,1,1]
nullT.v5.ne <- obs.coverage.v5.ne[,1,1]
nullT.ve.1.5n <- obs.coverage.ve.1.5n[,1,1]
nullT.ve.2n <- obs.coverage.ve.2n[,1,1]
nullT.v2.1.5n.ssv <- obs.coverage.v2.1.5n.ssv[,1,1]
nullT.v2.1.5n.bsv <- obs.coverage.v2.1.5n.bsv[,1,1]
nullT.v2.2n.ssv <- obs.coverage.v2.2n.ssv[,1,1]
nullT.v2.2n.bsv <- obs.coverage.v2.2n.bsv[,1,1]
nullT.v5.1.5n.ssv <- obs.coverage.v5.1.5n.ssv[,1,1]
nullT.v5.1.5n.bsv <- obs.coverage.v5.1.5n.bsv[,1,1]
nullT.v5.2n.ssv <- obs.coverage.v5.2n.ssv[,1,1]
nullT.v5.2n.bsv <- obs.coverage.v5.2n.bsv[,1,1]
smalld.ve.ne <- obs.coverage.ve.ne[,2,1]
smalld.v2.ne <- obs.coverage.v2.ne[,2,1]
smalld.v5.ne <- obs.coverage.v5.ne[,2,1]
smalld.ve.1.5n <- obs.coverage.ve.1.5n[,2,1]
smalld.ve.2n <- obs.coverage.ve.2n[,2,1]
smalld.v2.1.5n.ssv <- obs.coverage.v2.1.5n.ssv[,2,1]
smalld.v2.1.5n.bsv <- obs.coverage.v2.1.5n.bsv[,2,1]
smalld.v2.2n.ssv <- obs.coverage.v2.2n.ssv[,2,1]
smalld.v2.2n.bsv <- obs.coverage.v2.2n.bsv[,2,1]
smalld.v5.1.5n.ssv <- obs.coverage.v5.1.5n.ssv[,2,1]
smalld.v5.1.5n.bsv <- obs.coverage.v5.1.5n.bsv[,2,1]
smalld.v5.2n.ssv <- obs.coverage.v5.2n.ssv[,2,1]
smalld.v5.2n.bsv <- obs.coverage.v5.2n.bsv[,2,1]
midd.ve.ne <- obs.coverage.ve.ne[,3,1]
midd.v2.ne <- obs.coverage.v2.ne[,3,1]
midd.v5.ne <- obs.coverage.v5.ne[,3,1]
midd.ve.1.5n <- obs.coverage.ve.1.5n[,3,1]
midd.ve.2n <- obs.coverage.ve.2n[,3,1]
midd.v2.1.5n.ssv <- obs.coverage.v2.1.5n.ssv[,3,1]
midd.v2.1.5n.bsv <- obs.coverage.v2.1.5n.bsv[,3,1]
midd.v2.2n.ssv <- obs.coverage.v2.2n.ssv[,3,1]
midd.v2.2n.bsv <- obs.coverage.v2.2n.bsv[,3,1]
midd.v5.1.5n.ssv <- obs.coverage.v5.1.5n.ssv[,3,1]
midd.v5.1.5n.bsv <- obs.coverage.v5.1.5n.bsv[,3,1]
midd.v5.2n.ssv <- obs.coverage.v5.2n.ssv[,3,1]
midd.v5.2n.bsv <- obs.coverage.v5.2n.bsv[,3,1]
bigd.ve.ne <- obs.coverage.ve.ne[,4,1]
bigd.v2.ne <- obs.coverage.v2.ne[,4,1]
bigd.v5.ne <- obs.coverage.v5.ne[,4,1]
bigd.ve.1.5n <- obs.coverage.ve.1.5n[,4,1]
bigd.ve.2n <- obs.coverage.ve.2n[,4,1]
bigd.v2.1.5n.ssv <- obs.coverage.v2.1.5n.ssv[,4,1]
bigd.v2.1.5n.bsv <- obs.coverage.v2.1.5n.bsv[,4,1]
bigd.v2.2n.ssv <- obs.coverage.v2.2n.ssv[,4,1]
bigd.v2.2n.bsv <- obs.coverage.v2.2n.bsv[,4,1]
bigd.v5.1.5n.ssv <- obs.coverage.v5.1.5n.ssv[,4,1]
bigd.v5.1.5n.bsv <- obs.coverage.v5.1.5n.bsv[,4,1]
bigd.v5.2n.ssv <- obs.coverage.v5.2n.ssv[,4,1]
bigd.v5.2n.bsv <- obs.coverage.v5.2n.bsv[,4,1]

## Put it in a dataframe
coverage.classic.nullT <- data.frame(
    coverage.rate=c(nullT.ve.ne, nullT.v2.ne, nullT.v2.ne, nullT.v5.ne, 
nullT.v5.ne, nullT.ve.1.5n, nullT.ve.2n, nullT.v2.1.5n.ssv, nullT.v2.1.5n.bsv, 
nullT.v2.2n.ssv, nullT.v2.2n.bsv, nullT.v5.1.5n.ssv, nullT.v5.1.5n.bsv, 
nullT.v5.2n.ssv, nullT.v5.2n.bsv), #NOTE: doubled any with equal variance or 
#equal ns because they don't have separate ssv and bsv versions
    
Ns=factor(c('20,20','50,50','100,100','20,20','50,50','100,100','20,20','50,50',
'100,100','20,20','50,50','100,100','20,20','50,50','100,100','20,30','50,75',
'100,150','20,40','50,100','100,200','20,30','50,75','100,150','20,30','50,75',
'100,150','20,40','50,100','100,200','20,40','50,100','100,200','20,30','50,75',
'100,150','20,30','50,75','100,150','20,40','50,100','100,200','20,40','50,100',
'100,200'), 
levels=c('20,20','20,30','20,40','50,50','50,75','50,100','100,100','100,150','100,200')),                                                                      
                                                                                
                                                                                
                                                                          
    var.ratio=factor(rep(c(0,-1,1,-2,2,0,0,-1,1,-1,1,-2,2,-2,2), each=3), 
levels=c(-2,-1,0,1,2), labels=c(.2,.5,1,2,5)))
coverage.classic.nullT <- 
coverage.classic.nullT[order(coverage.classic.nullT$Ns, 
coverage.classic.nullT$var.ratio),]

coverage.classic.smalld <- data.frame(
    coverage.rate=c(smalld.ve.ne, smalld.v2.ne, smalld.v2.ne, smalld.v5.ne, 
smalld.v5.ne, smalld.ve.1.5n, smalld.ve.2n, smalld.v2.1.5n.ssv, 
smalld.v2.1.5n.bsv, smalld.v2.2n.ssv, smalld.v2.2n.bsv, smalld.v5.1.5n.ssv, 
smalld.v5.1.5n.bsv, smalld.v5.2n.ssv, smalld.v5.2n.bsv), #NOTE: doubled any 
#with equal variance or equal ns because they don't have separate ssv and bsv 
#versions
    
Ns=factor(c('20,20','50,50','100,100','20,20','50,50','100,100','20,20','50,50',
'100,100','20,20','50,50','100,100','20,20','50,50','100,100','20,30','50,75',
'100,150','20,40','50,100','100,200','20,30','50,75','100,150','20,30','50,75',
'100,150','20,40','50,100','100,200','20,40','50,100','100,200','20,30','50,75',
'100,150','20,30','50,75','100,150','20,40','50,100','100,200','20,40','50,100',
'100,200'), 
levels=c('20,20','20,30','20,40','50,50','50,75','50,100','100,100','100,150','100,200')),                                                                      
                                                                                
                                                                                
                                                                          
    var.ratio=factor(rep(c(0,-1,1,-2,2,0,0,-1,1,-1,1,-2,2,-2,2), each=3), 
levels=c(-2,-1,0,1,2), labels=c(.2,.5,1,2,5)))
coverage.classic.smalld <- 
coverage.classic.smalld[order(coverage.classic.smalld$Ns, 
coverage.classic.smalld$var.ratio),]

coverage.classic.midd <- data.frame(
    coverage.rate=c(midd.ve.ne, midd.v2.ne, midd.v2.ne, midd.v5.ne, midd.v5.ne, 
midd.ve.1.5n, midd.ve.2n, midd.v2.1.5n.ssv, midd.v2.1.5n.bsv, midd.v2.2n.ssv, 
midd.v2.2n.bsv, midd.v5.1.5n.ssv, midd.v5.1.5n.bsv, midd.v5.2n.ssv, 
midd.v5.2n.bsv), #NOTE: doubled any with equal variance or equal ns because 
#they don't have separate ssv and bsv versions
    
Ns=factor(c('20,20','50,50','100,100','20,20','50,50','100,100','20,20','50,50',
'100,100','20,20','50,50','100,100','20,20','50,50','100,100','20,30','50,75',
'100,150','20,40','50,100','100,200','20,30','50,75','100,150','20,30','50,75',
'100,150','20,40','50,100','100,200','20,40','50,100','100,200','20,30','50,75',
'100,150','20,30','50,75','100,150','20,40','50,100','100,200','20,40','50,100',
'100,200'), 
levels=c('20,20','20,30','20,40','50,50','50,75','50,100','100,100','100,150',
'100,200')),                                                                      
                                                                                
                                                                                
                                                                          
    var.ratio=factor(rep(c(0,-1,1,-2,2,0,0,-1,1,-1,1,-2,2,-2,2), each=3), 
levels=c(-2,-1,0,1,2), labels=c(.2,.5,1,2,5)))
coverage.classic.midd <- coverage.classic.midd[order(coverage.classic.midd$Ns, 
coverage.classic.midd$var.ratio),]

coverage.classic.bigd <- data.frame(
    coverage.rate=c(bigd.ve.ne, bigd.v2.ne, bigd.v2.ne, bigd.v5.ne, bigd.v5.ne, 
bigd.ve.1.5n, bigd.ve.2n, bigd.v2.1.5n.ssv, bigd.v2.1.5n.bsv, bigd.v2.2n.ssv, 
bigd.v2.2n.bsv, bigd.v5.1.5n.ssv, bigd.v5.1.5n.bsv, bigd.v5.2n.ssv, 
bigd.v5.2n.bsv), #NOTE: doubled any with equal variance or equal ns because 
#they don't have separate ssv and bsv versions
    
Ns=factor(c('20,20','50,50','100,100','20,20','50,50','100,100','20,20','50,50',
'100,100','20,20','50,50','100,100','20,20','50,50','100,100','20,30','50,75',
'100,150','20,40','50,100','100,200','20,30','50,75','100,150','20,30','50,75',
'100,150','20,40','50,100','100,200','20,40','50,100','100,200','20,30','50,75',
'100,150','20,30','50,75','100,150','20,40','50,100','100,200','20,40','50,100',
'100,200'), 
levels=c('20,20','20,30','20,40','50,50','50,75','50,100','100,100','100,150',
'100,200')),                                                                      
                                                                                
                                                                                
                                                                          
    var.ratio=factor(rep(c(0,-1,1,-2,2,0,0,-1,1,-1,1,-2,2,-2,2), each=3), 
levels=c(-2,-1,0,1,2), labels=c(.2,.5,1,2,5)))
coverage.classic.bigd <- coverage.classic.bigd[order(coverage.classic.bigd$Ns, 
coverage.classic.bigd$var.ratio),]


## Set up classic t test plot
CovClassic.nullT <- ggplot(coverage.classic.nullT, aes(x=as.numeric(var.ratio), y=coverage.rate, ymin=.85, ymax=1)) + 
    geom_point(size=3, aes(shape = Ns)) + 
    scale_shape_manual(name = 'Sample size ratio',
                       labels = c('20, 20', '20, 30', '20, 40',
                                  '50, 50', '50, 75', '50, 100',
                                  '100, 100', '100, 150', '100, 200'),
                       values=rep(c(1,2,0), times = 3)
                       ) +
    geom_line(size=.5, aes(linetype = Ns)) + 
    scale_linetype_manual(name = 'Sample size ratio',
                          labels = c('20, 20', '20, 30', '20, 40',
                                     '50, 50', '50, 75', '50, 100',
                                     '100, 100', '100, 150', '100, 200'),
                          values=rep(c('dotted', 'dashed', 'solid'), each = 3)
                          ) +
#     scale_color_manual(name = 'Sample size ratio',
#                        labels = c('20, 20', '20, 30', '20, 40',
#                                   '50, 50', '50, 75', '50, 100',
#                                   '100, 100', '100, 150', '100, 200'),
#                        values=rep(c('deepskyblue4','darkgoldenrod1', 
# 'grey41'), each=3)
#                        ) + 
    scale_x_continuous(breaks=seq(-2:2), labels=factor(c('1/5', '1/2', '1', '2', '5'))) +
    scale_y_continuous(breaks=seq(from=.85, to=1, length.out = 16)) +
    xlab(bquote(sigma[1]^2~'/'~sigma[2]^2)) + ylab('Coverage Probability') +
    theme(panel.grid.minor.y = element_blank(), panel.grid.major.x = element_blank(), panel.grid.minor.x = element_blank()) +
  ggtitle("Student's t test")

CovClassic.smalld <- ggplot(coverage.classic.smalld, 
aes(x=as.numeric(var.ratio), y=coverage.rate, shape=Ns, ymin=.85, 
ymax=1)) + 
    geom_point(size=3) + scale_shape(solid=FALSE) + 
    scale_shape_manual(values=rep(c(1,2,0), 3)) +
    geom_line(size=.5) + 
#     scale_color_manual(values=rep(c('deepskyblue4','darkgoldenrod1', 
# 'grey41'), each=3)) + 
    scale_x_discrete(breaks=seq(-2:2), labels=c(.2,.5,1,2,5)) +
    scale_y_continuous(breaks=seq(from=.85, to=1, length.out=16)) +
    xlab(bquote('s'[1]^2~'/'~'s'[2]^2)) + ylab('Coverage Probability') + 
ggtitle("Student's t test")

CovClassic.midd <- ggplot(coverage.classic.midd, aes(x=as.numeric(var.ratio), 
y=coverage.rate, shape=Ns, ymin=.85, ymax=1)) + 
    geom_point(size=3) + scale_shape(solid=FALSE) + 
    scale_shape_manual(values=rep(c(1,2,0), 3)) +
    geom_line(size=.5) + 
#     scale_color_manual(values=rep(c('deepskyblue4','darkgoldenrod1', 
# 'grey41'), each=3)) + 
    scale_x_discrete(breaks=seq(-2:2), labels=c(.2,.5,1,2,5)) +
    scale_y_continuous(breaks=seq(from=.85, to=1, length.out=16)) +
    xlab(bquote('s'[1]^2~'/'~'s'[2]^2)) + ylab('Coverage Probability') + 
ggtitle("Student's t test")

CovClassic.bigd <- ggplot(coverage.classic.bigd, aes(x=as.numeric(var.ratio), 
y=coverage.rate, shape=Ns, ymin=.85, ymax=1)) + 
    geom_point(size=3) + scale_shape(solid=FALSE) + 
    scale_shape_manual(values=rep(c(1,2,0), 3)) +
    geom_line(size=.5) + 
#     scale_color_manual(values=rep(c('deepskyblue4','darkgoldenrod1', 
# 'grey41'), each=3)) + 
    scale_x_discrete(breaks=seq(-2:2), labels=c(.2,.5,1,2,5)) +
    scale_y_continuous(breaks=seq(from=.85, to=1, length.out=16)) +
    xlab(bquote('s'[1]^2~'/'~'s'[2]^2)) + ylab('Coverage Probability') + 
ggtitle("Student's t test")

@


<<WelchCoverage, echo=FALSE>>=

##### Plot Welch results only

## Set up data
nullT.ve.ne <- obs.coverage.ve.ne[,1,2]
nullT.v2.ne <- obs.coverage.v2.ne[,1,2]
nullT.v5.ne <- obs.coverage.v5.ne[,1,2]
nullT.ve.1.5n <- obs.coverage.ve.1.5n[,1,2]
nullT.ve.2n <- obs.coverage.ve.2n[,1,2]
nullT.v2.1.5n.ssv <- obs.coverage.v2.1.5n.ssv[,1,2]
nullT.v2.1.5n.bsv <- obs.coverage.v2.1.5n.bsv[,1,2]
nullT.v2.2n.ssv <- obs.coverage.v2.2n.ssv[,1,2]
nullT.v2.2n.bsv <- obs.coverage.v2.2n.bsv[,1,2]
nullT.v5.1.5n.ssv <- obs.coverage.v5.1.5n.ssv[,1,2]
nullT.v5.1.5n.bsv <- obs.coverage.v5.1.5n.bsv[,1,2]
nullT.v5.2n.ssv <- obs.coverage.v5.2n.ssv[,1,2]
nullT.v5.2n.bsv <- obs.coverage.v5.2n.bsv[,1,2]
smalld.ve.ne <- obs.coverage.ve.ne[,2,2]
smalld.v2.ne <- obs.coverage.v2.ne[,2,2]
smalld.v5.ne <- obs.coverage.v5.ne[,2,2]
smalld.ve.1.5n <- obs.coverage.ve.1.5n[,2,2]
smalld.ve.2n <- obs.coverage.ve.2n[,2,2]
smalld.v2.1.5n.ssv <- obs.coverage.v2.1.5n.ssv[,2,2]
smalld.v2.1.5n.bsv <- obs.coverage.v2.1.5n.bsv[,2,2]
smalld.v2.2n.ssv <- obs.coverage.v2.2n.ssv[,2,2]
smalld.v2.2n.bsv <- obs.coverage.v2.2n.bsv[,2,2]
smalld.v5.1.5n.ssv <- obs.coverage.v5.1.5n.ssv[,2,2]
smalld.v5.1.5n.bsv <- obs.coverage.v5.1.5n.bsv[,2,2]
smalld.v5.2n.ssv <- obs.coverage.v5.2n.ssv[,2,2]
smalld.v5.2n.bsv <- obs.coverage.v5.2n.bsv[,2,2]
midd.ve.ne <- obs.coverage.ve.ne[,3,2]
midd.v2.ne <- obs.coverage.v2.ne[,3,2]
midd.v5.ne <- obs.coverage.v5.ne[,3,2]
midd.ve.1.5n <- obs.coverage.ve.1.5n[,3,2]
midd.ve.2n <- obs.coverage.ve.2n[,3,2]
midd.v2.1.5n.ssv <- obs.coverage.v2.1.5n.ssv[,3,2]
midd.v2.1.5n.bsv <- obs.coverage.v2.1.5n.bsv[,3,2]
midd.v2.2n.ssv <- obs.coverage.v2.2n.ssv[,3,2]
midd.v2.2n.bsv <- obs.coverage.v2.2n.bsv[,3,2]
midd.v5.1.5n.ssv <- obs.coverage.v5.1.5n.ssv[,3,2]
midd.v5.1.5n.bsv <- obs.coverage.v5.1.5n.bsv[,3,2]
midd.v5.2n.ssv <- obs.coverage.v5.2n.ssv[,3,2]
midd.v5.2n.bsv <- obs.coverage.v5.2n.bsv[,3,2]
bigd.ve.ne <- obs.coverage.ve.ne[,4,2]
bigd.v2.ne <- obs.coverage.v2.ne[,4,2]
bigd.v5.ne <- obs.coverage.v5.ne[,4,2]
bigd.ve.1.5n <- obs.coverage.ve.1.5n[,4,2]
bigd.ve.2n <- obs.coverage.ve.2n[,4,2]
bigd.v2.1.5n.ssv <- obs.coverage.v2.1.5n.ssv[,4,2]
bigd.v2.1.5n.bsv <- obs.coverage.v2.1.5n.bsv[,4,2]
bigd.v2.2n.ssv <- obs.coverage.v2.2n.ssv[,4,2]
bigd.v2.2n.bsv <- obs.coverage.v2.2n.bsv[,4,2]
bigd.v5.1.5n.ssv <- obs.coverage.v5.1.5n.ssv[,4,2]
bigd.v5.1.5n.bsv <- obs.coverage.v5.1.5n.bsv[,4,2]
bigd.v5.2n.ssv <- obs.coverage.v5.2n.ssv[,4,2]
bigd.v5.2n.bsv <- obs.coverage.v5.2n.bsv[,4,2]

## Put it in a dataframe
coverage.welch.nullT <- data.frame(
    coverage.rate=c(nullT.ve.ne, nullT.v2.ne, nullT.v2.ne, nullT.v5.ne, 
nullT.v5.ne, nullT.ve.1.5n, nullT.ve.2n, nullT.v2.1.5n.ssv, nullT.v2.1.5n.bsv, 
nullT.v2.2n.ssv, nullT.v2.2n.bsv, nullT.v5.1.5n.ssv, nullT.v5.1.5n.bsv, 
nullT.v5.2n.ssv, nullT.v5.2n.bsv), #NOTE: doubled any with equal variance or 
#equal ns because they don't have separate ssv and bsv versions
    
Ns=factor(c('20,20','50,50','100,100','20,20','50,50','100,100','20,20','50,50',
'100,100','20,20','50,50','100,100','20,20','50,50','100,100','20,30','50,75',
'100,150','20,40','50,100','100,200','20,30','50,75','100,150','20,30','50,75',
'100,150','20,40','50,100','100,200','20,40','50,100','100,200','20,30','50,75',
'100,150','20,30','50,75','100,150','20,40','50,100','100,200','20,40','50,100',
'100,200'), 
levels=c('20,20','20,30','20,40','50,50','50,75','50,100','100,100','100,150',
'100,200')),                                                                      
                                                                                
                                                                                
                                                                          
    var.ratio=factor(rep(c(0,-1,1,-2,2,0,0,-1,1,-1,1,-2,2,-2,2), each=3), 
levels=c(-2,-1,0,1,2), labels=c(.2,.5,1,2,5)))
coverage.welch.nullT <- coverage.welch.nullT[order(coverage.welch.nullT$Ns, 
coverage.welch.nullT$var.ratio),]

coverage.welch.smalld <- data.frame(
    coverage.rate=c(smalld.ve.ne, smalld.v2.ne, smalld.v2.ne, smalld.v5.ne, 
smalld.v5.ne, smalld.ve.1.5n, smalld.ve.2n, smalld.v2.1.5n.ssv, 
smalld.v2.1.5n.bsv, smalld.v2.2n.ssv, smalld.v2.2n.bsv, smalld.v5.1.5n.ssv, 
smalld.v5.1.5n.bsv, smalld.v5.2n.ssv, smalld.v5.2n.bsv), #NOTE: doubled any 
#with equal variance or equal ns because they don't have separate ssv and bsv 
#versions
    
Ns=factor(c('20,20','50,50','100,100','20,20','50,50','100,100','20,20','50,50',
'100,100','20,20','50,50','100,100','20,20','50,50','100,100','20,30','50,75',
'100,150','20,40','50,100','100,200','20,30','50,75','100,150','20,30','50,75',
'100,150','20,40','50,100','100,200','20,40','50,100','100,200','20,30','50,75',
'100,150','20,30','50,75','100,150','20,40','50,100','100,200','20,40','50,100',
'100,200'), 
levels=c('20,20','20,30','20,40','50,50','50,75','50,100','100,100','100,150',
'100,200')),                                                                      
                                                                                
                                                                                
                                                                          
    var.ratio=factor(rep(c(0,-1,1,-2,2,0,0,-1,1,-1,1,-2,2,-2,2), each=3), 
levels=c(-2,-1,0,1,2), labels=c(.2,.5,1,2,5)))
coverage.welch.smalld <- coverage.welch.smalld[order(coverage.welch.smalld$Ns, 
coverage.welch.smalld$var.ratio),]

coverage.welch.midd <- data.frame(
    coverage.rate=c(midd.ve.ne, midd.v2.ne, midd.v2.ne, midd.v5.ne, midd.v5.ne, 
midd.ve.1.5n, midd.ve.2n, midd.v2.1.5n.ssv, midd.v2.1.5n.bsv, midd.v2.2n.ssv, 
midd.v2.2n.bsv, midd.v5.1.5n.ssv, midd.v5.1.5n.bsv, midd.v5.2n.ssv, 
midd.v5.2n.bsv), #NOTE: doubled any with equal variance or equal ns because 
#they don't have separate ssv and bsv versions
    
Ns=factor(c('20,20','50,50','100,100','20,20','50,50','100,100','20,20','50,50',
'100,100','20,20','50,50','100,100','20,20','50,50','100,100','20,30','50,75',
'100,150','20,40','50,100','100,200','20,30','50,75','100,150','20,30','50,75',
'100,150','20,40','50,100','100,200','20,40','50,100','100,200','20,30','50,75',
'100,150','20,30','50,75','100,150','20,40','50,100','100,200','20,40','50,100',
'100,200'), 
levels=c('20,20','20,30','20,40','50,50','50,75','50,100','100,100','100,150',
'100,200')),                                                                      
                                                                                
                                                                                
                                                                          
    var.ratio=factor(rep(c(0,-1,1,-2,2,0,0,-1,1,-1,1,-2,2,-2,2), each=3), 
levels=c(-2,-1,0,1,2), labels=c(.2,.5,1,2,5)))
coverage.welch.midd <- coverage.welch.midd[order(coverage.welch.midd$Ns, 
coverage.welch.midd$var.ratio),]

coverage.welch.bigd <- data.frame(
    coverage.rate=c(bigd.ve.ne, bigd.v2.ne, bigd.v2.ne, bigd.v5.ne, bigd.v5.ne, 
bigd.ve.1.5n, bigd.ve.2n, bigd.v2.1.5n.ssv, bigd.v2.1.5n.bsv, bigd.v2.2n.ssv, 
bigd.v2.2n.bsv, bigd.v5.1.5n.ssv, bigd.v5.1.5n.bsv, bigd.v5.2n.ssv, 
bigd.v5.2n.bsv), #NOTE: doubled any with equal variance or equal ns because 
#they don't have separate ssv and bsv versions
    
Ns=factor(c('20,20','50,50','100,100','20,20','50,50','100,100','20,20','50,50',
'100,100','20,20','50,50','100,100','20,20','50,50','100,100','20,30','50,75',
'100,150','20,40','50,100','100,200','20,30','50,75','100,150','20,30','50,75',
'100,150','20,40','50,100','100,200','20,40','50,100','100,200','20,30','50,75',
'100,150','20,30','50,75','100,150','20,40','50,100','100,200','20,40','50,100',
'100,200'), 
levels=c('20,20','20,30','20,40','50,50','50,75','50,100','100,100','100,150',
'100,200')),                                                                      
                                                                                
                                                                                
                                                                          
    var.ratio=factor(rep(c(0,-1,1,-2,2,0,0,-1,1,-1,1,-2,2,-2,2), each=3), 
levels=c(-2,-1,0,1,2), labels=c(.2,.5,1,2,5)))
coverage.welch.bigd <- coverage.welch.bigd[order(coverage.welch.bigd$Ns, 
coverage.welch.bigd$var.ratio),]


## Set up welch t test plot
CovWelch.nullT <- ggplot(coverage.welch.nullT, aes(x=as.numeric(var.ratio), y=coverage.rate, ymin=.85, ymax=1)) + 
    geom_point(size=3, aes(shape = Ns)) + 
    scale_shape_manual(name = 'Sample size ratio',
                       labels = c('20, 20', '20, 30', '20, 40',
                                  '50, 50', '50, 75', '50, 100',
                                  '100, 100', '100, 150', '100, 200'),
                       values=rep(c(1,2,0), times = 3)
                       ) +
    geom_line(size=.5, aes(linetype = Ns)) + 
    scale_linetype_manual(name = 'Sample size ratio',
                          labels = c('20, 20', '20, 30', '20, 40',
                                     '50, 50', '50, 75', '50, 100',
                                     '100, 100', '100, 150', '100, 200'),
                          values=rep(c('dotted', 'dashed', 'solid'), each = 3)
                          ) +
#     scale_color_manual(name = 'Sample size ratio',
#                        labels = c('20, 20', '20, 30', '20, 40',
#                                   '50, 50', '50, 75', '50, 100',
#                                   '100, 100', '100, 150', '100, 200'),
#                        values=rep(c('deepskyblue4','darkgoldenrod1', 
# 'grey41'), each=3)
#                        ) + 
    scale_x_continuous(breaks=seq(-2:2), labels=factor(c('1/5', '1/2', '1', '2', '5'))) +
    scale_y_continuous(breaks=seq(from=.85, to=1, length.out = 16)) +
    xlab(bquote(sigma[1]^2~'/'~sigma[2]^2)) + ylab('Coverage Probability') +
    theme(panel.grid.minor.y = element_blank(), panel.grid.major.x = element_blank(), panel.grid.minor.x = element_blank()) +
  ggtitle("Welch's t test")

CovWelch.smalld <- ggplot(coverage.welch.smalld, aes(x=as.numeric(var.ratio), 
y=coverage.rate, shape=Ns, ymin=.85, ymax=1)) + 
    geom_point(size=3) + scale_shape(solid=FALSE) + 
    scale_shape_manual(values=rep(c(1,2,0), 3)) +
    geom_line(size=.5) + 
#     scale_color_manual(values=rep(c('deepskyblue4','darkgoldenrod1', 
# 'grey41'), each=3)) + 
    scale_x_discrete(breaks=seq(-2:2), labels=c(.2,.5,1,2,5)) +
    scale_y_continuous(breaks=seq(from=.85, to=1, length.out=16)) +
    xlab(bquote('s'[1]^2~'/'~'s'[2]^2)) + ylab('Coverage Probability') + 
ggtitle("Welch's t test")

CovWelch.midd <- ggplot(coverage.welch.midd, aes(x=as.numeric(var.ratio), 
y=coverage.rate, shape=Ns, ymin=.85, ymax=1)) + 
    geom_point(size=3) + scale_shape(solid=FALSE) + 
    scale_shape_manual(values=rep(c(1,2,0), 3)) +
    geom_line(size=.5) + 
#     scale_color_manual(values=rep(c('deepskyblue4','darkgoldenrod1', 
# 'grey41'), each=3)) + 
    scale_x_discrete(breaks=seq(-2:2), labels=c(.2,.5,1,2,5)) +
    scale_y_continuous(breaks=seq(from=.85, to=1, length.out=16)) +
    xlab(bquote('s'[1]^2~'/'~'s'[2]^2)) + ylab('Coverage Probability') + 
ggtitle("Welch's t test")

CovWelch.bigd <- ggplot(coverage.welch.bigd, aes(x=as.numeric(var.ratio), 
y=coverage.rate, shape=Ns, ymin=.85, ymax=1)) + 
    geom_point(size=3) + scale_shape(solid=FALSE) + 
    scale_shape_manual(values=rep(c(1,2,0), 3)) +
    geom_line(size=.5) + 
#     scale_color_manual(values=rep(c('deepskyblue4','darkgoldenrod1', 
# 'grey41'), each=3)) + 
    scale_x_discrete(breaks=seq(-2:2), labels=c(.2,.5,1,2,5)) +
    scale_y_continuous(breaks=seq(from=.85, to=1, length.out=16)) +
    xlab(bquote('s'[1]^2~'/'~'s'[2]^2)) + ylab('Coverage Probability') + 
ggtitle("Welch's t test")

@

\begin{figure*}
<<CoveragePlots, echo=FALSE, fig=TRUE>>=
layout <- matrix(c(1,2), nrow=2, byrow=TRUE)
multiplot(CovClassic.nullT, CovWelch.nullT, layout=layout)
@

\textit{Figure 6.} Coverage probabilities for Student's and Welch's t tests.
\end{figure*}

\subsection{Expanding to Interactions in 2 x 2 Factorial Designs}

The use of t tests can be expanded to study designs with more than two groups, where they can be used to test main effects, interactions, or other specific contrasts between the groups. For these tests, the researcher again has the choice of Student's or Welch's t test. To illustrate, consider the 2 x 2 design displayed in Figure 7. There are no main effects, but there is a cross-over interaction. The difference in means between groups B and C vs. groups A and D is the same as the difference for a medium-sized effect from simulations with two groups. We tested the 2 x 2 interaction using Student's and Welch's t tests when all sample sizes and variances were equal and when the sample size and variance of just one group (group A) differed from the others.

\begin{figure*}
<<plot2x2, echo = FALSE, fig = TRUE>>=
lowmean <- 6
highmean <- 6.71

int.means <- c(6, 6.71, 6.71, 6)
factor1 <- c(-1, -1, 1, 1)
factor2 <- factor(c(-1, 1, -1, 1))
int.data <- data.frame(factor1, factor2, int.means)
ggplot(int.data) + 
  geom_line(aes(y = int.means, x = factor1, linetype = factor2), color = 'black') +
  geom_point(aes(y = int.means, x = factor1, shape = 1, size = 3)) +
  scale_shape_identity() +
  coord_cartesian(ylim = c(5, 8), xlim = c(-1.25, 1.25)) +
  theme_classic() +
  scale_linetype_manual(values = c('solid', 'dashed')) +
  guides(linetype = FALSE, shape = FALSE, size = FALSE) +
  labs(x = NULL, y = NULL) +
  scale_x_continuous(breaks = c(-1, 1), labels = NULL) +
  annotate('text', label = c('A', 'B', 'C', 'D'), x = c(-1.1, -1.1, 1.1, 1.1), y = c(6, 6.71, 6.71, 6))
            
  
@

\textit{Figure 7.} Means in a 2 x 2 design with a cross-over interaction.
\end{figure*}



<<interaction data type1, echo = FALSE>>=
##### Student's t test data
## Set up data
nullT.ve.ne <- int.reject.null.ve.ne[,1,1]
nullT.v5.ne.bigvar <- int.reject.null.v5.ne.bigvar[,1,1]
nullT.v5.ne.smallvar <- int.reject.null.v5.ne.smallvar[,1,1]
nullT.ve.n2.bign <- int.reject.null.ve.n2.bign[,1,1]
nullT.ve.n2.smalln <- int.reject.null.ve.n2.smalln[,1,1]
nullT.v5.n2.bignbigvar <- int.reject.null.v5.n2.bignbigvar[,1,1]
nullT.v5.n2.bignsmallvar <- int.reject.null.v5.n2.bignsmallvar[,1,1]
nullT.v5.n2.smallnbigvar <- int.reject.null.v5.n2.smallnbigvar[,1,1]
nullT.v5.n2.smallnsmallvar <- int.reject.null.v5.n2.smallnsmallvar[,1,1]

midd.ve.ne <- int.reject.null.ve.ne[,2,1]
midd.v5.ne.bigvar <- int.reject.null.v5.ne.bigvar[,2,1]
midd.v5.ne.smallvar <- int.reject.null.v5.ne.smallvar[,2,1]
midd.ve.n2.bign <- int.reject.null.ve.n2.bign[,2,1]
midd.ve.n2.smalln <- int.reject.null.ve.n2.smalln[,2,1]
midd.v5.n2.bignbigvar <- int.reject.null.v5.n2.bignbigvar[,2,1]
midd.v5.n2.bignsmallvar <- int.reject.null.v5.n2.bignsmallvar[,2,1]
midd.v5.n2.smallnbigvar <- int.reject.null.v5.n2.smallnbigvar[,2,1]
midd.v5.n2.smallnsmallvar <- int.reject.null.v5.n2.smallnsmallvar[,2,1]

## Put it in a dataframe
int.type1.classic <- data.frame(
    type1.rate=c(nullT.ve.ne, nullT.v5.ne.bigvar, nullT.v5.ne.smallvar, nullT.ve.n2.bign, nullT.ve.n2.smalln, nullT.v5.n2.bignbigvar, nullT.v5.n2.bignsmallvar, nullT.v5.n2.smallnbigvar, nullT.v5.n2.smallnsmallvar),
    
Ns=factor(c('50,50,50,50','50,50,50,50','50,50,50,50','100,50,50,50','50,100,100,100','100,50,50,50','100,50,50,50','50,100,100,100','50,100,100,100'), 
levels=c('50,50,50,50','100,50,50,50','50,100,100,100')),
    var.ratio=factor(c(0,1,-1,0,0,1,-1,1,-1), 
levels=c(-1,0,1), labels=c(.2,0,5)))

## Plot it
T1classic <- ggplot(int.type1.classic, aes(x=as.numeric(var.ratio), y=type1.rate, ymin=0, ymax=.13)) + 
    geom_point(size=4, aes(shape = Ns, color = Ns)) + 
    scale_shape_manual(name = 'Sample size ratio \nGroup A,B,C,D',
                       labels = c('50,50,50,50', '100,50,50,50','50,100,100,100'),
                       values= rep(1, 3)
                       ) +
    geom_line(size=.5, aes(linetype = Ns, color = Ns)) + 
    scale_linetype_manual(name = 'Sample size ratio \nGroup A,B,C,D',
                          labels = c('50,50,50,50', '100,50,50,50','50,100,100,100'),
                          values= c('dotted', 'dashed', 'solid')
                          ) +
    scale_color_manual(name = 'Sample size ratio \nGroup A,B,C,D',
                       labels = c('50,50,50,50', '100,50,50,50','50,100,100,100'),
                       values= rep('black', 3)
                       ) + 
    scale_x_continuous(breaks=seq(-1:1), labels=factor(c('1/5', '1', '5'))) +
    scale_y_continuous(breaks=seq(from=0, to=.13, by=.01)) +
    xlab(bquote(frac(sigma[A]^2,{sigma[B]^2 == sigma[C]^2} == sigma[D]^2))) + ylab('Type I Error Rate') +
    theme(panel.grid.minor.y = element_blank(), panel.grid.major.x = element_blank(), panel.grid.minor.x = element_blank())









##### Welch's t test data
## Set up data
nullT.ve.ne <- int.reject.null.ve.ne[,1,2]
nullT.v5.ne.bigvar <- int.reject.null.v5.ne.bigvar[,1,2]
nullT.v5.ne.smallvar <- int.reject.null.v5.ne.smallvar[,1,2]
nullT.ve.n2.bign <- int.reject.null.ve.n2.bign[,1,2]
nullT.ve.n2.smalln <- int.reject.null.ve.n2.smalln[,1,2]
nullT.v5.n2.bignbigvar <- int.reject.null.v5.n2.bignbigvar[,1,2]
nullT.v5.n2.bignsmallvar <- int.reject.null.v5.n2.bignsmallvar[,1,2]
nullT.v5.n2.smallnbigvar <- int.reject.null.v5.n2.smallnbigvar[,1,2]
nullT.v5.n2.smallnsmallvar <- int.reject.null.v5.n2.smallnsmallvar[,1,2]

midd.ve.ne <- int.reject.null.ve.ne[,2,2]
midd.v5.ne.bigvar <- int.reject.null.v5.ne.bigvar[,2,2]
midd.v5.ne.smallvar <- int.reject.null.v5.ne.smallvar[,2,2]
midd.ve.n2.bign <- int.reject.null.ve.n2.bign[,2,2]
midd.ve.n2.smalln <- int.reject.null.ve.n2.smalln[,2,2]
midd.v5.n2.bignbigvar <- int.reject.null.v5.n2.bignbigvar[,2,2]
midd.v5.n2.bignsmallvar <- int.reject.null.v5.n2.bignsmallvar[,2,2]
midd.v5.n2.smallnbigvar <- int.reject.null.v5.n2.smallnbigvar[,2,2]
midd.v5.n2.smallnsmallvar <- int.reject.null.v5.n2.smallnsmallvar[,2,2]

## Put it in a dataframe
int.type1.welch <- data.frame(
    type1.rate=c(nullT.ve.ne, nullT.v5.ne.bigvar, nullT.v5.ne.smallvar, nullT.ve.n2.bign, nullT.ve.n2.smalln, nullT.v5.n2.bignbigvar, nullT.v5.n2.bignsmallvar, nullT.v5.n2.smallnbigvar, nullT.v5.n2.smallnsmallvar),
    
Ns=factor(c('50,50,50,50','50,50,50,50','50,50,50,50','100,50,50,50','50,100,100,100','100,50,50,50','100,50,50,50','50,100,100,100','50,100,100,100'), 
levels=c('50,50,50,50','100,50,50,50','50,100,100,100')),
    var.ratio=factor(c(0,1,-1,0,0,1,-1,1,-1), 
levels=c(-1,0,1), labels=c(.2,0,5)))

## Plot it
T1welch <- ggplot(int.type1.welch, aes(x=as.numeric(var.ratio), y=type1.rate, ymin=0, ymax=.13)) + 
    geom_point(size=4, aes(shape = Ns, color = Ns)) + 
    scale_shape_manual(name = 'Sample size ratio \nGroup A,B,C,D',
                       labels = c('50,50,50,50', '100,50,50,50','50,100,100,100'),
                       values= rep(1, 3)
                       ) +
    geom_line(size=.5, aes(linetype = Ns, color = Ns)) + 
    scale_linetype_manual(name = 'Sample size ratio \nGroup A,B,C,D',
                          labels = c('50,50,50,50', '100,50,50,50','50,100,100,100'),
                          values= c('dotted', 'dashed', 'solid')
                          ) +
    scale_color_manual(name = 'Sample size ratio \nGroup A,B,C,D',
                       labels = c('50,50,50,50', '100,50,50,50','50,100,100,100'),
                       values= rep('black', 3)
                       ) + 
    scale_x_continuous(breaks=seq(-1:1), labels=factor(c('1/5', '1', '5'))) +
    scale_y_continuous(breaks=seq(from=0, to=.13, by=.01)) +
    xlab(bquote(frac(sigma[A]^2,{sigma[B]^2 == sigma[C]^2} == sigma[D]^2))) + ylab('Type I Error Rate') +
    theme(panel.grid.minor.y = element_blank(), panel.grid.major.x = element_blank(), panel.grid.minor.x = element_blank())

@

\begin{figure*}    
<<Type1Int, echo=FALSE, fig=TRUE, height=10>>=
multiplot(T1classic+ggtitle("Student's t test"), T1welch+ggtitle("Welch's t test"))
@

\textit{Figure 8.} Type I error rates for Student's t test and Welch's t test 
of a 2 x 2 interaction as the variance and sample size of one group varies.
\end{figure*}




\section{Discussion}
    We set out to find a simple rule to help researchers decide when to use 
Student's t test and when to use Welch's t test. We believe that the simplest 
rule is to always use Welch's t test to compare the means of independent groups.
    
    The results of our simulations demonstrated that when the population 
variances or sample sizes were equal, using Welch's t test instead of Student's 
t test didn't hurt. Figure 1 shows that the Welch's t test degrees of freedom could 
drop below 70\% of the Student's t test degrees of freedom when only the variance 
ratio or the sample size ratio changed. Nevertheless, the false positive rates, 
power, and coverage probabilities of the two tests were almost identical under 
these conditions. The difference in the degrees of freedom of the two tests had a 
negligible effect on the outcomes. 

% \begin{figure*}  
% << tdist, echo=FALSE, fig=TRUE>>=
% 
% #plot t distributions with different degrees of freedom - this needs to go 
% #somewhere
% 
% x <- rep(seq(-4,4,.1), 2)
% y <- c(dt(seq(-4,4,.1),df=38), dt(seq(-4,4,.1),df=38/2))
% group <- factor(rep(c('df = 38', 'df = 19'), each=81))
% 
% ggplot(data.frame(x,y,group), aes(y=y, x=x, fill=group)) + 
% geom_line(aes(colour=group)) + labs(x=NULL,y=NULL,colour=NULL)
% @
% 
% \textit{Figure 7. Two t distributions with different degrees of freedom.}
% \end{figure*}
    
    More important than the degrees of freedom is the standard error, which 
affects the t-value itself. When either the variances or the sample sizes are 
equal, the pooled standard error of Student's t test and the separate variances 
standard error of Welch's t test are identical, and the two tests will 
generally agree with each other. However, when both the variances and the 
sample sizes are unequal, the pooled standard error of Student's t test gives 
more weight to the group with the larger sample size, as has been discussed by 
others \cite<e.g.,>{Coombs1996,Zimmerman2009}; if that group has the larger 
variance, then Student's t test becomes more conservative, but if that group 
has the smaller variance, then Student's t test becomes more liberal. In 
contrast, Welch's t test was more stable, regardless of which sample had the 
larger variance. This can be seen in Figures 3 and 6, where the type I error 
rate and coverage probability vary widely for Student's t test, but remain 
stable for Welch's t test.
    
    The biggest benefit of Student's t test was that it had more power than 
Welch's t test to detect true effects when the larger sample had the smaller 
variance---yet under these same conditions, it had an inflated type I error 
rate and the lowest coverage probability. Far from being underpowered, Welch's 
t test was more powerful than Student's t test when the larger sample had the 
larger variance, but under these same conditions it retained the expected type 
I error rate and coverage probability. Overall, Welch's t test did a better job 
of balancing researcher concerns about false positives, power, and estimation.
    
    We believe that researchers are more likely to use simple decision rules, 
and so we echo the recommendations of previous researchers to always use 
Welch's t test \cite{Zimmerman1996,Moser1992,Moser1989}. For researchers who 
insist on using Student's t test, if they run experiments, then there is some 
good news. With experiments, subjects are usually assigned evenly to 
conditions, and when sample sizes are equal the two tests perform equally well. 
However, for those whose research involves comparing pre-existing groups, it 
might not be possible to have equal sample sizes. Welch's t test is likely to 
outperform Student's t test because the sample variances are unlikely to be 
exactly equal. If a researcher still prefers a complex rule where using 
Student's t test is still possible, then one option is to examine boxplots to 
determine whether it is reasonable to assume that the group variances are 
equal. This judgment will be easier if the sample sizes are large. 
Nevertheless, researchers should rest assured that they won't suffer from using 
the simple rule to always use Welch's t test.
    
    We examined whether the ratio of the degrees of freedom from Welch's t test 
to the degrees of freedom from Student's t test could provide a  heuristic 
to determine whether or not the group variances are unequal. The rule had the 
greatest potential when it mattered the least---when the sample sizes of the 
two groups were equal. When the sample sizes were unequal, the degrees of 
freedom ratio was not a useful indicator of unequal variances. It could 
become  high for some variance and sample size ratios  and it could drop  low for other variance and sample size ratios. There seems to be no simple rule for deciding between 
the two tests based on the ratio of the degrees of freedom. Fortunately, the 
rule to always use Welch's t test is simpler and more effective.
    
    Prior simulation work comparing Student's t test to Welch's t test has 
focused on null hypothesis significance testing by emphasizing type I errors 
and power \cite{Boneau1960, Neuhauser2002, Zimmerman1993, Zimmerman2004, 
Zimmerman1996, Zimmerman2009}, but there are important implications of this work 
for effect size estimation as well. Some have called for researchers to report 
effect sizes and confidence intervals to address some of the limitations of 
merely reporting null hypothesis significance tests. It is important to remember 
that effect size estimation also involves assumptions about group variances.
    
    If the effect size of interest is the difference in
group means and its confidence interval, you must make a decision about whether or not to assume that the
group variances are equal. We found that using Student's t test to find the confidence
interval for the difference in group means, which involves the assumption that the 
group variances are equal, led to 
 unstable estimates. Under some conditions, the confidence intervals were 
less accurate than expected because they were too narrow, and under other 
conditions, they were more accurate than expected because they were too wide. 
Using Welch's t test, which does not assume that group variances are equal, led 
to more stable estimates across conditions.
    
    Cohen's d \cite{Cohen1992} is the most popular effect size for reporting 
the difference in two group means, but it also requires an assumption that the 
group variances are equal. Cohen's d standardizes the difference in means based on a 
common standard deviation of the population. This common standard deviation
is just the square root of the pooled variance from Student's t test.
But when the group variances are unequal, there is no common standard deviation. 
Cohen's d will suffer from the same problems as Student's t test. Given the same difference in group 
means, if the sample sizes and variances both differ, then Cohen's d will give more 
weight to the larger sample when pooling the variance. If the larger sample has the larger variance, the 
estimate of the standardized effect size will be smaller than if the larger sample 
has the smaller variance. In reality, either estimate is misleading
because there is no common standard deviation, so there can be no traditional Cohen's d.
    
    Standardized effect sizes such as Cohen's d are often desirable for their 
use in meta-analysis or other attempts to summarize the size of an effect based 
on a comprehensive body of research. But the equal variances 
problem applies to meta-analysis as well. Using Cohen's d as the basis for a 
meta-analysis involves an implicit assumption that the group variances across 
the body of research are equal, an assumption which might be untenable in many 
cases. Differences in group variances might not just be a nuisance 
that gets in the way of building a comprehensive knowledge in an area of study, but they might 
actually be an interesting part of the effect for meta-analysts to examine. The effect of the independent variable may be on the variances (scale) and not merely the means (location).
    
    The good news is that effect sizes are not just standardized differences in 
means, but they also include raw difference in means 
\cite{Cumming2014, Kelley2012}, and you can find confidence intervals around raw differences 
in means without assuming that group variances are equal. In fact, 95\% confidence intervals
based on Welch's t test are included in the 
default output of data analysis programs such as SPSS and R. Reporting 
descriptive statistics in their original scale might be a better practice than 
reporting only standardized effect sizes anyway, for the following reasons. First, unlike Cohen's d, reporting raw descriptive statistics does not require the researcher to commit to an 
equal variances assumption. Second, it provides all of the necessary 
information for others who want to assume equal variances to find Cohen's d, or 
one of the alternative standardized effect sizes that have been recommended \cite{Peng2013, 
Grissom2001}. Third, it allows other researchers to examine whether differences 
in group variances are a consistent part of an effect, which would be lost by 
just reporting the standardized difference.

    We suspect that most statistics courses in psychology teach Student's t 
test thoroughly and only briefly touch on Welch's t test, if they teach it at 
all. Indeed, we have heard colleagues complain that when they use Welch's t test in a 
manuscript, reviewers are suspicious of the 
degrees of freedom with decimals. These reviewers must not have learned that 
degrees of freedom with decimals are the norm for Welch's t test and related methods such as corrections used for assumptions in repeated measures ANOVA \cite{Gonzalez2008}. The emphasis 
on Student's t test in teaching is consistent with the common strategy to start 
out by assuming that variances are equal and to only use Welch's t test as a 
back-up when the assumption has been violated. But why should we spend so much 
time on the equal variances assumption in the first place? Why not teach 
Welch's t test at the outset without imposing 
restrictive assumptions? Student's t test could still be taught briefly so that 
students understand the existing literature, but we believe it would be 
beneficial to emphasize Welch's t test as the default approach. As demonstrated 
in our simulations, this approach will help lead to better decision-making when 
it comes to analyzing data. 

Our discussion has focused on the relatively simple case of testing the means from two independent groups.
The simulations show that the pattern is not straightforward and depends on complex combinations between which group has the larger sample size and which group has the larger variance.  In the case of two groups we arrived at a simple conclusion that on balance when considering the false positive rate and power a strategy that always uses the Welch t test is an efficient one. As the number of groups increases and as the study design becomes more complicated such as with repeated measures, random effect models, complex contrasts, etc., then the story will likely be even more complicated. We should move toward an overall analysis strategy that has relatively few moving parts so that it can be described easily in a scientific report, is relatively easy to describe in a  preregistration plan and provides robust estimates.


<< junk, echo=FALSE >>=

####################### boxplots of the simulated df ratios 

## Equal ns

# equal vars, equal ns
#Bplot.ve.ne.ns20 <- qplot(y=dfs.list$ve.ns20.ne.nullT, geom='boxplot', x=' ', 
#ylab=bquote(frac('df'['Welch'],'df'['Student'])), 
#xlab=bquote(frac('n'[1],'n'[2])~'='~frac(20,20)~', 
#'~frac('var'[1],'var'[2])~'='~1), ylim=c(.5,1))
#Bplot.ve.ne.ns50 <- qplot(y=dfs.list$ve.ns50.ne.nullT, geom='boxplot', x=' ', 
#ylab=bquote(frac('df'['Welch'],'df'['Student'])), 
#xlab=bquote(frac('n'[1],'n'[2])~'='~frac(50,50)~', 
#'~frac('var'[1],'var'[2])~'='~1), ylim=c(.5,1))
#Bplot.ve.ne.ns100 <- qplot(y=dfs.list$ve.ns100.ne.nullT, geom='boxplot', x=' 
#', ylab=bquote(frac('df'['Welch'],'df'['Student'])), 
#xlab=bquote(frac('n'[1],'n'[2])~'='~frac(100,100)~', 
#'~frac('var'[1],'var'[2])~'='~1), ylim=c(.5,1))

#grid.arrange(Bplot.ve.ne.ns20, Bplot.ve.ne.ns50, Bplot.ve.ne.ns100, ncol=3, 
#heights=unit(.5, 'npc'), main='Figure 3. Degrees of Freedom Ratios with 
#Increasing Sample Sizes')



# different vars, equal ns
#Bplot.ve.ne.ns50 <- qplot(y=dfs.list$ve.ns50.ne.nullT, geom='boxplot', x=' ', 
#ylab=bquote(frac('df'['Welch'],'df'['Student'])), 
#xlab=bquote(frac('n'[1],'n'[2])~'='~frac(50,50)~', 
#'~frac('var'[1],'var'[2])~'='~1), ylim=c(.5,1))
#Bplot.v2.ne.ns50 <- qplot(y=dfs.list$v2.ns50.ne.nullT, geom='boxplot', x=' ', 
#ylab=bquote(frac('df'['Welch'],'df'['Student'])), 
#xlab=bquote(frac('n'[1],'n'[2])~'='~frac(50,50)~', 
#'~frac('var'[1],'var'[2])~'='~frac(1,2)), ylim=c(.5,1))
#Bplot.v5.ne.ns50 <- qplot(y=dfs.list$v5.ns50.ne.nullT, geom='boxplot', x=' ', 
#ylab=bquote(frac('df'['Welch'],'df'['Student'])), 
#xlab=bquote(frac('n'[1],'n'[2])~'='~frac(50,50)~', 
#'~frac('var'[1],'var'[2])~'='~frac(1,5)), ylim=c(.5,1))

#grid.arrange(Bplot.ve.ne.ns50, Bplot.v2.ne.ns50, Bplot.v5.ne.ns50, ncol=3, 
#heights=unit(.5,'npc'), main='Figure 4. Degrees of Freedom Ratios with Changing 
#Variance Ratios')




# equal vars, changing N ratios
#Bplot.ve.ne.ns50 <- qplot(y=dfs.list$ve.ns50.ne.nullT, geom='boxplot', x=' ', 
#ylab=bquote(frac('df'['Welch'],'df'['Student'])), 
#xlab=bquote(frac('n'[1],'n'[2])~'='~frac(50,50)~', 
#'~frac('var'[1],'var'[2])~'='~1), ylim=c(.45,1))
#Bplot.ve.1.5n.ns50 <- qplot(y=dfs.list$ve.ns50.1.5n.nullT, geom='boxplot', x=' 
#', ylab=bquote(frac('df'['Welch'],'df'['Student'])), 
#xlab=bquote(frac('n'[1],'n'[2])~'='~frac(50,75)~', 
#'~frac('var'[1],'var'[2])~'='~1), ylim=c(.45,1))
#Bplot.ve.2n.ns50 <- qplot(y=dfs.list$ve.ns50.2n.nullT, geom='boxplot', x=' ', 
#ylab=bquote(frac('df'['Welch'],'df'['Student'])), 
#xlab=bquote(frac('n'[1],'n'[2])~'='~frac(50,100)~', 
#'~frac('var'[1],'var'[2])~'='~1), ylim=c(.45,1))

#grid.arrange(Bplot.ve.ne.ns50, Bplot.ve.1.5n.ns50 , Bplot.ve.2n.ns50, ncol=3, 
#heights=unit(.5,'npc'), main='Figure 5. Degrees of Freedom Ratios with Changing 
#Sample Size Ratios')


# different vars, changing N ratios
#Bplot.ve.1.5n.ns50 <- qplot(y=dfs.list$ve.ns50.1.5n.nullT, geom='boxplot', x=' 
#', ylab=bquote(frac('df'['Welch'],'df'['Student'])), 
#xlab=bquote(frac('n'[1],'n'[2])~'='~frac(50,75)~', 
#'~frac('var'[1],'var'[2])~'='~1), ylim=c(.4,1))
#Bplot.v2.1.5n.ssv.ns50 <- qplot(y=dfs.list$v2.ns50.1.5n.ssv.nullT, 
#geom='boxplot', x=' ', ylab=bquote(frac('df'['Welch'],'df'['Student'])), 
#xlab=bquote(frac('n'[1],'n'[2])~'='~frac(50,75)~', 
#'~frac('var'[1],'var'[2])~'='~frac(1,2)), ylim=c(.4,1))
#Bplot.v5.1.5n.ssv.ns50 <- qplot(y=dfs.list$v5.ns50.1.5n.ssv.nullT, 
#geom='boxplot', x=' ', ylab=bquote(frac('df'['Welch'],'df'['Student'])), 
#xlab=bquote(frac('n'[1],'n'[2])~'='~frac(50,75)~', 
#'~frac('var'[1],'var'[2])~'='~frac(1,5)), ylim=c(.4,1))

#Bplot.ve.1.5n.ns50 <- qplot(y=dfs.list$ve.ns50.1.5n.nullT, geom='boxplot', x=' 
#', ylab=bquote(frac('df'['Welch'],'df'['Student'])), 
#xlab=bquote(frac('n'[1],'n'[2])~'='~frac(50,75)~', 
#'~frac('var'[1],'var'[2])~'='~1), ylim=c(.4,1))
#Bplot.v2.1.5n.bsv.ns50 <- qplot(y=dfs.list$v2.ns50.1.5n.bsv.nullT, 
#geom='boxplot', x=' ', ylab=bquote(frac('df'['Welch'],'df'['Student'])), 
#xlab=bquote(frac('n'[1],'n'[2])~'='~frac(50,75)~', 
#'~frac('var'[1],'var'[2])~'='~2), ylim=c(.4,1))
#Bplot.v5.1.5n.bsv.ns50 <- qplot(y=dfs.list$v5.ns50.1.5n.bsv.nullT, 
#geom='boxplot', x=' ', ylab=bquote(frac('df'['Welch'],'df'['Student'])), 
#xlab=bquote(frac('n'[1],'n'[2])~'='~frac(50,75)~', 
#'~frac('var'[1],'var'[2])~'='~5), ylim=c(.4,1))

#grid.arrange(Bplot.ve.1.5n.ns50, Bplot.v2.1.5n.ssv.ns50, 
#Bplot.v5.1.5n.ssv.ns50, Bplot.ve.1.5n.ns50,  Bplot.v2.1.5n.bsv.ns50, 
#Bplot.v5.1.5n.bsv.ns50, ncol=3, main='Figure 6. Degrees of Freedom Ratios with 
#Changing Variance Ratios and Sample Size Ratios')




################## Boxplots of first simulations




### all null true

## Equal ns

# equal vars, equal ns
# 
# Bplot.ve.ne.ns20 <- ggplot(venesim1$ve.ns20.ne.nullT, aes(y=dv, 
#x=factor(group, levels=c(1,2)))) +
#     geom_boxplot() +
#     scale_x_discrete(labels=c(bquote('n'[1]~'= 20'), bquote('n'[2]~'= 20'))) +
#     scale_y_continuous(limits=c(-2.5,15.5)) +
#     labs(title='Equal variances', x='', y='') +
#     theme(plot.margin=unit(c(0,0,0,0),'picas'))
# 
# Bplot.ve.ne.ns50 <- ggplot(venesim1$ve.ns50.ne.nullT, aes(y=dv, 
#x=factor(group, levels=c(1,2)))) +
#     geom_boxplot() +
#     scale_x_discrete(labels=c(bquote('n'[1]~'= 50'), bquote('n'[2]~'= 50'))) +
#     scale_y_continuous(limits=c(-2.5,15.5)) +
#     labs(x='', y='') +
#     theme(plot.margin=unit(c(0,0,0,0),'picas'))
# 
# Bplot.ve.ne.ns100 <- ggplot(venesim1$ve.ns100.ne.nullT, aes(y=dv, 
#x=factor(group, levels=c(1,2)))) +
#     geom_boxplot() +
#     scale_x_discrete(labels=c(bquote('n'[1]~'= 100'), bquote('n'[2]~'= 
#100'))) +
#     scale_y_continuous(limits=c(-2.5,15.5)) +
#     labs(x='', y='') +
#     theme(plot.margin=unit(c(0,0,0,0),'picas'))
# 
# # double vars, equal ns
# Bplot.v2.ne.ns20 <- ggplot(v2nesim1$v2.ns20.ne.nullT, aes(y=dv, 
#x=factor(group, levels=c(1,2)))) +
#     geom_boxplot() +
#     scale_x_discrete(labels=c(bquote('n'[1]~'= 20'), bquote('n'[2]~'= 20'))) +
#     scale_y_continuous(limits=c(-2.5,15.5)) +
#     labs(title='Double variances', x='', y='') +
#     theme(plot.margin=unit(c(0,0,0,0),'picas'))
# 
# Bplot.v2.ne.ns50 <- ggplot(v2nesim1$v2.ns50.ne.nullT, aes(y=dv, 
#x=factor(group, levels=c(1,2)))) +
#     geom_boxplot() +
#     scale_x_discrete(labels=c(bquote('n'[1]~'= 50'), bquote('n'[2]~'= 50'))) +
#     scale_y_continuous(limits=c(-2.5,15.5)) +
#     labs(x='', y='') +
#     theme(plot.margin=unit(c(0,0,0,0),'picas'))
# 
# Bplot.v2.ne.ns100 <- ggplot(v2nesim1$v2.ns100.ne.nullT, aes(y=dv, 
#x=factor(group, levels=c(1,2)))) +
#     geom_boxplot() +
#     scale_x_discrete(labels=c(bquote('n'[1]~'= 100'), bquote('n'[2]~'= 
#100'))) +
#     scale_y_continuous(limits=c(-2.5,15.5)) +
#     labs(x='', y='') +
#     theme(plot.margin=unit(c(0,0,0,0),'picas'))
# 
# # 5x vars, equal ns
# Bplot.v5.ne.ns20 <- ggplot(v5nesim1$v5.ns20.ne.nullT, aes(y=dv, 
#x=factor(group, levels=c(1,2)))) +
#     geom_boxplot() +
#     scale_x_discrete(labels=c(bquote('n'[1]~'= 20'), bquote('n'[2]~'= 20'))) +
#     scale_y_continuous(limits=c(-2.5,15.5)) +
#     labs(title='5x variances', x='', y='') +
#     theme(plot.margin=unit(c(0,0,0,0),'picas'))
# 
# Bplot.v5.ne.ns50 <- ggplot(v5nesim1$v5.ns50.ne.nullT, aes(y=dv, 
#x=factor(group, levels=c(1,2)))) +
#     geom_boxplot() +
#     scale_x_discrete(labels=c(bquote('n'[1]~'= 50'), bquote('n'[2]~'= 50'))) +
#     scale_y_continuous(limits=c(-2.5,15.5)) +
#     labs(x='', y='') +
#     theme(plot.margin=unit(c(0,0,0,0),'picas'))
#     
# Bplot.v5.ne.ns100 <- ggplot(v5nesim1$v5.ns100.ne.nullT, aes(y=dv, 
#x=factor(group, levels=c(1,2)))) +
#     geom_boxplot() +
#     scale_x_discrete(labels=c(bquote('n'[1]~'= 100'), bquote('n'[2]~'= 
#100'))) +
#     scale_y_continuous(limits=c(-2.5,15.5)) +
#     labs(x='', y='') +
#     theme(plot.margin=unit(c(0,0,0,0),'picas'))
# 
# # Plot equal ns
# grid.arrange(Bplot.ve.ne.ns20, Bplot.v2.ne.ns20, Bplot.v5.ne.ns20, 
#Bplot.ve.ne.ns50, Bplot.v2.ne.ns50, Bplot.v5.ne.ns50, Bplot.ve.ne.ns100, 
#Bplot.v2.ne.ns100, Bplot.v5.ne.ns100, nrow=3, ncol=3, main='Figure 1. Boxplots 
#of First Simulations When Sample Sizes are Equal')
# 
# 
# 
# ## We'll only use it when the small N = 50 as an example
# 
# # Variances equal
# # NOTE: it's not really ssv when variances are equal
# Bplot.ve.n2.ssv <- ggplot(ven2sim1$ve.ns50.2n.nullT, aes(y=dv, 
#x=factor(group, levels=c(1,2)))) +
#     geom_boxplot() +
#     scale_x_discrete(labels=c(bquote('n'[1]~'= 50'), bquote('n'[2]~'= 100'))) 
#+
#     scale_y_continuous(limits=c(-3.5,16)) +
#     labs(title='Equal variances', x='', y='') +
#     theme(plot.margin=unit(c(0,0,0,0),'picas'))
# 
# Bplot.ve.n1.5.ssv <- ggplot(ven15sim1$ve.ns50.1.5n.nullT, aes(y=dv, 
#x=factor(group, levels=c(1,2)))) +
#     geom_boxplot() +
#     scale_x_discrete(labels=c(bquote('n'[1]~'= 50'), bquote('n'[2]~'= 75'))) +
#     scale_y_continuous(limits=c(-3.5,16)) +
#     labs(x='', y='') +
#     theme(plot.margin=unit(c(0,0,0,0),'picas'))
# 
# Bplot.ve.n1.5.bsv <- ggplot(ven15sim1$ve.ns50.1.5n.nullT, aes(y=dv, 
#x=factor(group, levels=c(2,1)))) +
#     geom_boxplot() +
#     scale_x_discrete(labels=c(bquote('n'[1]~'= 75'), bquote('n'[2]~'= 50'))) +
#     scale_y_continuous(limits=c(-3.5,16)) +
#     labs(x='', y='') +
#     theme(plot.margin=unit(c(0,0,0,0),'picas'))
# 
# Bplot.ve.n2.bsv <- ggplot(ven2sim1$ve.ns50.2n.nullT, aes(y=dv, 
#x=factor(group, levels=c(2,1)))) +
#     geom_boxplot() +
#     scale_x_discrete(labels=c(bquote('n'[1]~'= 100'), bquote('n'[2]~'= 50'))) 
#+
#     scale_y_continuous(limits=c(-3.5,16)) +
#     labs(x='', y='') +
#     theme(plot.margin=unit(c(0,0,0,0),'picas'))
# 
# 
# # Variances double
# # NOTE: it's not really ssv when variances are equal
# Bplot.v2.n2.ssv <- ggplot(v2n2ssvsim1$v2.ns50.2n.ssv.nullT, aes(y=dv, 
#x=factor(group, levels=c(1,2)))) +
#     geom_boxplot() +
#     scale_x_discrete(labels=c(bquote('n'[1]~'= 50'), bquote('n'[2]~'= 100'))) 
#+
#     scale_y_continuous(limits=c(-3.5,16)) +
#     labs(title='Double variances', x='', y='') +
#     theme(plot.margin=unit(c(0,0,0,0),'picas'))
# 
# Bplot.v2.n1.5.ssv <- ggplot(v2n15ssvsim1$v2.ns50.1.5n.ssv.nullT, aes(y=dv, 
#x=factor(group, levels=c(1,2)))) +
#     geom_boxplot() +
#     scale_x_discrete(labels=c(bquote('n'[1]~'= 50'), bquote('n'[2]~'= 75'))) +
#     scale_y_continuous(limits=c(-3.5,16)) +
#     labs(x='', y='') +
#     theme(plot.margin=unit(c(0,0,0,0),'picas'))
# 
# Bplot.v2.n1.5.bsv <- ggplot(v2n15bsvsim1$v2.ns50.1.5n.bsv.nullT, aes(y=dv, 
#x=factor(group, levels=c(2,1)))) +
#     geom_boxplot() +
#     scale_x_discrete(labels=c(bquote('n'[1]~'= 75'), bquote('n'[2]~'= 50'))) +
#     scale_y_continuous(limits=c(-3.5,16)) +
#     labs(x='', y='') +
#     theme(plot.margin=unit(c(0,0,0,0),'picas'))
# 
# Bplot.v2.n2.bsv <- ggplot(v2n2bsvsim1$v2.ns50.2n.bsv.nullT, aes(y=dv, 
#x=factor(group, levels=c(2,1)))) +
#     geom_boxplot() +
#     scale_x_discrete(labels=c(bquote('n'[1]~'= 100'), bquote('n'[2]~'= 50'))) 
#+
#     scale_y_continuous(limits=c(-3.5,16)) +
#     labs(x='', y='') +
#     theme(plot.margin=unit(c(0,0,0,0),'picas'))
# 
# # Variances 5x
# # NOTE: it's not really ssv when variances are equal
# Bplot.v5.n2.ssv <- ggplot(v5n2ssvsim1$v5.ns50.2n.ssv.nullT, aes(y=dv, 
#x=factor(group, levels=c(1,2)))) +
#     geom_boxplot() +
#     scale_x_discrete(labels=c(bquote('n'[1]~'= 50'), bquote('n'[2]~'= 100'))) 
#+
#     scale_y_continuous(limits=c(-3.5,16)) +
#     labs(title='5x variances', x='', y='') +
#     theme(plot.margin=unit(c(0,0,0,0),'picas'))
# 
# Bplot.v5.n1.5.ssv <- ggplot(v5n15ssvsim1$v5.ns50.1.5n.ssv.nullT, aes(y=dv, 
#x=factor(group, levels=c(1,2)))) +
#     geom_boxplot() +
#     scale_x_discrete(labels=c(bquote('n'[1]~'= 50'), bquote('n'[2]~'= 75'))) +
#     scale_y_continuous(limits=c(-3.5,16)) +
#     labs(x='', y='') +
#     theme(plot.margin=unit(c(0,0,0,0),'picas'))
# 
# Bplot.v5.n1.5.bsv <- ggplot(v5n15bsvsim1$v5.ns50.1.5n.bsv.nullT, aes(y=dv, 
#x=factor(group, levels=c(2,1)))) +
#     geom_boxplot() +
#     scale_x_discrete(labels=c(bquote('n'[1]~'= 75'), bquote('n'[2]~'= 50'))) +
#     scale_y_continuous(limits=c(-3.5,16)) +
#     labs(x='', y='') +
#     theme(plot.margin=unit(c(0,0,0,0),'picas'))
# 
# Bplot.v5.n2.bsv <- ggplot(v5n2bsvsim1$v5.ns50.2n.bsv.nullT, aes(y=dv, 
#x=factor(group, levels=c(2,1)))) +
#     geom_boxplot() +
#     scale_x_discrete(labels=c(bquote('n'[1]~'= 100'), bquote('n'[2]~'= 50'))) 
#+
#     scale_y_continuous(limits=c(-3.5,16)) +
#     labs(x='', y='') +
#     theme(plot.margin=unit(c(0,0,0,0),'picas'))
# 
# # Plot n = 20, different vars, different ns
# grid.arrange(Bplot.ve.n2.ssv, Bplot.v2.n2.ssv, Bplot.v5.n2.ssv, 
#Bplot.ve.n1.5.ssv, Bplot.v2.n1.5.ssv, Bplot.v5.n1.5.ssv, Bplot.ve.n1.5.bsv, 
#Bplot.v2.n1.5.bsv, Bplot.v5.n1.5.bsv, Bplot.ve.n2.bsv, Bplot.v2.n2.bsv, 
#Bplot.v5.n2.bsv, nrow=4, main='Figure 2. Boxplots of First Simulations When 
#Sample Sizes are Unequal')

@

%' \begin{figure}
%' <<varDifferentBoxplots, echo=FALSE, fig=TRUE>>=
%' 
%' x1 <- seq(0,12,.1)
%' y1 <- dnorm(x1, mean=6, sd=sqrt(2))
%' x2 <- seq(1.13,13.13,.1)
%' y2 <- dnorm(x2, mean=7.13, sd=sqrt(10))
%' 
%' x <- c(x1,x2)
%' y <- c(y1,y2)
%' group <- factor(c(rep(1,length(x1)), rep(2,length(x2))))
%' 
%' ve.distributions <- qplot(x=x, y=y, fill=group, colour=group, geom='line', 
%xlab=NULL, ylab=NULL) + geom_vline(xintercept=as.numeric(by(x,group,mean))) + 
%theme(legend.position='none')
%' 
%' set.seed(2184)
%' ve.bplots <- c(list(ve.distributions), 
%'                lapply(1:4, function(x){
%'                  qplot(x=factor(rep(c(1,2), each=20)), 
%'                        y=c(rnorm(20,6,sqrt(2)),rnorm(20,7.13,sqrt(10))), 
%'                        geom='boxplot', 
%'                        colour=factor(rep(c(1,2), each=20)), 
%'                        xlab=NULL, 
%'                        ylab=NULL, 
%'                        ylim=c(-3,18)) + 
%'                    theme(legend.position='none', 
%axis.text.x=element_blank())}), 
%'                lapply(1:4, function(x){
%'                  qplot(x=factor(rep(c(1,2), each=100)), 
%'                        y=c(rnorm(100,6,sqrt(2)),rnorm(100,7.13,sqrt(10))), 
%'                        geom='boxplot', 
%'                        colour=factor(rep(c(1,2), each=100)), 
%'                        xlab=NULL, 
%'                        ylab=NULL, 
%'                        ylim=c(-3,18)) + 
%'                    theme(legend.position='none', 
%axis.text.x=element_blank())}))
%' 
%' layout <- matrix(c(0,1,1,0,2,3,4,5,6,7,8,9), nrow=3, byrow=TRUE)
%' multiplot(plotlist=ve.bplots, layout=layout)
%' 
%' @
%' 
%' \textit{Figure 2.} Boxplots for groups with unequal variances. The first row 
%displays groups with $n$=20 and the second row displays groups with $n$=100.
%' \end{figure}


\bibliography{bibliography}
\bibliographystyle{apacite}

\end{document}