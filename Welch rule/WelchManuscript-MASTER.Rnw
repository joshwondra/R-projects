\documentclass[man,a4paper,noextraspace]{apa6}

\title{Always Use the Separate Variances t Test for Two Independent Groups}
\shorttitle{Separate Variances t Test}
\author{Joshua D. Wondra and Richard Gonzalez}
\affiliation{University of Michigan}

\abstract{This is an abstract}
\keywords{t test, new statistics}


\begin{document}
\maketitle

    Data analysis involves a series of decisions on the part of the researcher about which statistical test answers the research question, whether the data fit the requirements of the test, and whether there are alternative options that will do a better job. Recent discussions of false positives in psychology research (CITATIONS) highlight the tension between two valued outcomes of the decision process. On the one hand, researchers want to avoid mistakenly claiming that there is a true effect where none exists, which involves concerns about false positives. On the other hand, researchers want to find true effects where they do exist, which involves concerns about power. In addition to these two, there is a growing concern with accurate effect size estimation (CITATIONS). Some have argued that because so many studies lack the power to find small effects, those that make it into published papers are spuriously large (CITATIONS).

    One of the first decisions that many researchers learn is how to compare the means of two independent groups--they run a t test. But even this basic comparison presents a choice between the classic Student's t test (CITATION) or the alternative Welch-Satterthwaite test (CITATION). Most researchers learn about Student's t test in the first statistics class that they ever take.  
    
When you use Student's t test to compare the means of independent groups, you make three assumptions: 
    
    1. Normality: The population for each group has a normal distribution.

    2. Independence: All observations are independent of each other, meaning that the probability of one observation having a particular value does not depend on the probability of another observation having a particular value.

    3. Equal variances: The population variances for the two groups are equal. 

If these assumptions hold, then you can compuate the t-value by taking the difference in group means and dividing by the standard error of that difference:   
    \begin{equation}
    t = \frac{\overline{x_1}-\overline{x_2}}{\sigma_{\overline{x_1}-\overline{x_2}}}
    \end{equation}
    The p-value for the value of the t statistic depends on the degrees of freedom, $df=n_1+n_2-2$. You are more likely to reject the null hypothesis and conclude that there is a difference in the group means as the degrees of freedom get larger, meaning that the sample sizes are larger, and as the value of the t test gets larger, meaning that the difference in group means gets larger or the standard error gets smaller. With the equal variances assumption, the standard error is based on a common variance that pools the estimates of the variances of the two groups. 

    If either the data or the study design suggests that one or more of the assumptions has been violated, then Student's t test is not the right choice. Specifically, if the equal variances assumption has been violated, then the Welch-Satterthwaite test (hereafter called the Welch t test for the sake of brevity) is a good alternative choice. Many researchers might not learn about the Welch-Satterthwaite test in their formal statistical training, though most have encountered it in their own analyses. Those who use SPSS (CITATION) to analyze their data encounter the Welch test in the "Equal variances not assumed" line that appears by default whenever they run an independent samples t test. Those who use \texttt{R} (CITATION) get the Welch test by default when they use the \texttt{t.test()} function and can only get the Student's t test results by setting the \texttt{var.equal} argument to \texttt{TRUE}.
    
        As with Student's t test, the Welch t test assumes normality and independence; however, it does not assume that the population variances are equal. The standard error is based on separate group variances instead of a common variance. Additionally, the Welch t test decreases the degrees of freedom to the extent that the group variances differ. Because of these differences, the two tests can disagree about whether there is a difference in group means. The penalty to the degrees of freedom pushes the Welch test in the direction of being more conservative and less likely to reject the null. On the one hand, this might make the Welch test a better choice if Student's t test finds more false positives when variances are unequal. On the other hand, this might make the Welch test a worse choice if it is not powerful enough to detect true effects. 
        
    However, the Welch test is not necessarily always more conservative. The power of the two tests is not only based on the degrees of freedom, but also on the standard error. This means that the Welch test could be more powerful than Student's test if the separate variances standard error is smaller than the pooled variances standard error. Notably, the two standard errors are equal when either the sample sizes or the variances of the two groups are identical, so the Welch test could only be more powerful when both the sample sizes and variances are unequal.  
    
    How do you decide which test to use? The typical approach is to use Student's t test unless there is evidence that the populations have unequal variances. The challenge is how to decide that there is enough evidence that the variances are unequal. 

    One option is to run another test of the null hypothesis that the variances are equal, such as Levene's test for homogeneity which shows up by default in SPSS, and use the Welch test if you reject the null. This is usually seen as an ineffective strategy because tests of assumptions make their own assumptions that go unchecked and they are sensitive to sample size (CITATIONS). 

    A second option is to visualize the data using boxplots and make a judgment about whether the variances appear to differ. With smaller sample sizes, you can tolerate larger apparent differences. This strategy can be enhanced by simulating data for two groups of sample sizes equal to your own data, changing whether the variances are equal or unequal, and seeing how variable the boxplots look under each condition. 

    A third option that has not been tested to our knowledge is to examine the ratio of the degrees of freedom between Student's test and Welch's test. If they differ to a large extent, then it might be a sign that the group variances differ.

    A fourth option is to change the typical approach. When variances and sample sizes are equal, Welch's t test is equivalent to Student's t test. If using the Welch test generally leads to better decisions than Student's t test under both ideal and non-ideal conditions, then instead of using Student's t test by default it might be better to always use the Welch test.
    
    We examined these possibilities in a Monte Carlo simulation study and examined the false positives, power, and coverage probability of Student's and Welch's t tests under different conditions. 

\section{Method}
    We ran Monte Carlo simulations of two independent groups with normally distributed data. We examined the type I error rate, power, and coverage probability for both Student's t test and Welch's t test under different conditions. We varied the variance ratio ($\sigma_{1}/\sigma_{2}$ = 1/5, 1/2, 1, 2, or 5), the sample size ratio ($n_{1}/n_{2}$ = 1, 2/3, or 1/2), and sample sizes (small $n$ = 20, 50, or 100). 
    
    Additionally, we varied the size of the difference in group means using Cohen's d values of 0, .2, .5, and .8. Importantly, Cohen's d requires the group variances to be equal because it uses a pooled variance. For the conditions with unequal variances, we computed the mean differences for the same Cohen's d values as though the two different population variances were incorrect estimates of a common population variance. This is comparable to the implicit assumption that researchers make when they use the equal variances t test on data that violate the equal variances assumption.
    
    The full set of conditions is displayed in Table 1 (NOTE TO RICH: Table 1 is in a Word document but not here because we need to discuss how to approach effect sizes with unequal variances). For each condition, we set the seed to 2184 and ran 10,000 simulations. We did not simulate the conditions with equal sample sizes and variance ratios of 2 and 5 because they were identical to the conditions with equal sample sizes and variance ratios of 1/2 and 1/5. 

\section{Results}    
<<setup, echo=FALSE>>=

# set images to pdf instead of png
opts_chunk$set(dev = 'pdf')

## Retrieve tables with general data
load('/users/joshwondra/R-projects/Welch rule/veNeSeed2184Tables.Rdata')
load('/users/joshwondra/R-projects/Welch rule/v2NeSeed2184Tables.Rdata')
load('/users/joshwondra/R-projects/Welch rule/v5NeSeed2184Tables.Rdata')
load('/users/joshwondra/R-projects/Welch rule/ve1andhalfnSeed2184Tables.Rdata')
load('/users/joshwondra/R-projects/Welch rule/ve2nSeed2184Tables.Rdata')
load('/users/joshwondra/R-projects/Welch rule/v21andhalfnSSVSeed2184Tables.Rdata')
load('/users/joshwondra/R-projects/Welch rule/v51andhalfnSSVSeed2184Tables.Rdata')
load('/users/joshwondra/R-projects/Welch rule/v22nSSVSeed2184Tables.Rdata')
load('/users/joshwondra/R-projects/Welch rule/v52nSSVSeed2184Tables.Rdata')
load('/users/joshwondra/R-projects/Welch rule/v21andhalfnBSVSeed2184Tables.Rdata')
load('/users/joshwondra/R-projects/Welch rule/v51andhalfnBSVSeed2184Tables.Rdata')
load('/users/joshwondra/R-projects/Welch rule/v22nBSVSeed2184Tables.Rdata')
load('/users/joshwondra/R-projects/Welch rule/v52nBSVSeed2184Tables.Rdata')

## Retrieve data for boxplots
load('/users/joshwondra/R-projects/Welch rule/veNeSeed2184-sim1.Rdata')
load('/users/joshwondra/R-projects/Welch rule/v2NeSeed2184-sim1.Rdata')
load('/users/joshwondra/R-projects/Welch rule/v5NeSeed2184-sim1.Rdata')
load('/users/joshwondra/R-projects/Welch rule/veN15Seed2184-sim1.Rdata')
load('/users/joshwondra/R-projects/Welch rule/v2N15ssvSeed2184-sim1.Rdata')
load('/users/joshwondra/R-projects/Welch rule/v2N15bsvSeed2184-sim1.Rdata')
load('/users/joshwondra/R-projects/Welch rule/v5N15ssvSeed2184-sim1.Rdata')
load('/users/joshwondra/R-projects/Welch rule/v5N15bsvSeed2184-sim1.Rdata')
load('/users/joshwondra/R-projects/Welch rule/veN2Seed2184-sim1.Rdata')
load('/users/joshwondra/R-projects/Welch rule/v2N2ssvSeed2184-sim1.Rdata')
load('/users/joshwondra/R-projects/Welch rule/v2N2bsvSeed2184-sim1.Rdata')
load('/users/joshwondra/R-projects/Welch rule/v5N2ssvSeed2184-sim1.Rdata')
load('/users/joshwondra/R-projects/Welch rule/v5N2bsvSeed2184-sim1.Rdata')

## Load packages
library(ggplot2)
library(grid)
library(gridExtra)

# Multiple plot function
# retrieved from http://www.cookbook-r.com/Graphs/Multiple_graphs_on_one_page_(ggplot2)/

# ggplot objects can be passed in ..., or to plotlist (as a list of ggplot objects)
# - cols:   Number of columns in layout
# - layout: A matrix specifying the layout. If present, 'cols' is ignored.

# If the layout is something like matrix(c(1,2,3,3), nrow=2, byrow=TRUE),
# then plot 1 will go in the upper left, 2 will go in the upper right, and
# 3 will go all the way across the bottom.

multiplot <- function(..., plotlist=NULL, file, cols=1, layout=NULL) {

  # Make a list from the ... arguments and plotlist
  plots <- c(list(...), plotlist)

  numPlots = length(plots)

  # If layout is NULL, then use 'cols' to determine layout
  if (is.null(layout)) {
    # Make the panel
    # ncol: Number of columns of plots
    # nrow: Number of rows needed, calculated from # of cols
    layout <- matrix(seq(1, cols * ceiling(numPlots/cols)),
                    ncol = cols, nrow = ceiling(numPlots/cols))
  }

 if (numPlots==1) {
    print(plots[[1]])

  } else {
    # Set up the page
    grid.newpage()
    pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout))))

    # Make each plot, in the correct location
    for (i in 1:numPlots) {
      # Get the i,j matrix positions of the regions that contain this subplot
      matchidx <- as.data.frame(which(layout == i, arr.ind = TRUE))

      print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row,
                                      layout.pos.col = matchidx$col))
    }
  }
}

@
\subsection{Boxplots} One option for deciding whether the group variances are equal is to examine boxplots. Figure 1 displays boxplots for the first simulation each condition when sample sizes are equal. When variances are unequal, the larger variance is for the group to the right. When the sample sizes are smaller, it can be more difficult to tell whether the variances are unequal or not. When the sample sizes are 20, the variance of one group looks a bit larger than the other when they are really equal and the variances look approximately equal when one group has twice the variance of the other. As the sample size and increases, it becomes clearer to see whether there is a violation - at 100 subjects per condition, when variances are equal the boxes and whiskers are approximately the same length, and when variances are unequal both the box and whiskers appear wider in the group with the bigger variance. By examining several additional simulations it would be possible to see how much variability in the boxplots is normal when variances are equal or unequal.
   
<<equalNboxplots, echo=FALSE, collapse=TRUE>>=

### all null true

## Equal ns

# equal vars, equal ns

Bplot.ve.ne.ns20 <- ggplot(venesim1$ve.ns20.ne.nullT, aes(y=dv, x=factor(group, levels=c(1,2)))) +
    geom_boxplot() +
    scale_x_discrete(labels=c(bquote('n'[1]~'= 20'), bquote('n'[2]~'= 20'))) +
    scale_y_continuous(limits=c(-2.5,15.5)) +
    labs(title='Equal variances', x='', y='') +
    theme(plot.margin=unit(c(0,0,0,0),'picas'))

Bplot.ve.ne.ns50 <- ggplot(venesim1$ve.ns50.ne.nullT, aes(y=dv, x=factor(group, levels=c(1,2)))) +
    geom_boxplot() +
    scale_x_discrete(labels=c(bquote('n'[1]~'= 50'), bquote('n'[2]~'= 50'))) +
    scale_y_continuous(limits=c(-2.5,15.5)) +
    labs(x='', y='') +
    theme(plot.margin=unit(c(0,0,0,0),'picas'))

Bplot.ve.ne.ns100 <- ggplot(venesim1$ve.ns100.ne.nullT, aes(y=dv, x=factor(group, levels=c(1,2)))) +
    geom_boxplot() +
    scale_x_discrete(labels=c(bquote('n'[1]~'= 100'), bquote('n'[2]~'= 100'))) +
    scale_y_continuous(limits=c(-2.5,15.5)) +
    labs(x='', y='') +
    theme(plot.margin=unit(c(0,0,0,0),'picas'))

# double vars, equal ns
Bplot.v2.ne.ns20 <- ggplot(v2nesim1$v2.ns20.ne.nullT, aes(y=dv, x=factor(group, levels=c(1,2)))) +
    geom_boxplot() +
    scale_x_discrete(labels=c(bquote('n'[1]~'= 20'), bquote('n'[2]~'= 20'))) +
    scale_y_continuous(limits=c(-2.5,15.5)) +
    labs(title='Double variances', x='', y='') +
    theme(plot.margin=unit(c(0,0,0,0),'picas'))

Bplot.v2.ne.ns50 <- ggplot(v2nesim1$v2.ns50.ne.nullT, aes(y=dv, x=factor(group, levels=c(1,2)))) +
    geom_boxplot() +
    scale_x_discrete(labels=c(bquote('n'[1]~'= 50'), bquote('n'[2]~'= 50'))) +
    scale_y_continuous(limits=c(-2.5,15.5)) +
    labs(x='', y='') +
    theme(plot.margin=unit(c(0,0,0,0),'picas'))

Bplot.v2.ne.ns100 <- ggplot(v2nesim1$v2.ns100.ne.nullT, aes(y=dv, x=factor(group, levels=c(1,2)))) +
    geom_boxplot() +
    scale_x_discrete(labels=c(bquote('n'[1]~'= 100'), bquote('n'[2]~'= 100'))) +
    scale_y_continuous(limits=c(-2.5,15.5)) +
    labs(x='', y='') +
    theme(plot.margin=unit(c(0,0,0,0),'picas'))

# 5x vars, equal ns
Bplot.v5.ne.ns20 <- ggplot(v5nesim1$v5.ns20.ne.nullT, aes(y=dv, x=factor(group, levels=c(1,2)))) +
    geom_boxplot() +
    scale_x_discrete(labels=c(bquote('n'[1]~'= 20'), bquote('n'[2]~'= 20'))) +
    scale_y_continuous(limits=c(-2.5,15.5)) +
    labs(title='5x variances', x='', y='') +
    theme(plot.margin=unit(c(0,0,0,0),'picas'))

Bplot.v5.ne.ns50 <- ggplot(v5nesim1$v5.ns50.ne.nullT, aes(y=dv, x=factor(group, levels=c(1,2)))) +
    geom_boxplot() +
    scale_x_discrete(labels=c(bquote('n'[1]~'= 50'), bquote('n'[2]~'= 50'))) +
    scale_y_continuous(limits=c(-2.5,15.5)) +
    labs(x='', y='') +
    theme(plot.margin=unit(c(0,0,0,0),'picas'))
    
Bplot.v5.ne.ns100 <- ggplot(v5nesim1$v5.ns100.ne.nullT, aes(y=dv, x=factor(group, levels=c(1,2)))) +
    geom_boxplot() +
    scale_x_discrete(labels=c(bquote('n'[1]~'= 100'), bquote('n'[2]~'= 100'))) +
    scale_y_continuous(limits=c(-2.5,15.5)) +
    labs(x='', y='') +
    theme(plot.margin=unit(c(0,0,0,0),'picas'))

# Plot equal ns
grid.arrange(Bplot.ve.ne.ns20, Bplot.v2.ne.ns20, Bplot.v5.ne.ns20, Bplot.ve.ne.ns50, Bplot.v2.ne.ns50, Bplot.v5.ne.ns50, Bplot.ve.ne.ns100, Bplot.v2.ne.ns100, Bplot.v5.ne.ns100, nrow=3, ncol=3, main='Figure 1. Boxplots of First Simulations When Sample Sizes are Equal')

@

Figure 2 displays a sample of boxplots for simulations when the sample size ratio and variance ratio are both changing (NOTE: I'm not sure that this adds much so we might just drop it).

<<differentNboxplots, echo=FALSE>>=

## We'll only use it when the small N = 50 as an example

# Variances equal
# NOTE: it's not really ssv when variances are equal
Bplot.ve.n2.ssv <- ggplot(ven2sim1$ve.ns50.2n.nullT, aes(y=dv, x=factor(group, levels=c(1,2)))) +
    geom_boxplot() +
    scale_x_discrete(labels=c(bquote('n'[1]~'= 50'), bquote('n'[2]~'= 100'))) +
    scale_y_continuous(limits=c(-3.5,16)) +
    labs(title='Equal variances', x='', y='') +
    theme(plot.margin=unit(c(0,0,0,0),'picas'))

Bplot.ve.n1.5.ssv <- ggplot(ven15sim1$ve.ns50.1.5n.nullT, aes(y=dv, x=factor(group, levels=c(1,2)))) +
    geom_boxplot() +
    scale_x_discrete(labels=c(bquote('n'[1]~'= 50'), bquote('n'[2]~'= 75'))) +
    scale_y_continuous(limits=c(-3.5,16)) +
    labs(x='', y='') +
    theme(plot.margin=unit(c(0,0,0,0),'picas'))

Bplot.ve.n1.5.bsv <- ggplot(ven15sim1$ve.ns50.1.5n.nullT, aes(y=dv, x=factor(group, levels=c(2,1)))) +
    geom_boxplot() +
    scale_x_discrete(labels=c(bquote('n'[1]~'= 75'), bquote('n'[2]~'= 50'))) +
    scale_y_continuous(limits=c(-3.5,16)) +
    labs(x='', y='') +
    theme(plot.margin=unit(c(0,0,0,0),'picas'))

Bplot.ve.n2.bsv <- ggplot(ven2sim1$ve.ns50.2n.nullT, aes(y=dv, x=factor(group, levels=c(2,1)))) +
    geom_boxplot() +
    scale_x_discrete(labels=c(bquote('n'[1]~'= 100'), bquote('n'[2]~'= 50'))) +
    scale_y_continuous(limits=c(-3.5,16)) +
    labs(x='', y='') +
    theme(plot.margin=unit(c(0,0,0,0),'picas'))


# Variances double
# NOTE: it's not really ssv when variances are equal
Bplot.v2.n2.ssv <- ggplot(v2n2ssvsim1$v2.ns50.2n.ssv.nullT, aes(y=dv, x=factor(group, levels=c(1,2)))) +
    geom_boxplot() +
    scale_x_discrete(labels=c(bquote('n'[1]~'= 50'), bquote('n'[2]~'= 100'))) +
    scale_y_continuous(limits=c(-3.5,16)) +
    labs(title='Double variances', x='', y='') +
    theme(plot.margin=unit(c(0,0,0,0),'picas'))

Bplot.v2.n1.5.ssv <- ggplot(v2n15ssvsim1$v2.ns50.1.5n.ssv.nullT, aes(y=dv, x=factor(group, levels=c(1,2)))) +
    geom_boxplot() +
    scale_x_discrete(labels=c(bquote('n'[1]~'= 50'), bquote('n'[2]~'= 75'))) +
    scale_y_continuous(limits=c(-3.5,16)) +
    labs(x='', y='') +
    theme(plot.margin=unit(c(0,0,0,0),'picas'))

Bplot.v2.n1.5.bsv <- ggplot(v2n15bsvsim1$v2.ns50.1.5n.bsv.nullT, aes(y=dv, x=factor(group, levels=c(2,1)))) +
    geom_boxplot() +
    scale_x_discrete(labels=c(bquote('n'[1]~'= 75'), bquote('n'[2]~'= 50'))) +
    scale_y_continuous(limits=c(-3.5,16)) +
    labs(x='', y='') +
    theme(plot.margin=unit(c(0,0,0,0),'picas'))

Bplot.v2.n2.bsv <- ggplot(v2n2bsvsim1$v2.ns50.2n.bsv.nullT, aes(y=dv, x=factor(group, levels=c(2,1)))) +
    geom_boxplot() +
    scale_x_discrete(labels=c(bquote('n'[1]~'= 100'), bquote('n'[2]~'= 50'))) +
    scale_y_continuous(limits=c(-3.5,16)) +
    labs(x='', y='') +
    theme(plot.margin=unit(c(0,0,0,0),'picas'))

# Variances 5x
# NOTE: it's not really ssv when variances are equal
Bplot.v5.n2.ssv <- ggplot(v5n2ssvsim1$v5.ns50.2n.ssv.nullT, aes(y=dv, x=factor(group, levels=c(1,2)))) +
    geom_boxplot() +
    scale_x_discrete(labels=c(bquote('n'[1]~'= 50'), bquote('n'[2]~'= 100'))) +
    scale_y_continuous(limits=c(-3.5,16)) +
    labs(title='5x variances', x='', y='') +
    theme(plot.margin=unit(c(0,0,0,0),'picas'))

Bplot.v5.n1.5.ssv <- ggplot(v5n15ssvsim1$v5.ns50.1.5n.ssv.nullT, aes(y=dv, x=factor(group, levels=c(1,2)))) +
    geom_boxplot() +
    scale_x_discrete(labels=c(bquote('n'[1]~'= 50'), bquote('n'[2]~'= 75'))) +
    scale_y_continuous(limits=c(-3.5,16)) +
    labs(x='', y='') +
    theme(plot.margin=unit(c(0,0,0,0),'picas'))

Bplot.v5.n1.5.bsv <- ggplot(v5n15bsvsim1$v5.ns50.1.5n.bsv.nullT, aes(y=dv, x=factor(group, levels=c(2,1)))) +
    geom_boxplot() +
    scale_x_discrete(labels=c(bquote('n'[1]~'= 75'), bquote('n'[2]~'= 50'))) +
    scale_y_continuous(limits=c(-3.5,16)) +
    labs(x='', y='') +
    theme(plot.margin=unit(c(0,0,0,0),'picas'))

Bplot.v5.n2.bsv <- ggplot(v5n2bsvsim1$v5.ns50.2n.bsv.nullT, aes(y=dv, x=factor(group, levels=c(2,1)))) +
    geom_boxplot() +
    scale_x_discrete(labels=c(bquote('n'[1]~'= 100'), bquote('n'[2]~'= 50'))) +
    scale_y_continuous(limits=c(-3.5,16)) +
    labs(x='', y='') +
    theme(plot.margin=unit(c(0,0,0,0),'picas'))

# Plot n = 20, different vars, different ns
grid.arrange(Bplot.ve.n2.ssv, Bplot.v2.n2.ssv, Bplot.v5.n2.ssv, Bplot.ve.n1.5.ssv, Bplot.v2.n1.5.ssv, Bplot.v5.n1.5.ssv, Bplot.ve.n1.5.bsv, Bplot.v2.n1.5.bsv, Bplot.v5.n1.5.bsv, Bplot.ve.n2.bsv, Bplot.v2.n2.bsv, Bplot.v5.n2.bsv, nrow=4, main='Figure 2. Boxplots of First Simulations When Sample Sizes are Unequal')

@

\subsection{Does the df Ratio Help?}
    We examined whether looking at ratio of the Welch degrees of freedom to the classic t test degrees of freedom would provide a heuristic for deciding that equal variances does not hold. Figure 3 displays boxplots of the ratio of Welch degrees of freedom to the classic t test degrees of freedom as sample sizes increase when  the variance and sample size ratios equal. As the sample sizes increase, the degrees of freedom ratio stays close to 1. 

<< dfratiosNincrease, echo=FALSE >>=

load('/users/joshwondra/R-projects/Welch rule/dfs.list.RData')

## Equal ns

# equal vars, equal ns
Bplot.ve.ne.ns20 <- qplot(y=dfs.list$ve.ns20.ne.nullT, geom='boxplot', x=' ', ylab=bquote(frac('df'['Welch'],'df'['Student'])), xlab=bquote(frac('n'[1],'n'[2])~'='~frac(20,20)~', '~frac('var'[1],'var'[2])~'='~1), ylim=c(.5,1))
Bplot.ve.ne.ns50 <- qplot(y=dfs.list$ve.ns50.ne.nullT, geom='boxplot', x=' ', ylab=bquote(frac('df'['Welch'],'df'['Student'])), xlab=bquote(frac('n'[1],'n'[2])~'='~frac(50,50)~', '~frac('var'[1],'var'[2])~'='~1), ylim=c(.5,1))
Bplot.ve.ne.ns100 <- qplot(y=dfs.list$ve.ns100.ne.nullT, geom='boxplot', x=' ', ylab=bquote(frac('df'['Welch'],'df'['Student'])), xlab=bquote(frac('n'[1],'n'[2])~'='~frac(100,100)~', '~frac('var'[1],'var'[2])~'='~1), ylim=c(.5,1))

grid.arrange(Bplot.ve.ne.ns20, Bplot.ve.ne.ns50, Bplot.ve.ne.ns100, ncol=3, heights=unit(.5, 'npc'), main='Figure 3. Degrees of Freedom Ratios with Increasing Sample Sizes')

@

    Figure 4 displays the ratio as the group variances become increasingly different when both the sample size and the sample size ratio stay the same. Under this condition, the distribution moves away from 1 as the difference in group variances increases.

<< dfratiosDiffvars, echo=FALSE>>=

# different vars, equal ns
Bplot.ve.ne.ns50 <- qplot(y=dfs.list$ve.ns50.ne.nullT, geom='boxplot', x=' ', ylab=bquote(frac('df'['Welch'],'df'['Student'])), xlab=bquote(frac('n'[1],'n'[2])~'='~frac(50,50)~', '~frac('var'[1],'var'[2])~'='~1), ylim=c(.5,1))
Bplot.v2.ne.ns50 <- qplot(y=dfs.list$v2.ns50.ne.nullT, geom='boxplot', x=' ', ylab=bquote(frac('df'['Welch'],'df'['Student'])), xlab=bquote(frac('n'[1],'n'[2])~'='~frac(50,50)~', '~frac('var'[1],'var'[2])~'='~frac(1,2)), ylim=c(.5,1))
Bplot.v5.ne.ns50 <- qplot(y=dfs.list$v5.ns50.ne.nullT, geom='boxplot', x=' ', ylab=bquote(frac('df'['Welch'],'df'['Student'])), xlab=bquote(frac('n'[1],'n'[2])~'='~frac(50,50)~', '~frac('var'[1],'var'[2])~'='~frac(1,5)), ylim=c(.5,1))

grid.arrange(Bplot.ve.ne.ns50, Bplot.v2.ne.ns50, Bplot.v5.ne.ns50, ncol=3, heights=unit(.5,'npc'), main='Figure 4. Degrees of Freedom Ratios with Changing Variance Ratios')

@

    In the first two plots, the degrees of freedom ratios are generally above 95\% when variances are equal and below 95\% when they differ. A useful heuristic might be to assume unequal variances when the ratio falls below 95\%. But now look what happens when the variances are equal and the sample size ratio changes in Figure 5. Here too the ratio drops when the sample sizes become increasingly uneven, even though the variances stay the same. Our 95\% heuristic would lead us astray and we would incorrectly conclude that the variances are unequal.

<<dfratiosDiffNratios, echo=FALSE>>=

# equal vars, changing N ratios
Bplot.ve.ne.ns50 <- qplot(y=dfs.list$ve.ns50.ne.nullT, geom='boxplot', x=' ', ylab=bquote(frac('df'['Welch'],'df'['Student'])), xlab=bquote(frac('n'[1],'n'[2])~'='~frac(50,50)~', '~frac('var'[1],'var'[2])~'='~1), ylim=c(.45,1))
Bplot.ve.1.5n.ns50 <- qplot(y=dfs.list$ve.ns50.1.5n.nullT, geom='boxplot', x=' ', ylab=bquote(frac('df'['Welch'],'df'['Student'])), xlab=bquote(frac('n'[1],'n'[2])~'='~frac(50,75)~', '~frac('var'[1],'var'[2])~'='~1), ylim=c(.45,1))
Bplot.ve.2n.ns50 <- qplot(y=dfs.list$ve.ns50.2n.nullT, geom='boxplot', x=' ', ylab=bquote(frac('df'['Welch'],'df'['Student'])), xlab=bquote(frac('n'[1],'n'[2])~'='~frac(50,100)~', '~frac('var'[1],'var'[2])~'='~1), ylim=c(.45,1))

grid.arrange(Bplot.ve.ne.ns50, Bplot.ve.1.5n.ns50 , Bplot.ve.2n.ns50, ncol=3, heights=unit(.5,'npc'), main='Figure 5. Degrees of Freedom Ratios with Changing Sample Size Ratios')

@

    Finally, Figure 6 demonstrates one example of what happens to the degrees of freedom ratio when the variances become increasingly unequal and the sample sizes are unequal. In this case, the effect of different variances depends on whether the larger group has the larger variance or the smaller variance. In the first row, when the larger group has the larger variance, the move from equal to unequal variances actual counteracts the effect of the unequal sample sizes, and the degrees of freedom are generally higher at both unequal variance ratios that we sampled. However, in the second row, when the larger group has the smaller variance, the degrees of freedom ratio drops as the variances become increasingly unequal. 

<<dfratiosDiffvarsDiffNratios, echo=FALSE>>=

# different vars, changing N ratios
Bplot.ve.1.5n.ns50 <- qplot(y=dfs.list$ve.ns50.1.5n.nullT, geom='boxplot', x=' ', ylab=bquote(frac('df'['Welch'],'df'['Student'])), xlab=bquote(frac('n'[1],'n'[2])~'='~frac(50,75)~', '~frac('var'[1],'var'[2])~'='~1), ylim=c(.4,1))
Bplot.v2.1.5n.ssv.ns50 <- qplot(y=dfs.list$v2.ns50.1.5n.ssv.nullT, geom='boxplot', x=' ', ylab=bquote(frac('df'['Welch'],'df'['Student'])), xlab=bquote(frac('n'[1],'n'[2])~'='~frac(50,75)~', '~frac('var'[1],'var'[2])~'='~frac(1,2)), ylim=c(.4,1))
Bplot.v5.1.5n.ssv.ns50 <- qplot(y=dfs.list$v5.ns50.1.5n.ssv.nullT, geom='boxplot', x=' ', ylab=bquote(frac('df'['Welch'],'df'['Student'])), xlab=bquote(frac('n'[1],'n'[2])~'='~frac(50,75)~', '~frac('var'[1],'var'[2])~'='~frac(1,5)), ylim=c(.4,1))

Bplot.ve.1.5n.ns50 <- qplot(y=dfs.list$ve.ns50.1.5n.nullT, geom='boxplot', x=' ', ylab=bquote(frac('df'['Welch'],'df'['Student'])), xlab=bquote(frac('n'[1],'n'[2])~'='~frac(50,75)~', '~frac('var'[1],'var'[2])~'='~1), ylim=c(.4,1))
Bplot.v2.1.5n.bsv.ns50 <- qplot(y=dfs.list$v2.ns50.1.5n.bsv.nullT, geom='boxplot', x=' ', ylab=bquote(frac('df'['Welch'],'df'['Student'])), xlab=bquote(frac('n'[1],'n'[2])~'='~frac(50,75)~', '~frac('var'[1],'var'[2])~'='~2), ylim=c(.4,1))
Bplot.v5.1.5n.bsv.ns50 <- qplot(y=dfs.list$v5.ns50.1.5n.bsv.nullT, geom='boxplot', x=' ', ylab=bquote(frac('df'['Welch'],'df'['Student'])), xlab=bquote(frac('n'[1],'n'[2])~'='~frac(50,75)~', '~frac('var'[1],'var'[2])~'='~5), ylim=c(.4,1))

grid.arrange(Bplot.ve.1.5n.ns50, Bplot.v2.1.5n.ssv.ns50, Bplot.v5.1.5n.ssv.ns50, Bplot.ve.1.5n.ns50,  Bplot.v2.1.5n.bsv.ns50, Bplot.v5.1.5n.bsv.ns50, ncol=3, main='Figure 6. Degrees of Freedom Ratios with Changing Variance Ratios and Sample Size Ratios')

@

    In short, the usefulness of a heuristic based on the ratio of degrees of freedom from the Welch test to degrees of freedom from Student's test is limited to cases when the sample sizes are equal. 

    One of the concerns about using the Welch test as an alternative to Student's t test is that the penalty on the degrees of freedom makes it difficult to find effects. The boxplots show that under ideal conditions, when the true population variances and the sample sizes are equal, the penalty is small. Even under non-ideal conditions, when both the population variances and the sample sizes are unequal, there is variability in the penalty depending on the specific configuration of sample sizes and variances.

\subsection{When Does Each Test Perform Best?}
    The simple rule based on the degrees of freedom penalty did not work well, so we decided to examine when the Welch and Student t tests would perform best based on the sample size, variance ratio, and sample size ratio. We examined how well each test balances the concerns about false positives, power, and estimation, and we report the observed Type I error rates, observed power, and coverage probability for the classic and separate variance t tests over the 10000 simulations.
\subsection{Type I Error Rates}
<<type1 setup, echo=FALSE>>=

##### Student's t test data
## Set up data
nullT.ve.ne <- reject.null.ve.ne[,1,1]
nullT.v2.ne <- reject.null.v2.ne[,1,1]
nullT.v5.ne <- reject.null.v5.ne[,1,1]
nullT.ve.1.5n <- reject.null.ve.1.5n[,1,1]
nullT.ve.2n <- reject.null.ve.2n[,1,1]
nullT.v2.1.5n.ssv <- reject.null.v2.1.5n.ssv[,1,1]
nullT.v2.1.5n.bsv <- reject.null.v2.1.5n.bsv[,1,1]
nullT.v2.2n.ssv <- reject.null.v2.2n.ssv[,1,1]
nullT.v2.2n.bsv <- reject.null.v2.2n.bsv[,1,1]
nullT.v5.1.5n.ssv <- reject.null.v5.1.5n.ssv[,1,1]
nullT.v5.1.5n.bsv <- reject.null.v5.1.5n.bsv[,1,1]
nullT.v5.2n.ssv <- reject.null.v5.2n.ssv[,1,1]
nullT.v5.2n.bsv <- reject.null.v5.2n.bsv[,1,1]

## Put it in a dataframe
type1.classic <- data.frame(
    type1.rate=c(nullT.ve.ne, nullT.v2.ne, nullT.v2.ne, nullT.v5.ne, nullT.v5.ne, nullT.ve.1.5n, nullT.ve.2n, nullT.v2.1.5n.ssv, nullT.v2.1.5n.bsv, nullT.v2.2n.ssv, nullT.v2.2n.bsv, nullT.v5.1.5n.ssv, nullT.v5.1.5n.bsv, nullT.v5.2n.ssv, nullT.v5.2n.bsv), #NOTE: doubled any with equal variance or equal ns because they don't have separate ssv and bsv versions
    Ns=factor(c('20,20','50,50','100,100','20,20','50,50','100,100','20,20','50,50','100,100','20,20','50,50','100,100','20,20','50,50','100,100','20,30','50,75','100,150','20,40','50,100','100,200','20,30','50,75','100,150','20,30','50,75','100,150','20,40','50,100','100,200','20,40','50,100','100,200','20,30','50,75','100,150','20,30','50,75','100,150','20,40','50,100','100,200','20,40','50,100','100,200'), levels=c('20,20','20,30','20,40','50,50','50,75','50,100','100,100','100,150','100,200')),
    var.ratio=factor(rep(c(0,-1,1,-2,2,0,0,-1,1, -1,1,-2,2,-2,2), each=3), levels=c(-2,-1,0,1,2), labels=c(.2,.5,1,2,5)))

## Plot it
T1classic <- ggplot(type1.classic, aes(x=as.numeric(var.ratio), y=type1.rate, shape=Ns, color=Ns, ymin=0, ymax=.13)) + 
    geom_point(size=4, solid=FALSE) + 
    scale_shape_manual(values=rep(c(1,2,0), 3)) +
    geom_line(size=.5) + 
    scale_color_manual(values=rep(c('deepskyblue4','darkgoldenrod1', 'darkgrey'), each=3)) + 
    scale_x_discrete(breaks=seq(-2:2), labels=c(.2,.5,1,2,5)) +
    scale_y_continuous(breaks=seq(from=0, to=.13, by=.01)) +
    xlab('Variance Ratio (var1/var2)') + ylab('Type I Error Rate') + ggtitle("Figure 7. Type I Error Rates for Student's t Test")


##### Welch's t test data
## Set up data
welch.nullT.ve.ne <- reject.null.ve.ne[,1,2]
welch.nullT.v2.ne <- reject.null.v2.ne[,1,2]
welch.nullT.v5.ne <- reject.null.v5.ne[,1,2]
welch.nullT.ve.1.5n <- reject.null.ve.1.5n[,1,2]
welch.nullT.ve.2n <- reject.null.ve.2n[,1,2]
welch.nullT.v2.1.5n.ssv <- reject.null.v2.1.5n.ssv[,1,2]
welch.nullT.v2.1.5n.bsv <- reject.null.v2.1.5n.bsv[,1,2]
welch.nullT.v2.2n.ssv <- reject.null.v2.2n.ssv[,1,2]
welch.nullT.v2.2n.bsv <- reject.null.v2.2n.bsv[,1,2]
welch.nullT.v5.1.5n.ssv <- reject.null.v5.1.5n.ssv[,1,2]
welch.nullT.v5.1.5n.bsv <- reject.null.v5.1.5n.bsv[,1,2]
welch.nullT.v5.2n.ssv <- reject.null.v5.2n.ssv[,1,2]
welch.nullT.v5.2n.bsv <- reject.null.v5.2n.bsv[,1,2]

## Put it in a dataframe
type1.welch <- data.frame(
    type1.rate=c(welch.nullT.ve.ne, welch.nullT.v2.ne, welch.nullT.v2.ne, welch.nullT.v5.ne, welch.nullT.v5.ne, welch.nullT.ve.1.5n, welch.nullT.ve.2n, welch.nullT.v2.1.5n.ssv, welch.nullT.v2.1.5n.bsv, welch.nullT.v2.2n.ssv, welch.nullT.v2.2n.bsv, welch.nullT.v5.1.5n.ssv, welch.nullT.v5.1.5n.bsv, welch.nullT.v5.2n.ssv, welch.nullT.v5.2n.bsv), #NOTE: doubled any with equal variance or equal ns because they don't have separate ssv and bsv versions
    Ns=factor(c('20,20','50,50','100,100','20,20','50,50','100,100','20,20','50,50','100,100','20,20','50,50','100,100','20,20','50,50','100,100','20,30','50,75','100,150','20,40','50,100','100,200','20,30','50,75','100,150','20,30','50,75','100,150','20,40','50,100','100,200','20,40','50,100','100,200','20,30','50,75','100,150','20,30','50,75','100,150','20,40','50,100','100,200','20,40','50,100','100,200'), levels=c('20,20','20,30','20,40','50,50','50,75','50,100','100,100','100,150','100,200')),                                                                                                                                                                                                                                                                                                                
    var.ratio=factor(rep(c(0,-1,1,-2,2,0,0,-1,1, -1,1,-2,2,-2,2), each=3), levels=c(-2,-1,0,1,2), labels=c(.2,.5,1,2,5)))
type1.welch <- type1.welch[order(type1.welch$Ns, type1.welch$var.ratio),]

## Plot it
T1welch <- ggplot(type1.welch, aes(x=as.numeric(var.ratio), y=type1.rate, shape=Ns, color=Ns, ymin=0, ymax=.13)) + 
    geom_point(size=4, solid=FALSE) + 
    scale_shape_manual(values=rep(c(1,2,0), 3)) +
    geom_line(size=.5) + 
    scale_color_manual(values=rep(c('deepskyblue4','darkgoldenrod1', 'darkgrey'), each=3)) + 
    scale_x_discrete(breaks=seq(-2:2), labels=c(.2,.5,1,2,5)) +
    scale_y_continuous(breaks=seq(from=0, to=.13, by=.01)) +
    xlab('Variance Ratio (var1/var2)') + ylab('Type I Error Rate') + ggtitle("Figure 8. Type I Error Rates for Welch's t Test")

@

    In this section we report type I error rates for the classic and separate variance t tests when the null hypothesis is true. Prior research has demonstrated that when either sample sizes or variances are equal, the type I error rates are preserved at .05 for both tests (CITATIONS). This prior work has typically examined sample sizes that are far smaller than the typical psychology experiment.
    
    Figure 7 displays the type I error rate for the classic t test across the different conditions. Consistent with prior research, the type I error rate remained close to .05 when either the sample size or the population variances were equal, but it varied widely when both population variances and sample sizes were unequal. On the left side of Figure 1, when the group with the larger sample size had the larger variance, the type I error rate dropped as low as about .01, whereas on the right side, when the group with the larger sample size had the smaller variance, the type I error rate rose as high as .12, which is more than double the normally accepted false positive rate. 
    
<<type1 classic plot, echo=FALSE>>=
T1classic
@

    Figure 8 displays the type I error rate for the separate variance t test across the different conditions. In contrast to the classic t test, and consistent with prior research, the type I error rate remained close to .05 across all conditions. 
    
    Here we see the degrees of freedom penalty at work. Our boxplots showed that the penalty was greatest when the large group had the small variance, which reduced the false positive rate of the classic t test. In contrast, the penalty was less severe when the large group had the large variance, so that the Welch test did not reduce the type I error rate beyond the .05 level. Additionally, we can see the different effects of using pooled and separate variances to compute the standard errors. Because the pooled variance of the classic t test weighs the larger sample more heavily, it becomes more liberal when the associated variance is small and more conservative when the associated variance is large. 

<<type1 Welch plot, echo=FALSE>>=
T1welch
@

\subsection{Power}
<<classic power setup, echo=FALSE>>=

## Set up data for observed power

# d=.2
smalld.ve.ne <- reject.null.ve.ne[,2,1]
smalld.v2.ne <- reject.null.v2.ne[,2,1]
smalld.v5.ne <- reject.null.v5.ne[,2,1]
smalld.ve.1.5n <- reject.null.ve.1.5n[,2,1]
smalld.ve.2n <- reject.null.ve.2n[,2,1]
smalld.v2.1.5n.ssv <- reject.null.v2.1.5n.ssv[,2,1]
smalld.v2.1.5n.bsv <- reject.null.v2.1.5n.bsv[,2,1]
smalld.v2.2n.ssv <- reject.null.v2.2n.ssv[,2,1]
smalld.v2.2n.bsv <- reject.null.v2.2n.bsv[,2,1]
smalld.v5.1.5n.ssv <- reject.null.v5.1.5n.ssv[,2,1]
smalld.v5.1.5n.bsv <- reject.null.v5.1.5n.bsv[,2,1]
smalld.v5.2n.ssv <- reject.null.v5.2n.ssv[,2,1]
smalld.v5.2n.bsv <- reject.null.v5.2n.bsv[,2,1]

power.classic.smalld <- data.frame(
    power=c(smalld.ve.ne, smalld.v2.ne, smalld.v2.ne, smalld.v5.ne, smalld.v5.ne, smalld.ve.1.5n, smalld.ve.2n, smalld.v2.1.5n.ssv, smalld.v2.1.5n.bsv, smalld.v2.2n.ssv, smalld.v2.2n.bsv, smalld.v5.1.5n.ssv, smalld.v5.1.5n.bsv, smalld.v5.2n.ssv, smalld.v5.2n.bsv), #NOTE: doubled any with equal variance or equal ns because they don't have separate ssv and bsv versions
    Ns=factor(c('20,20','50,50','100,100','20,20','50,50','100,100','20,20','50,50','100,100','20,20','50,50','100,100','20,20','50,50','100,100','20,30','50,75','100,150','20,40','50,100','100,200','20,30','50,75','100,150','20,30','50,75','100,150','20,40','50,100','100,200','20,40','50,100','100,200','20,30','50,75','100,150','20,30','50,75','100,150','20,40','50,100','100,200','20,40','50,100','100,200'), levels=c('20,20','20,30','20,40','50,50','50,75','50,100','100,100','100,150','100,200')),                                                                                                                                                                                                                                                                                                                
    var.ratio=factor(rep(c(0,-1,1,-2,2,0,0,-1,1,-1,1,-2,2,-2,2), each=3), levels=c(-2,-1,0,1,2), labels=c(.2,.5,1,2,5)))
power.classic.smalld <- power.classic.smalld[order(power.classic.smalld$Ns, power.classic.smalld$var.ratio),]


# d=.5
midd.ve.ne <- reject.null.ve.ne[,3,1]
midd.v2.ne <- reject.null.v2.ne[,3,1]
midd.v5.ne <- reject.null.v5.ne[,3,1]
midd.ve.1.5n <- reject.null.ve.1.5n[,3,1]
midd.ve.2n <- reject.null.ve.2n[,3,1]
midd.v2.1.5n.ssv <- reject.null.v2.1.5n.ssv[,3,1]
midd.v2.1.5n.bsv <- reject.null.v2.1.5n.bsv[,3,1]
midd.v2.2n.ssv <- reject.null.v2.2n.ssv[,3,1]
midd.v2.2n.bsv <- reject.null.v2.2n.bsv[,3,1]
midd.v5.1.5n.ssv <- reject.null.v5.1.5n.ssv[,3,1]
midd.v5.1.5n.bsv <- reject.null.v5.1.5n.bsv[,3,1]
midd.v5.2n.ssv <- reject.null.v5.2n.ssv[,3,1]
midd.v5.2n.bsv <- reject.null.v5.2n.bsv[,3,1]

power.classic.midd <- data.frame(
    power=c(midd.ve.ne, midd.v2.ne, midd.v2.ne, midd.v5.ne, midd.v5.ne, midd.ve.1.5n, midd.ve.2n, midd.v2.1.5n.ssv, midd.v2.1.5n.bsv, midd.v2.2n.ssv, midd.v2.2n.bsv, midd.v5.1.5n.ssv, midd.v5.1.5n.bsv, midd.v5.2n.ssv, midd.v5.2n.bsv), #NOTE: doubled any with equal variance or equal ns because they don't have separate ssv and bsv versions
    Ns=factor(c('20,20','50,50','100,100','20,20','50,50','100,100','20,20','50,50','100,100','20,20','50,50','100,100','20,20','50,50','100,100','20,30','50,75','100,150','20,40','50,100','100,200','20,30','50,75','100,150','20,30','50,75','100,150','20,40','50,100','100,200','20,40','50,100','100,200','20,30','50,75','100,150','20,30','50,75','100,150','20,40','50,100','100,200','20,40','50,100','100,200'), levels=c('20,20','20,30','20,40','50,50','50,75','50,100','100,100','100,150','100,200')),                                                                                                                                                                                                                                                                                                                
    var.ratio=factor(rep(c(0,-1,1,-2,2,0,0,-1,1,-1,1,-2,2,-2,2), each=3), levels=c(-2,-1,0,1,2), labels=c(.2,.5,1,2,5)))
power.classic.midd <- power.classic.midd[order(power.classic.midd$Ns, power.classic.midd$var.ratio),]

# d=.8
bigd.ve.ne <- reject.null.ve.ne[,4,1]
bigd.v2.ne <- reject.null.v2.ne[,4,1]
bigd.v5.ne <- reject.null.v5.ne[,4,1]
bigd.ve.1.5n <- reject.null.ve.1.5n[,4,1]
bigd.ve.2n <- reject.null.ve.2n[,4,1]
bigd.v2.1.5n.ssv <- reject.null.v2.1.5n.ssv[,4,1]
bigd.v2.1.5n.bsv <- reject.null.v2.1.5n.bsv[,4,1]
bigd.v2.2n.ssv <- reject.null.v2.2n.ssv[,4,1]
bigd.v2.2n.bsv <- reject.null.v2.2n.bsv[,4,1]
bigd.v5.1.5n.ssv <- reject.null.v5.1.5n.ssv[,4,1]
bigd.v5.1.5n.bsv <- reject.null.v5.1.5n.bsv[,4,1]
bigd.v5.2n.ssv <- reject.null.v5.2n.ssv[,4,1]
bigd.v5.2n.bsv <- reject.null.v5.2n.bsv[,4,1]

power.classic.bigd <- data.frame(
    power=c(bigd.ve.ne, bigd.v2.ne, bigd.v2.ne, bigd.v5.ne, bigd.v5.ne, bigd.ve.1.5n, bigd.ve.2n, bigd.v2.1.5n.ssv, bigd.v2.1.5n.bsv, bigd.v2.2n.ssv, bigd.v2.2n.bsv, bigd.v5.1.5n.ssv, bigd.v5.1.5n.bsv, bigd.v5.2n.ssv, bigd.v5.2n.bsv), #NOTE: doubled any with equal variance or equal ns because they don't have separate ssv and bsv versions
    Ns=factor(c('20,20','50,50','100,100','20,20','50,50','100,100','20,20','50,50','100,100','20,20','50,50','100,100','20,20','50,50','100,100','20,30','50,75','100,150','20,40','50,100','100,200','20,30','50,75','100,150','20,30','50,75','100,150','20,40','50,100','100,200','20,40','50,100','100,200','20,30','50,75','100,150','20,30','50,75','100,150','20,40','50,100','100,200','20,40','50,100','100,200'), levels=c('20,20','20,30','20,40','50,50','50,75','50,100','100,100','100,150','100,200')),                                                                                                                                                                                                                                                                                                                
    var.ratio=factor(rep(c(0,-1,1,-2,2,0,0,-1,1,-1,1,-2,2,-2,2), each=3), levels=c(-2,-1,0,1,2), labels=c(.2,.5,1,2,5)))
power.classic.bigd <- power.classic.bigd[order(power.classic.bigd$Ns, power.classic.bigd$var.ratio),]


## Set up data for deviation from expected power
power.classic.smalld$expected <- c(rep(.095, 5), rep(.104, 5), rep(.111, 5), rep(.168, 5), rep(.192, 5), rep(.209, 5), rep(.291, 5), rep(.339, 5), rep(.370, 5))
power.classic.smalld$power.diff <- power.classic.smalld$power-power.classic.smalld$expected

power.classic.midd$expected <- c(rep(.338, 5), rep(.397, 5), rep(.435, 5), rep(.697, 5), rep(.776, 5), rep(.818, 5), rep(.940, 5), rep(.971, 5), rep(.983, 5))
power.classic.midd$power.diff <- power.classic.midd$power-power.classic.midd$expected

power.classic.bigd$expected <- c(rep(.693, 5), rep(.775, 5), rep(.819, 5), rep(.977, 5), rep(.992, 5), rep(.996, 5), rep(1, 5), rep(1, 5), rep(1, 5))
power.classic.bigd$power.diff <- power.classic.bigd$power-power.classic.bigd$expected

## Classic t, d=.2, power
## Plot using ggplot2
smalld.classic.power <- ggplot(power.classic.smalld, aes(x=as.numeric(var.ratio), y=power, shape=Ns, color=Ns, ymin=0, ymax=1)) + 
    geom_point(size=3, solid=FALSE) + 
    scale_shape_manual(values=rep(c(1,2,0), 3)) +
    geom_line(size=.5) + 
    scale_color_manual(values=rep(c('deepskyblue4','darkgoldenrod1', 'darkgrey'), each=3)) + 
    scale_x_discrete(breaks=seq(-2:2), labels=c(.2,.5,1,2,5)) +
    scale_y_continuous(breaks=seq(from=0, to=1, by=.1)) +
    xlab('Variance Ratio (var1/var2)') + ylab('Small Effect') + ggtitle("Student's t Test")



## Set up classic t test plot of deviation from expected power
smalld.classic.deviation <- ggplot(power.classic.smalld, aes(x=as.numeric(var.ratio), y=power.diff, shape=Ns, color=Ns, ymin=-.3, ymax=.3)) + 
    geom_point(size=3, solid=FALSE) + 
    scale_shape_manual(values=rep(c(1,2,0), 3)) +
    geom_line(size=.5) + 
    scale_color_manual(values=rep(c('deepskyblue4','darkgoldenrod1', 'darkgrey'), each=3)) + 
    scale_x_discrete(breaks=seq(-2:2), labels=c(.2,.5,1,2,5)) +
    scale_y_continuous(breaks=seq(from=-.3, to=.3, length.out=7)) +
    xlab('Variance Ratio (var1/var2)') + ylab('Deviation') + ggtitle("Student's t Test")


## Classic t, d=.5, power
## Set up classic t test plot of deviation from expected power
midd.classic.power <- ggplot(power.classic.midd, aes(x=as.numeric(var.ratio), y=power, shape=Ns, color=Ns, ymin=0, ymax=1)) + 
    geom_point(size=3, solid=FALSE) + 
    scale_shape_manual(values=rep(c(1,2,0), 3)) +
    geom_line(size=.5) + 
    scale_color_manual(values=rep(c('deepskyblue4','darkgoldenrod1', 'darkgrey'), each=3)) + 
    scale_x_discrete(breaks=seq(-2:2), labels=c(.2,.5,1,2,5)) +
    scale_y_continuous(breaks=seq(from=0, to=1, by=.1)) +
    xlab('Variance Ratio (var1/var2)') + ylab('Medium Effect') #+ ggtitle("Student's t Test")

## Set up classic t test plot of deviation from expected power
midd.classic.deviation <- ggplot(power.classic.midd, aes(x=as.numeric(var.ratio), y=power.diff, shape=Ns, color=Ns, ymin=-.3, ymax=.3)) + 
    geom_point(size=3, solid=FALSE) + 
    scale_shape_manual(values=rep(c(1,2,0), 3)) +
    geom_line(size=.5) + 
    scale_color_manual(values=rep(c('deepskyblue4','darkgoldenrod1', 'darkgrey'), each=3)) + 
    scale_x_discrete(breaks=seq(-2:2), labels=c(.2,.5,1,2,5)) +
    scale_y_continuous(breaks=seq(from=-.3, to=.3, length.out=7)) +
    xlab('Variance Ratio (var1/var2)') + ylab('Deviation') + ggtitle("Student's t Test")




## Classic t, d=.8, power
bigd.classic.power <- ggplot(power.classic.bigd, aes(x=as.numeric(var.ratio), y=power, shape=Ns, color=Ns, ymin=0, ymax=1)) + 
    geom_point(size=3, solid=FALSE) + 
    scale_shape_manual(values=rep(c(1,2,0), 3)) +
    geom_line(size=.5) + 
    scale_color_manual(values=rep(c('deepskyblue4','darkgoldenrod1', 'darkgrey'), each=3)) + 
    scale_x_discrete(breaks=seq(-2:2), labels=c(.2,.5,1,2,5)) +
    scale_y_continuous(breaks=seq(from=0, to=1, by=.1)) +
    xlab('Variance Ratio (var1/var2)') + ylab('Large Effect') #+ ggtitle("Student's t Test")

## Set up classic t test plot of deviation from expected power
bigd.classic.deviation <- ggplot(power.classic.bigd, aes(x=as.numeric(var.ratio), y=power.diff, shape=Ns, color=Ns, ymin=-.3, ymax=.3)) + 
    geom_point(size=3, solid=FALSE) + 
    scale_shape_manual(values=rep(c(1,2,0), 3)) +
    geom_line(size=.5) + 
    scale_color_manual(values=rep(c('deepskyblue4','darkgoldenrod1', 'darkgrey'), each=3)) + 
    scale_x_discrete(breaks=seq(-2:2), labels=c(.2,.5,1,2,5)) +
    scale_y_continuous(breaks=seq(from=-.3, to=.3, length.out=7)) +
    xlab('Variance Ratio (var1/var2)') + ylab('Deviation') + ggtitle("Student's t Test")



@

<<welch power setup, echo=FALSE>>=

## Set up data for observed power

# d=.2
smalld.ve.ne <- reject.null.ve.ne[,2,2]
smalld.v2.ne <- reject.null.v2.ne[,2,2]
smalld.v5.ne <- reject.null.v5.ne[,2,2]
smalld.ve.1.5n <- reject.null.ve.1.5n[,2,2]
smalld.ve.2n <- reject.null.ve.2n[,2,2]
smalld.v2.1.5n.ssv <- reject.null.v2.1.5n.ssv[,2,2]
smalld.v2.1.5n.bsv <- reject.null.v2.1.5n.bsv[,2,2]
smalld.v2.2n.ssv <- reject.null.v2.2n.ssv[,2,2]
smalld.v2.2n.bsv <- reject.null.v2.2n.bsv[,2,2]
smalld.v5.1.5n.ssv <- reject.null.v5.1.5n.ssv[,2,2]
smalld.v5.1.5n.bsv <- reject.null.v5.1.5n.bsv[,2,2]
smalld.v5.2n.ssv <- reject.null.v5.2n.ssv[,2,2]
smalld.v5.2n.bsv <- reject.null.v5.2n.bsv[,2,2]

power.welch.smalld <- data.frame(
    power=c(smalld.ve.ne, smalld.v2.ne, smalld.v2.ne, smalld.v5.ne, smalld.v5.ne, smalld.ve.1.5n, smalld.ve.2n, smalld.v2.1.5n.ssv, smalld.v2.1.5n.bsv, smalld.v2.2n.ssv, smalld.v2.2n.bsv, smalld.v5.1.5n.ssv, smalld.v5.1.5n.bsv, smalld.v5.2n.ssv, smalld.v5.2n.bsv), #NOTE: doubled any with equal variance or equal ns because they don't have separate ssv and bsv versions
    Ns=factor(c('20,20','50,50','100,100','20,20','50,50','100,100','20,20','50,50','100,100','20,20','50,50','100,100','20,20','50,50','100,100','20,30','50,75','100,150','20,40','50,100','100,200','20,30','50,75','100,150','20,30','50,75','100,150','20,40','50,100','100,200','20,40','50,100','100,200','20,30','50,75','100,150','20,30','50,75','100,150','20,40','50,100','100,200','20,40','50,100','100,200'), levels=c('20,20','20,30','20,40','50,50','50,75','50,100','100,100','100,150','100,200')),                                                                                                                                                                                                                                                                                                                
    var.ratio=factor(rep(c(0,-1,1,-2,2,0,0,-1,1,-1,1,-2,2,-2,2), each=3), levels=c(-2,-1,0,1,2), labels=c(.2,.5,1,2,5)))
power.welch.smalld <- power.welch.smalld[order(power.welch.smalld$Ns, power.welch.smalld$var.ratio),]

# d=.5
midd.ve.ne <- reject.null.ve.ne[,3,2]
midd.v2.ne <- reject.null.v2.ne[,3,2]
midd.v5.ne <- reject.null.v5.ne[,3,2]
midd.ve.1.5n <- reject.null.ve.1.5n[,3,2]
midd.ve.2n <- reject.null.ve.2n[,3,2]
midd.v2.1.5n.ssv <- reject.null.v2.1.5n.ssv[,3,2]
midd.v2.1.5n.bsv <- reject.null.v2.1.5n.bsv[,3,2]
midd.v2.2n.ssv <- reject.null.v2.2n.ssv[,3,2]
midd.v2.2n.bsv <- reject.null.v2.2n.bsv[,3,2]
midd.v5.1.5n.ssv <- reject.null.v5.1.5n.ssv[,3,2]
midd.v5.1.5n.bsv <- reject.null.v5.1.5n.bsv[,3,2]
midd.v5.2n.ssv <- reject.null.v5.2n.ssv[,3,2]
midd.v5.2n.bsv <- reject.null.v5.2n.bsv[,3,2]

power.welch.midd <- data.frame(
    power=c(midd.ve.ne, midd.v2.ne, midd.v2.ne, midd.v5.ne, midd.v5.ne, midd.ve.1.5n, midd.ve.2n, midd.v2.1.5n.ssv, midd.v2.1.5n.bsv, midd.v2.2n.ssv, midd.v2.2n.bsv, midd.v5.1.5n.ssv, midd.v5.1.5n.bsv, midd.v5.2n.ssv, midd.v5.2n.bsv), #NOTE: doubled any with equal variance or equal ns because they don't have separate ssv and bsv versions
    Ns=factor(c('20,20','50,50','100,100','20,20','50,50','100,100','20,20','50,50','100,100','20,20','50,50','100,100','20,20','50,50','100,100','20,30','50,75','100,150','20,40','50,100','100,200','20,30','50,75','100,150','20,30','50,75','100,150','20,40','50,100','100,200','20,40','50,100','100,200','20,30','50,75','100,150','20,30','50,75','100,150','20,40','50,100','100,200','20,40','50,100','100,200'), levels=c('20,20','20,30','20,40','50,50','50,75','50,100','100,100','100,150','100,200')),                                                                                                                                                                                                                                                                                                                
    var.ratio=factor(rep(c(0,-1,1,-2,2,0,0,-1,1,-1,1,-2,2,-2,2), each=3), levels=c(-2,-1,0,1,2), labels=c(.2,.5,1,2,5)))
power.welch.midd <- power.welch.midd[order(power.welch.midd$Ns, power.welch.midd$var.ratio),]

# d=.8
bigd.ve.ne <- reject.null.ve.ne[,4,2]
bigd.v2.ne <- reject.null.v2.ne[,4,2]
bigd.v5.ne <- reject.null.v5.ne[,4,2]
bigd.ve.1.5n <- reject.null.ve.1.5n[,4,2]
bigd.ve.2n <- reject.null.ve.2n[,4,2]
bigd.v2.1.5n.ssv <- reject.null.v2.1.5n.ssv[,4,2]
bigd.v2.1.5n.bsv <- reject.null.v2.1.5n.bsv[,4,2]
bigd.v2.2n.ssv <- reject.null.v2.2n.ssv[,4,2]
bigd.v2.2n.bsv <- reject.null.v2.2n.bsv[,4,2]
bigd.v5.1.5n.ssv <- reject.null.v5.1.5n.ssv[,4,2]
bigd.v5.1.5n.bsv <- reject.null.v5.1.5n.bsv[,4,2]
bigd.v5.2n.ssv <- reject.null.v5.2n.ssv[,4,2]
bigd.v5.2n.bsv <- reject.null.v5.2n.bsv[,4,2]

power.welch.bigd <- data.frame(
    power=c(bigd.ve.ne, bigd.v2.ne, bigd.v2.ne, bigd.v5.ne, bigd.v5.ne, bigd.ve.1.5n, bigd.ve.2n, bigd.v2.1.5n.ssv, bigd.v2.1.5n.bsv, bigd.v2.2n.ssv, bigd.v2.2n.bsv, bigd.v5.1.5n.ssv, bigd.v5.1.5n.bsv, bigd.v5.2n.ssv, bigd.v5.2n.bsv), #NOTE: doubled any with equal variance or equal ns because they don't have separate ssv and bsv versions
    Ns=factor(c('20,20','50,50','100,100','20,20','50,50','100,100','20,20','50,50','100,100','20,20','50,50','100,100','20,20','50,50','100,100','20,30','50,75','100,150','20,40','50,100','100,200','20,30','50,75','100,150','20,30','50,75','100,150','20,40','50,100','100,200','20,40','50,100','100,200','20,30','50,75','100,150','20,30','50,75','100,150','20,40','50,100','100,200','20,40','50,100','100,200'), levels=c('20,20','20,30','20,40','50,50','50,75','50,100','100,100','100,150','100,200')),                                                                                                                                                                                                                                                                                                                
    var.ratio=factor(rep(c(0,-1,1,-2,2,0,0,-1,1,-1,1,-2,2,-2,2), each=3), levels=c(-2,-1,0,1,2), labels=c(.2,.5,1,2,5)))
power.welch.bigd <- power.welch.bigd[order(power.welch.bigd$Ns, power.welch.bigd$var.ratio),]




## Set up data for deviation from expected power
## Equal Ns
power.welch.smalld$expected <- c(rep(.095, 5), rep(.104, 5), rep(.111, 5), rep(.168, 5), rep(.192, 5), rep(.209, 5), rep(.291, 5), rep(.339, 5), rep(.370, 5))
power.welch.smalld$power.diff <- power.welch.smalld$power-power.welch.smalld$expected

power.welch.midd$expected <- c(rep(.338, 5), rep(.397, 5), rep(.435, 5), rep(.697, 5), rep(.776, 5), rep(.818, 5), rep(.940, 5), rep(.971, 5), rep(.983, 5))
power.welch.midd$power.diff <- power.welch.midd$power-power.welch.midd$expected

power.welch.bigd$expected <- c(rep(.693, 5), rep(.775, 5), rep(.819, 5), rep(.977, 5), rep(.992, 5), rep(.996, 5), rep(1, 5), rep(1, 5), rep(1, 5))
power.welch.bigd$power.diff <- power.welch.bigd$power-power.welch.bigd$expected


## Welch t, d=.2, power
smalld.welch.power <- ggplot(power.welch.smalld, aes(x=as.numeric(var.ratio), y=power, shape=Ns, color=Ns, ymin=0, ymax=1)) + 
    geom_point(size=3, solid=FALSE) + 
    scale_shape_manual(values=rep(c(1,2,0), 3)) +
    geom_line(size=.5) + 
    scale_color_manual(values=rep(c('deepskyblue4','darkgoldenrod1', 'darkgrey'), each=3)) + 
    scale_x_discrete(breaks=seq(-2:2), labels=c(.2,.5,1,2,5)) +
    scale_y_continuous(breaks=seq(from=0, to=1, by=.1)) +
    xlab('Variance Ratio (var1/var2)') + ylab('Small Effect') + ggtitle("Welch's t Test")



## Set up classic t test plot of deviation from expected power
smalld.welch.deviation <- ggplot(power.welch.smalld, aes(x=as.numeric(var.ratio), y=power.diff, shape=Ns, color=Ns, ymin=-.3, ymax=.3)) + 
    geom_point(size=3, solid=FALSE) + 
    scale_shape_manual(values=rep(c(1,2,0), 3)) +
    geom_line(size=.5) + 
    scale_color_manual(values=rep(c('deepskyblue4','darkgoldenrod1', 'darkgrey'), each=3)) + 
    scale_x_discrete(breaks=seq(-2:2), labels=c(.2,.5,1,2,5)) +
    scale_y_continuous(breaks=seq(from=-.3, to=.3, length.out=7)) +
    xlab('Variance Ratio (var1/var2)') + ylab('Deviation') + ggtitle("Welch's t Test")


## Welch t, d=.5, power
midd.welch.power <- ggplot(power.welch.midd, aes(x=as.numeric(var.ratio), y=power, shape=Ns, color=Ns, ymin=0, ymax=1)) + 
    geom_point(size=3, solid=FALSE) + 
    scale_shape_manual(values=rep(c(1,2,0), 3)) +
    geom_line(size=.5) + 
    scale_color_manual(values=rep(c('deepskyblue4','darkgoldenrod1', 'darkgrey'), each=3)) + 
    scale_x_discrete(breaks=seq(-2:2), labels=c(.2,.5,1,2,5)) +
    scale_y_continuous(breaks=seq(from=0, to=1, by=.1)) +
    xlab('Variance Ratio (var1/var2)') + ylab('Medium Effect') #+ ggtitle("Welch's t Test")



## Set up Welch t test plot of deviation from expected power
midd.welch.deviation <- ggplot(power.welch.midd, aes(x=as.numeric(var.ratio), y=power.diff, shape=Ns, color=Ns, ymin=-.3, ymax=.3)) + 
    geom_point(size=3, solid=FALSE) + 
    scale_shape_manual(values=rep(c(1,2,0), 3)) +
    geom_line(size=.5) + 
    scale_color_manual(values=rep(c('deepskyblue4','darkgoldenrod1', 'darkgrey'), each=3)) + 
    scale_x_discrete(breaks=seq(-2:2), labels=c(.2,.5,1,2,5)) +
    scale_y_continuous(breaks=seq(from=-.3, to=.3, length.out=7)) +
    xlab('Variance Ratio (var1/var2)') + ylab('Deviation') + ggtitle("Welch's t Test")



## Welch t, d=.8, power
bigd.welch.power <- ggplot(power.welch.bigd, aes(x=as.numeric(var.ratio), y=power, shape=Ns, color=Ns, ymin=0, ymax=1)) + 
    geom_point(size=3, solid=FALSE) + 
    scale_shape_manual(values=rep(c(1,2,0), 3)) +
    geom_line(size=.5) + 
    scale_color_manual(values=rep(c('deepskyblue4','darkgoldenrod1', 'darkgrey'), each=3)) + 
    scale_x_discrete(breaks=seq(-2:2), labels=c(.2,.5,1,2,5)) +
    scale_y_continuous(breaks=seq(from=0, to=1, by=.1)) +
    xlab('Variance Ratio (var1/var2)') + ylab('Large Effect') #+ ggtitle("Welch's t Test")



## Set up Welch t test plot of deviation from expected power
bigd.welch.deviation <- ggplot(power.welch.bigd, aes(x=as.numeric(var.ratio), y=power.diff, shape=Ns, color=Ns, ymin=-.3, ymax=.3)) + 
    geom_point(size=3, solid=FALSE) + 
    scale_shape_manual(values=rep(c(1,2,0), 3)) +
    geom_line(size=.5) + 
    scale_color_manual(values=rep(c('deepskyblue4','darkgoldenrod1', 'darkgrey'), each=3)) + 
    scale_x_discrete(breaks=seq(-2:2), labels=c(.2,.5,1,2,5)) +
    scale_y_continuous(breaks=seq(from=-.3, to=.3, length.out=7)) +
    xlab('Variance Ratio (var1/var2)') + ylab('Deviation') + ggtitle("Welch's t Test")

@

    Figure 9 displays the power of the classic and Welch tests to detect small, medium, and large effects under the different conditions. Overall, the classic t test is more powerful when the large sample has the smaller variance, whereas the Welch test is more powerful when the small sample has the smaller variance. These differences are the most dramatic when one sample is twice the size of the other. 
    
    The conditions in which the classic t test has the greatest power over the Welch test, when one sample is twice the size of the other and the large sample has the small variance, are the same conditions in which the classic t test had a risk of doubling the false positive rate. In contrast, the Welch test was more powerful than the classic test under other conditions and never inflated the type I error rate.

    Taken together, the type I error rates and power favor the Welch test over the classic t test as better balancing researcher concerns.

<<plot power, echo=FALSE>>=

grid.arrange(smalld.classic.power, smalld.welch.power, midd.classic.power, midd.welch.power, bigd.classic.power, bigd.welch.power, nrow=3, main='Figure 9. Power of the Classic and Welch Tests')

#multiplot(smalld.classic.power, smalld.classic.deviation, smalld.welch.power, smalld.welch.deviation, cols=2)

#multiplot(midd.classic.power, midd.classic.deviation, midd.welch.power, midd.welch.deviation, cols=2)

#multiplot(bigd.classic.power, bigd.classic.deviation, bigd.welch.power, bigd.welch.deviation, cols=2)

@

\subsection{Coverage Probability}
Because the accuracy of a confidence interval is influenced by the variance and sample size, but not by the true effect size, we only show the coverage probability when the null hypothesis is true (the coverage probabilities are identical across all effect sizes). Table 10 displays the coverage probability, which is how often the 95\% confidence interval contains the true mean difference in groups, for the two tests under the different conditions. The coverage probability for the classic t test varies dramatically. When it is the least powerful, it is the most accurate. When it is the most powerful, the effect size estimation is the least accurate, and what seems to be a 95\% confidence interval drops as low as an 88\% confidence interval. Once again, the Welch test retains the expected 95\% rate and turns out to best meet a researcher's concerns. 

<<classic coverage, echo=FALSE>>=

##### Plot classic results only

## Set up data
nullT.ve.ne <- obs.coverage.ve.ne[,1,1]
nullT.v2.ne <- obs.coverage.v2.ne[,1,1]
nullT.v5.ne <- obs.coverage.v5.ne[,1,1]
nullT.ve.1.5n <- obs.coverage.ve.1.5n[,1,1]
nullT.ve.2n <- obs.coverage.ve.2n[,1,1]
nullT.v2.1.5n.ssv <- obs.coverage.v2.1.5n.ssv[,1,1]
nullT.v2.1.5n.bsv <- obs.coverage.v2.1.5n.bsv[,1,1]
nullT.v2.2n.ssv <- obs.coverage.v2.2n.ssv[,1,1]
nullT.v2.2n.bsv <- obs.coverage.v2.2n.bsv[,1,1]
nullT.v5.1.5n.ssv <- obs.coverage.v5.1.5n.ssv[,1,1]
nullT.v5.1.5n.bsv <- obs.coverage.v5.1.5n.bsv[,1,1]
nullT.v5.2n.ssv <- obs.coverage.v5.2n.ssv[,1,1]
nullT.v5.2n.bsv <- obs.coverage.v5.2n.bsv[,1,1]
smalld.ve.ne <- obs.coverage.ve.ne[,2,1]
smalld.v2.ne <- obs.coverage.v2.ne[,2,1]
smalld.v5.ne <- obs.coverage.v5.ne[,2,1]
smalld.ve.1.5n <- obs.coverage.ve.1.5n[,2,1]
smalld.ve.2n <- obs.coverage.ve.2n[,2,1]
smalld.v2.1.5n.ssv <- obs.coverage.v2.1.5n.ssv[,2,1]
smalld.v2.1.5n.bsv <- obs.coverage.v2.1.5n.bsv[,2,1]
smalld.v2.2n.ssv <- obs.coverage.v2.2n.ssv[,2,1]
smalld.v2.2n.bsv <- obs.coverage.v2.2n.bsv[,2,1]
smalld.v5.1.5n.ssv <- obs.coverage.v5.1.5n.ssv[,2,1]
smalld.v5.1.5n.bsv <- obs.coverage.v5.1.5n.bsv[,2,1]
smalld.v5.2n.ssv <- obs.coverage.v5.2n.ssv[,2,1]
smalld.v5.2n.bsv <- obs.coverage.v5.2n.bsv[,2,1]
midd.ve.ne <- obs.coverage.ve.ne[,3,1]
midd.v2.ne <- obs.coverage.v2.ne[,3,1]
midd.v5.ne <- obs.coverage.v5.ne[,3,1]
midd.ve.1.5n <- obs.coverage.ve.1.5n[,3,1]
midd.ve.2n <- obs.coverage.ve.2n[,3,1]
midd.v2.1.5n.ssv <- obs.coverage.v2.1.5n.ssv[,3,1]
midd.v2.1.5n.bsv <- obs.coverage.v2.1.5n.bsv[,3,1]
midd.v2.2n.ssv <- obs.coverage.v2.2n.ssv[,3,1]
midd.v2.2n.bsv <- obs.coverage.v2.2n.bsv[,3,1]
midd.v5.1.5n.ssv <- obs.coverage.v5.1.5n.ssv[,3,1]
midd.v5.1.5n.bsv <- obs.coverage.v5.1.5n.bsv[,3,1]
midd.v5.2n.ssv <- obs.coverage.v5.2n.ssv[,3,1]
midd.v5.2n.bsv <- obs.coverage.v5.2n.bsv[,3,1]
bigd.ve.ne <- obs.coverage.ve.ne[,4,1]
bigd.v2.ne <- obs.coverage.v2.ne[,4,1]
bigd.v5.ne <- obs.coverage.v5.ne[,4,1]
bigd.ve.1.5n <- obs.coverage.ve.1.5n[,4,1]
bigd.ve.2n <- obs.coverage.ve.2n[,4,1]
bigd.v2.1.5n.ssv <- obs.coverage.v2.1.5n.ssv[,4,1]
bigd.v2.1.5n.bsv <- obs.coverage.v2.1.5n.bsv[,4,1]
bigd.v2.2n.ssv <- obs.coverage.v2.2n.ssv[,4,1]
bigd.v2.2n.bsv <- obs.coverage.v2.2n.bsv[,4,1]
bigd.v5.1.5n.ssv <- obs.coverage.v5.1.5n.ssv[,4,1]
bigd.v5.1.5n.bsv <- obs.coverage.v5.1.5n.bsv[,4,1]
bigd.v5.2n.ssv <- obs.coverage.v5.2n.ssv[,4,1]
bigd.v5.2n.bsv <- obs.coverage.v5.2n.bsv[,4,1]

## Put it in a dataframe
coverage.classic.nullT <- data.frame(
    coverage.rate=c(nullT.ve.ne, nullT.v2.ne, nullT.v2.ne, nullT.v5.ne, nullT.v5.ne, nullT.ve.1.5n, nullT.ve.2n, nullT.v2.1.5n.ssv, nullT.v2.1.5n.bsv, nullT.v2.2n.ssv, nullT.v2.2n.bsv, nullT.v5.1.5n.ssv, nullT.v5.1.5n.bsv, nullT.v5.2n.ssv, nullT.v5.2n.bsv), #NOTE: doubled any with equal variance or equal ns because they don't have separate ssv and bsv versions
    Ns=factor(c('20,20','50,50','100,100','20,20','50,50','100,100','20,20','50,50','100,100','20,20','50,50','100,100','20,20','50,50','100,100','20,30','50,75','100,150','20,40','50,100','100,200','20,30','50,75','100,150','20,30','50,75','100,150','20,40','50,100','100,200','20,40','50,100','100,200','20,30','50,75','100,150','20,30','50,75','100,150','20,40','50,100','100,200','20,40','50,100','100,200'), levels=c('20,20','20,30','20,40','50,50','50,75','50,100','100,100','100,150','100,200')),                                                                                                                                                                                                                                                                                                                
    var.ratio=factor(rep(c(0,-1,1,-2,2,0,0,-1,1,-1,1,-2,2,-2,2), each=3), levels=c(-2,-1,0,1,2), labels=c(.2,.5,1,2,5)))
coverage.classic.nullT <- coverage.classic.nullT[order(coverage.classic.nullT$Ns, coverage.classic.nullT$var.ratio),]

coverage.classic.smalld <- data.frame(
    coverage.rate=c(smalld.ve.ne, smalld.v2.ne, smalld.v2.ne, smalld.v5.ne, smalld.v5.ne, smalld.ve.1.5n, smalld.ve.2n, smalld.v2.1.5n.ssv, smalld.v2.1.5n.bsv, smalld.v2.2n.ssv, smalld.v2.2n.bsv, smalld.v5.1.5n.ssv, smalld.v5.1.5n.bsv, smalld.v5.2n.ssv, smalld.v5.2n.bsv), #NOTE: doubled any with equal variance or equal ns because they don't have separate ssv and bsv versions
    Ns=factor(c('20,20','50,50','100,100','20,20','50,50','100,100','20,20','50,50','100,100','20,20','50,50','100,100','20,20','50,50','100,100','20,30','50,75','100,150','20,40','50,100','100,200','20,30','50,75','100,150','20,30','50,75','100,150','20,40','50,100','100,200','20,40','50,100','100,200','20,30','50,75','100,150','20,30','50,75','100,150','20,40','50,100','100,200','20,40','50,100','100,200'), levels=c('20,20','20,30','20,40','50,50','50,75','50,100','100,100','100,150','100,200')),                                                                                                                                                                                                                                                                                                                
    var.ratio=factor(rep(c(0,-1,1,-2,2,0,0,-1,1,-1,1,-2,2,-2,2), each=3), levels=c(-2,-1,0,1,2), labels=c(.2,.5,1,2,5)))
coverage.classic.smalld <- coverage.classic.smalld[order(coverage.classic.smalld$Ns, coverage.classic.smalld$var.ratio),]

coverage.classic.midd <- data.frame(
    coverage.rate=c(midd.ve.ne, midd.v2.ne, midd.v2.ne, midd.v5.ne, midd.v5.ne, midd.ve.1.5n, midd.ve.2n, midd.v2.1.5n.ssv, midd.v2.1.5n.bsv, midd.v2.2n.ssv, midd.v2.2n.bsv, midd.v5.1.5n.ssv, midd.v5.1.5n.bsv, midd.v5.2n.ssv, midd.v5.2n.bsv), #NOTE: doubled any with equal variance or equal ns because they don't have separate ssv and bsv versions
    Ns=factor(c('20,20','50,50','100,100','20,20','50,50','100,100','20,20','50,50','100,100','20,20','50,50','100,100','20,20','50,50','100,100','20,30','50,75','100,150','20,40','50,100','100,200','20,30','50,75','100,150','20,30','50,75','100,150','20,40','50,100','100,200','20,40','50,100','100,200','20,30','50,75','100,150','20,30','50,75','100,150','20,40','50,100','100,200','20,40','50,100','100,200'), levels=c('20,20','20,30','20,40','50,50','50,75','50,100','100,100','100,150','100,200')),                                                                                                                                                                                                                                                                                                                
    var.ratio=factor(rep(c(0,-1,1,-2,2,0,0,-1,1,-1,1,-2,2,-2,2), each=3), levels=c(-2,-1,0,1,2), labels=c(.2,.5,1,2,5)))
coverage.classic.midd <- coverage.classic.midd[order(coverage.classic.midd$Ns, coverage.classic.midd$var.ratio),]

coverage.classic.bigd <- data.frame(
    coverage.rate=c(bigd.ve.ne, bigd.v2.ne, bigd.v2.ne, bigd.v5.ne, bigd.v5.ne, bigd.ve.1.5n, bigd.ve.2n, bigd.v2.1.5n.ssv, bigd.v2.1.5n.bsv, bigd.v2.2n.ssv, bigd.v2.2n.bsv, bigd.v5.1.5n.ssv, bigd.v5.1.5n.bsv, bigd.v5.2n.ssv, bigd.v5.2n.bsv), #NOTE: doubled any with equal variance or equal ns because they don't have separate ssv and bsv versions
    Ns=factor(c('20,20','50,50','100,100','20,20','50,50','100,100','20,20','50,50','100,100','20,20','50,50','100,100','20,20','50,50','100,100','20,30','50,75','100,150','20,40','50,100','100,200','20,30','50,75','100,150','20,30','50,75','100,150','20,40','50,100','100,200','20,40','50,100','100,200','20,30','50,75','100,150','20,30','50,75','100,150','20,40','50,100','100,200','20,40','50,100','100,200'), levels=c('20,20','20,30','20,40','50,50','50,75','50,100','100,100','100,150','100,200')),                                                                                                                                                                                                                                                                                                                
    var.ratio=factor(rep(c(0,-1,1,-2,2,0,0,-1,1,-1,1,-2,2,-2,2), each=3), levels=c(-2,-1,0,1,2), labels=c(.2,.5,1,2,5)))
coverage.classic.bigd <- coverage.classic.bigd[order(coverage.classic.bigd$Ns, coverage.classic.bigd$var.ratio),]


## Set up classic t test plot
CovClassic.nullT <- ggplot(coverage.classic.nullT, aes(x=as.numeric(var.ratio), y=coverage.rate, shape=Ns, color=Ns, ymin=.85, ymax=1)) + 
    geom_point(size=3, solid=FALSE) + 
    scale_shape_manual(values=rep(c(1,2,0), 3)) +
    geom_line(size=.5) + 
    scale_color_manual(values=rep(c('deepskyblue4','darkgoldenrod1', 'darkgrey'), each=3)) + 
    scale_x_discrete(breaks=seq(-2:2), labels=c(.2,.5,1,2,5)) +
    scale_y_continuous(breaks=seq(from=.85, to=1, length.out=16)) +
    xlab('Variance Ratio (var1/var2)') + ylab('Coverage Probability') + ggtitle("Student's t Test")

CovClassic.smalld <- ggplot(coverage.classic.smalld, aes(x=as.numeric(var.ratio), y=coverage.rate, shape=Ns, color=Ns, ymin=.85, ymax=1)) + 
    geom_point(size=3, solid=FALSE) + 
    scale_shape_manual(values=rep(c(1,2,0), 3)) +
    geom_line(size=.5) + 
    scale_color_manual(values=rep(c('deepskyblue4','darkgoldenrod1', 'darkgrey'), each=3)) + 
    scale_x_discrete(breaks=seq(-2:2), labels=c(.2,.5,1,2,5)) +
    scale_y_continuous(breaks=seq(from=.85, to=1, length.out=16)) +
    xlab('Variance Ratio (var1/var2)') + ylab('Coverage Probability') + ggtitle("Student's t Test")

CovClassic.midd <- ggplot(coverage.classic.midd, aes(x=as.numeric(var.ratio), y=coverage.rate, shape=Ns, color=Ns, ymin=.85, ymax=1)) + 
    geom_point(size=3, solid=FALSE) + 
    scale_shape_manual(values=rep(c(1,2,0), 3)) +
    geom_line(size=.5) + 
    scale_color_manual(values=rep(c('deepskyblue4','darkgoldenrod1', 'darkgrey'), each=3)) + 
    scale_x_discrete(breaks=seq(-2:2), labels=c(.2,.5,1,2,5)) +
    scale_y_continuous(breaks=seq(from=.85, to=1, length.out=16)) +
    xlab('Variance Ratio (var1/var2)') + ylab('Coverage Probability') + ggtitle("Student's t Test")

CovClassic.bigd <- ggplot(coverage.classic.bigd, aes(x=as.numeric(var.ratio), y=coverage.rate, shape=Ns, color=Ns, ymin=.85, ymax=1)) + 
    geom_point(size=3, solid=FALSE) + 
    scale_shape_manual(values=rep(c(1,2,0), 3)) +
    geom_line(size=.5) + 
    scale_color_manual(values=rep(c('deepskyblue4','darkgoldenrod1', 'darkgrey'), each=3)) + 
    scale_x_discrete(breaks=seq(-2:2), labels=c(.2,.5,1,2,5)) +
    scale_y_continuous(breaks=seq(from=.85, to=1, length.out=16)) +
    xlab('Variance Ratio (var1/var2)') + ylab('Coverage Probability') + ggtitle("Student's t Test")

@


<<welch coverage, echo=FALSE>>=

##### Plot Welch results only

## Set up data
nullT.ve.ne <- obs.coverage.ve.ne[,1,2]
nullT.v2.ne <- obs.coverage.v2.ne[,1,2]
nullT.v5.ne <- obs.coverage.v5.ne[,1,2]
nullT.ve.1.5n <- obs.coverage.ve.1.5n[,1,2]
nullT.ve.2n <- obs.coverage.ve.2n[,1,2]
nullT.v2.1.5n.ssv <- obs.coverage.v2.1.5n.ssv[,1,2]
nullT.v2.1.5n.bsv <- obs.coverage.v2.1.5n.bsv[,1,2]
nullT.v2.2n.ssv <- obs.coverage.v2.2n.ssv[,1,2]
nullT.v2.2n.bsv <- obs.coverage.v2.2n.bsv[,1,2]
nullT.v5.1.5n.ssv <- obs.coverage.v5.1.5n.ssv[,1,2]
nullT.v5.1.5n.bsv <- obs.coverage.v5.1.5n.bsv[,1,2]
nullT.v5.2n.ssv <- obs.coverage.v5.2n.ssv[,1,2]
nullT.v5.2n.bsv <- obs.coverage.v5.2n.bsv[,1,2]
smalld.ve.ne <- obs.coverage.ve.ne[,2,2]
smalld.v2.ne <- obs.coverage.v2.ne[,2,2]
smalld.v5.ne <- obs.coverage.v5.ne[,2,2]
smalld.ve.1.5n <- obs.coverage.ve.1.5n[,2,2]
smalld.ve.2n <- obs.coverage.ve.2n[,2,2]
smalld.v2.1.5n.ssv <- obs.coverage.v2.1.5n.ssv[,2,2]
smalld.v2.1.5n.bsv <- obs.coverage.v2.1.5n.bsv[,2,2]
smalld.v2.2n.ssv <- obs.coverage.v2.2n.ssv[,2,2]
smalld.v2.2n.bsv <- obs.coverage.v2.2n.bsv[,2,2]
smalld.v5.1.5n.ssv <- obs.coverage.v5.1.5n.ssv[,2,2]
smalld.v5.1.5n.bsv <- obs.coverage.v5.1.5n.bsv[,2,2]
smalld.v5.2n.ssv <- obs.coverage.v5.2n.ssv[,2,2]
smalld.v5.2n.bsv <- obs.coverage.v5.2n.bsv[,2,2]
midd.ve.ne <- obs.coverage.ve.ne[,3,2]
midd.v2.ne <- obs.coverage.v2.ne[,3,2]
midd.v5.ne <- obs.coverage.v5.ne[,3,2]
midd.ve.1.5n <- obs.coverage.ve.1.5n[,3,2]
midd.ve.2n <- obs.coverage.ve.2n[,3,2]
midd.v2.1.5n.ssv <- obs.coverage.v2.1.5n.ssv[,3,2]
midd.v2.1.5n.bsv <- obs.coverage.v2.1.5n.bsv[,3,2]
midd.v2.2n.ssv <- obs.coverage.v2.2n.ssv[,3,2]
midd.v2.2n.bsv <- obs.coverage.v2.2n.bsv[,3,2]
midd.v5.1.5n.ssv <- obs.coverage.v5.1.5n.ssv[,3,2]
midd.v5.1.5n.bsv <- obs.coverage.v5.1.5n.bsv[,3,2]
midd.v5.2n.ssv <- obs.coverage.v5.2n.ssv[,3,2]
midd.v5.2n.bsv <- obs.coverage.v5.2n.bsv[,3,2]
bigd.ve.ne <- obs.coverage.ve.ne[,4,2]
bigd.v2.ne <- obs.coverage.v2.ne[,4,2]
bigd.v5.ne <- obs.coverage.v5.ne[,4,2]
bigd.ve.1.5n <- obs.coverage.ve.1.5n[,4,2]
bigd.ve.2n <- obs.coverage.ve.2n[,4,2]
bigd.v2.1.5n.ssv <- obs.coverage.v2.1.5n.ssv[,4,2]
bigd.v2.1.5n.bsv <- obs.coverage.v2.1.5n.bsv[,4,2]
bigd.v2.2n.ssv <- obs.coverage.v2.2n.ssv[,4,2]
bigd.v2.2n.bsv <- obs.coverage.v2.2n.bsv[,4,2]
bigd.v5.1.5n.ssv <- obs.coverage.v5.1.5n.ssv[,4,2]
bigd.v5.1.5n.bsv <- obs.coverage.v5.1.5n.bsv[,4,2]
bigd.v5.2n.ssv <- obs.coverage.v5.2n.ssv[,4,2]
bigd.v5.2n.bsv <- obs.coverage.v5.2n.bsv[,4,2]

## Put it in a dataframe
coverage.welch.nullT <- data.frame(
    coverage.rate=c(nullT.ve.ne, nullT.v2.ne, nullT.v2.ne, nullT.v5.ne, nullT.v5.ne, nullT.ve.1.5n, nullT.ve.2n, nullT.v2.1.5n.ssv, nullT.v2.1.5n.bsv, nullT.v2.2n.ssv, nullT.v2.2n.bsv, nullT.v5.1.5n.ssv, nullT.v5.1.5n.bsv, nullT.v5.2n.ssv, nullT.v5.2n.bsv), #NOTE: doubled any with equal variance or equal ns because they don't have separate ssv and bsv versions
    Ns=factor(c('20,20','50,50','100,100','20,20','50,50','100,100','20,20','50,50','100,100','20,20','50,50','100,100','20,20','50,50','100,100','20,30','50,75','100,150','20,40','50,100','100,200','20,30','50,75','100,150','20,30','50,75','100,150','20,40','50,100','100,200','20,40','50,100','100,200','20,30','50,75','100,150','20,30','50,75','100,150','20,40','50,100','100,200','20,40','50,100','100,200'), levels=c('20,20','20,30','20,40','50,50','50,75','50,100','100,100','100,150','100,200')),                                                                                                                                                                                                                                                                                                                
    var.ratio=factor(rep(c(0,-1,1,-2,2,0,0,-1,1,-1,1,-2,2,-2,2), each=3), levels=c(-2,-1,0,1,2), labels=c(.2,.5,1,2,5)))
coverage.welch.nullT <- coverage.welch.nullT[order(coverage.welch.nullT$Ns, coverage.welch.nullT$var.ratio),]

coverage.welch.smalld <- data.frame(
    coverage.rate=c(smalld.ve.ne, smalld.v2.ne, smalld.v2.ne, smalld.v5.ne, smalld.v5.ne, smalld.ve.1.5n, smalld.ve.2n, smalld.v2.1.5n.ssv, smalld.v2.1.5n.bsv, smalld.v2.2n.ssv, smalld.v2.2n.bsv, smalld.v5.1.5n.ssv, smalld.v5.1.5n.bsv, smalld.v5.2n.ssv, smalld.v5.2n.bsv), #NOTE: doubled any with equal variance or equal ns because they don't have separate ssv and bsv versions
    Ns=factor(c('20,20','50,50','100,100','20,20','50,50','100,100','20,20','50,50','100,100','20,20','50,50','100,100','20,20','50,50','100,100','20,30','50,75','100,150','20,40','50,100','100,200','20,30','50,75','100,150','20,30','50,75','100,150','20,40','50,100','100,200','20,40','50,100','100,200','20,30','50,75','100,150','20,30','50,75','100,150','20,40','50,100','100,200','20,40','50,100','100,200'), levels=c('20,20','20,30','20,40','50,50','50,75','50,100','100,100','100,150','100,200')),                                                                                                                                                                                                                                                                                                                
    var.ratio=factor(rep(c(0,-1,1,-2,2,0,0,-1,1,-1,1,-2,2,-2,2), each=3), levels=c(-2,-1,0,1,2), labels=c(.2,.5,1,2,5)))
coverage.welch.smalld <- coverage.welch.smalld[order(coverage.welch.smalld$Ns, coverage.welch.smalld$var.ratio),]

coverage.welch.midd <- data.frame(
    coverage.rate=c(midd.ve.ne, midd.v2.ne, midd.v2.ne, midd.v5.ne, midd.v5.ne, midd.ve.1.5n, midd.ve.2n, midd.v2.1.5n.ssv, midd.v2.1.5n.bsv, midd.v2.2n.ssv, midd.v2.2n.bsv, midd.v5.1.5n.ssv, midd.v5.1.5n.bsv, midd.v5.2n.ssv, midd.v5.2n.bsv), #NOTE: doubled any with equal variance or equal ns because they don't have separate ssv and bsv versions
    Ns=factor(c('20,20','50,50','100,100','20,20','50,50','100,100','20,20','50,50','100,100','20,20','50,50','100,100','20,20','50,50','100,100','20,30','50,75','100,150','20,40','50,100','100,200','20,30','50,75','100,150','20,30','50,75','100,150','20,40','50,100','100,200','20,40','50,100','100,200','20,30','50,75','100,150','20,30','50,75','100,150','20,40','50,100','100,200','20,40','50,100','100,200'), levels=c('20,20','20,30','20,40','50,50','50,75','50,100','100,100','100,150','100,200')),                                                                                                                                                                                                                                                                                                                
    var.ratio=factor(rep(c(0,-1,1,-2,2,0,0,-1,1,-1,1,-2,2,-2,2), each=3), levels=c(-2,-1,0,1,2), labels=c(.2,.5,1,2,5)))
coverage.welch.midd <- coverage.welch.midd[order(coverage.welch.midd$Ns, coverage.welch.midd$var.ratio),]

coverage.welch.bigd <- data.frame(
    coverage.rate=c(bigd.ve.ne, bigd.v2.ne, bigd.v2.ne, bigd.v5.ne, bigd.v5.ne, bigd.ve.1.5n, bigd.ve.2n, bigd.v2.1.5n.ssv, bigd.v2.1.5n.bsv, bigd.v2.2n.ssv, bigd.v2.2n.bsv, bigd.v5.1.5n.ssv, bigd.v5.1.5n.bsv, bigd.v5.2n.ssv, bigd.v5.2n.bsv), #NOTE: doubled any with equal variance or equal ns because they don't have separate ssv and bsv versions
    Ns=factor(c('20,20','50,50','100,100','20,20','50,50','100,100','20,20','50,50','100,100','20,20','50,50','100,100','20,20','50,50','100,100','20,30','50,75','100,150','20,40','50,100','100,200','20,30','50,75','100,150','20,30','50,75','100,150','20,40','50,100','100,200','20,40','50,100','100,200','20,30','50,75','100,150','20,30','50,75','100,150','20,40','50,100','100,200','20,40','50,100','100,200'), levels=c('20,20','20,30','20,40','50,50','50,75','50,100','100,100','100,150','100,200')),                                                                                                                                                                                                                                                                                                                
    var.ratio=factor(rep(c(0,-1,1,-2,2,0,0,-1,1,-1,1,-2,2,-2,2), each=3), levels=c(-2,-1,0,1,2), labels=c(.2,.5,1,2,5)))
coverage.welch.bigd <- coverage.welch.bigd[order(coverage.welch.bigd$Ns, coverage.welch.bigd$var.ratio),]


## Set up welch t test plot
CovWelch.nullT <- ggplot(coverage.welch.nullT, aes(x=as.numeric(var.ratio), y=coverage.rate, shape=Ns, color=Ns, ymin=.85, ymax=1)) + 
    geom_point(size=3, solid=FALSE) + 
    scale_shape_manual(values=rep(c(1,2,0), 3)) +
    geom_line(size=.5) + 
    scale_color_manual(values=rep(c('deepskyblue4','darkgoldenrod1', 'darkgrey'), each=3)) + 
    scale_x_discrete(breaks=seq(-2:2), labels=c(.2,.5,1,2,5)) +
    scale_y_continuous(breaks=seq(from=.85, to=1, length.out=16)) +
    xlab('Variance Ratio (var1/var2)') + ylab('Coverage Probability') + ggtitle("Welch's t Test")

CovWelch.smalld <- ggplot(coverage.welch.smalld, aes(x=as.numeric(var.ratio), y=coverage.rate, shape=Ns, color=Ns, ymin=.85, ymax=1)) + 
    geom_point(size=3, solid=FALSE) + 
    scale_shape_manual(values=rep(c(1,2,0), 3)) +
    geom_line(size=.5) + 
    scale_color_manual(values=rep(c('deepskyblue4','darkgoldenrod1', 'darkgrey'), each=3)) + 
    scale_x_discrete(breaks=seq(-2:2), labels=c(.2,.5,1,2,5)) +
    scale_y_continuous(breaks=seq(from=.85, to=1, length.out=16)) +
    xlab('Variance Ratio (var1/var2)') + ylab('Coverage Probability') + ggtitle("Welch's t Test")

CovWelch.midd <- ggplot(coverage.welch.midd, aes(x=as.numeric(var.ratio), y=coverage.rate, shape=Ns, color=Ns, ymin=.85, ymax=1)) + 
    geom_point(size=3, solid=FALSE) + 
    scale_shape_manual(values=rep(c(1,2,0), 3)) +
    geom_line(size=.5) + 
    scale_color_manual(values=rep(c('deepskyblue4','darkgoldenrod1', 'darkgrey'), each=3)) + 
    scale_x_discrete(breaks=seq(-2:2), labels=c(.2,.5,1,2,5)) +
    scale_y_continuous(breaks=seq(from=.85, to=1, length.out=16)) +
    xlab('Variance Ratio (var1/var2)') + ylab('Coverage Probability') + ggtitle("Welch's t Test")

CovWelch.bigd <- ggplot(coverage.welch.bigd, aes(x=as.numeric(var.ratio), y=coverage.rate, shape=Ns, color=Ns, ymin=.85, ymax=1)) + 
    geom_point(size=3, solid=FALSE) + 
    scale_shape_manual(values=rep(c(1,2,0), 3)) +
    geom_line(size=.5) + 
    scale_color_manual(values=rep(c('deepskyblue4','darkgoldenrod1', 'darkgrey'), each=3)) + 
    scale_x_discrete(breaks=seq(-2:2), labels=c(.2,.5,1,2,5)) +
    scale_y_continuous(breaks=seq(from=.85, to=1, length.out=16)) +
    xlab('Variance Ratio (var1/var2)') + ylab('Coverage Probability') + ggtitle("Welch's t Test")

@

<<coverage plots, echo=FALSE>>=

grid.arrange(CovClassic.nullT, CovWelch.nullT, nrow=1, main='Figure 10. Coverage Probabilities for the Classic and Welch Tests')

@


\section{Discussion}
    Make a note that our df ratio rule works best when it doesn't matter - when sample sizes are equal.
    
    In much experimental work, the choice between the tests is probably fine if the experimenter ensures that sample sizes are equal. This is generally not an option with pre-existing groups. 



\end{document}