\documentclass[man,a4paper,noextraspace,apacite]{apa6}
\usepackage{apacite}

\title{Always Use Welch's t Test to Compare the Means of Two Independent Groups}
\shorttitle{Welch t Test}
\author{Joshua D. Wondra and Richard Gonzalez}
\affiliation{University of Michigan}

\abstract{This is an abstract}
\keywords{t test, new statistics, Welch}

\authornote{Joshua D. Wondra, Department of Psychology, University of Michigan.

Richard Gonzalez, Department of Psychology, University of Michigan.

Correspondence concerning this article should be addressed to Josh Wondra, Department of Psychology, University of Michigan, 530 Church St., Ann Arbor, MI 48109-1043.

Contact: jdwondra@umich.edu}


\begin{document}
\maketitle

    Data analysis involves a series of decisions on the part of the researcher about which statistical test answers the research question, whether the data fit the requirements of the test, and whether there are alternative options that will do a better job. Recent discussions of false positives in psychology research \cite<e.g.,>{Fiedler2012, Ioannidis2012, Nosek2012, Simmons2011, Wagenmakers2012} highlight the tension between two valued outcomes of the decision process. On the one hand, researchers want to avoid mistakenly claiming that there is a true effect where none exists, which involves concerns about false positives. On the other hand, researchers want to find true effects where they do exist, which involves concerns about power. In addition to these two, there is a growing concern with estimating and reporting effect sizes \cite{Cumming2014}. Some have argued that due to some common research practices such as running underpowered studies, those effects that make it into published papers are spuriously large \cite<e.g.,>{Bakker2012, Ioannidis2008}.

    One of the first decisions that many researchers learn is how to compare the means of two independent groups--they run a t test. But even this basic comparison presents a choice between the classic Student's t test \cite{Student1908} or the alternative Welch-Satterthwaite test \cite{Welch1938, Satterthwaite1946}. Most researchers learn about Student's t test in the first statistics class that they ever take. When you use Student's t test to compare the means of independent groups, you make three assumptions: 
\begin{APAenumerate}
    \item Normality: The population for each group has a normal distribution.
    \item Independence: All observations are independent of each other, meaning that the probability of one observation having a particular value does not depend on the probability of another observation having a particular value.
    \item Equal variances: The population variances for the two groups are equal.
\end{APAenumerate}
    
If these assumptions hold, then you can find the t-value by taking the difference in group means and dividing by the standard error of that difference:   
    \begin{equation}
    t = \frac{\overline{x}_1-\overline{x}_2}{s_{\overline{x}_1-\overline{x}_2}}
    \end{equation}
    The p-value for the value of the t statistic depends on the degrees of freedom, $df=n_1+n_2-2$. You are more likely to reject the null hypothesis and conclude that there is a difference in the group means as both the degrees of freedom and the t value get larger. This means that you are more likely to conclude that there is a difference when the sample size gets larger, when the difference in group means gets larger, or when the standard error gets smaller. 
    
    In order to compute the standard error for the t test, you need to find the variance of the difference in group means. When the population variances are equal, then the variance of the difference in means is equal to the variance of either group. Unfortunately, even if the population variances are equal, the group variances in the actual data are rarely identical due to sampling error, so you can't just use one of the observed group variances to find the standard error. Student's t test deals with this problem by pooling together the two group variances to estimate a single common variance. The group with the larger sample size is given more weight than the group with the smaller sample size. This means that when the larger group has the larger variance, the standard error is bigger, but when the larger group has the smaller variance, the standard error is smaller.

    If either the data or the study design suggests that one or more of the assumptions has been violated, then Student's t test is not the right choice. Specifically, if the equal variances assumption has been violated, then the Welch-Satterthwaite t test (hereafter called the Welch t test for the sake of brevity) is a good alternative choice. Many researchers might not learn about the Welch-Satterthwaite test in their formal statistical training, though most have encountered it in their analyses. For those who use SPSS to analyze their data, the Welch t test is in the "Equal variances not assumed" row that appears by default whenever they run an independent samples t test. For those who use R, the Welch t test is the default when they use the \texttt{t.test()} function and they can only get Student's t test by setting the \texttt{var.equal} argument to \texttt{TRUE}.
    
        As with Student's t test, the Welch t test assumes normality and independence; however, it does not assume that the population variances are equal. The standard error is based on separate group variances instead of a common variance. Additionally, the Welch t test decreases the degrees of freedom to the extent that the group variances are unequal. Because of these differences, the two tests can disagree about whether there is a difference in group means. The penalty to the degrees of freedom pushes the Welch t test in the direction of being more conservative and less likely to reject the null. On the one hand, this might make the Welch t test a better choice if Student's t test finds more false positives when variances are unequal. On the other hand, this might make the Welch t test a worse choice if it is not powerful enough to detect true effects. 
        
    However, the Welch t test is not necessarily always more conservative. The power of the two tests is not only based on the degrees of freedom, but also on the standard error. This means that the Welch t test could be more powerful than Student's test if the separate variances standard error is smaller than the pooled variances standard error. 
    
    How do you decide which test to use? The typical approach is to use Student's t test unless there is evidence that the two groups have unequal population variances. The challenge is how to find that evidence. 

    One option is to run another test of the null hypothesis that the variances are equal, such as Levene's test for homogeneity which shows up by default in SPSS, and use the Welch t test if you reject the null. However, these tests of assumptions make their own assumptions that go unchecked and they are sensitive to sample size \cite{Gonzalez2008}. In addition, simulation studies find that a two-step process of running tests of equal variances to decide whether to use Student's or Welch's t test is not very effective \cite{Zimmerman1996,Zimmerman2004}. 

    A second option is to visualize the data using boxplots and make a judgment about whether the variances appear to differ.  With smaller sample sizes, you can tolerate larger apparent differences. This strategy can be enhanced by simulating data for two groups of sample sizes equal to those in your data, changing whether the variances are equal or unequal, and seeing if the boxplots of your data look like the boxplots of the simulations with equal variances. 

    A third option that has not been tested to our knowledge is to examine the ratio of the degrees of freedom between Student's t test and Welch's t test. If they differ to a large extent, then it might be a sign that the group variances differ.

    A fourth option is to change the typical approach. Under ideal conditions, when variances and sample sizes are equal, Welch's t test is equivalent to Student's t test. If using the Welch t test generally leads to better decisions than Student's t test under both ideal and non-ideal conditions, then instead of using Student's t test by default it might be better to always use Welch's t test.
    
    We examined these second, third, and fourth options in a Monte Carlo simulation study. 
    
\section{Method}

    We ran Monte Carlo simulations of two independent groups with normally distributed data. We examined the type I error rate, power, and coverage probability for both Student's t test and Welch's t test under different conditions. We varied the ratio of population variances ($\sigma_{1}^2/\sigma_{2}^2$ = 1/5, 1/2, 1, 2, or 5; smallest $\sigma^2$ = 2), the sample sizes (smallest n = 20, 50, or 100), and the ratio of sample sizes ($n_{1}/n_{2}$ = 1, 2/3, or 1/2). 
    
    Additionally, we varied the size of the difference in group means based on Cohen's d values of 0, .2, .5, and .8 when variances were equal. Importantly, Cohen's d assumes that the population variances are equal and pools the group variances just like Student's t test. This means that there is no true Cohen's d when variances are unequal. Therefore, we used the same differences in group means when variances were unequal. Because we changed the variance ratio by increasing the variance of one group, the mean differences could be considered to represent smaller effects when variances are unequal.
    
    For each condition, we set the seed to 2184 and ran 10,000 simulations. When we report conditions with equal sample sizes and variance ratios of 2 and 5, they are identical to the conditions with equal sample sizes and variance ratios of 1/2 and 1/5. 

\section{Results}   

<<setup, echo=FALSE>>=

## Retrieve tables with general data
load('/users/joshwondra/R-projects/Welch rule/veNeSeed2184Tables.Rdata')
load('/users/joshwondra/R-projects/Welch rule/v2NeSeed2184Tables.Rdata')
load('/users/joshwondra/R-projects/Welch rule/v5NeSeed2184Tables.Rdata')
load('/users/joshwondra/R-projects/Welch rule/ve1andhalfnSeed2184Tables.Rdata')
load('/users/joshwondra/R-projects/Welch rule/ve2nSeed2184Tables.Rdata')
load('/users/joshwondra/R-projects/Welch rule/v21andhalfnSSVSeed2184Tables.Rdata')
load('/users/joshwondra/R-projects/Welch rule/v51andhalfnSSVSeed2184Tables.Rdata')
load('/users/joshwondra/R-projects/Welch rule/v22nSSVSeed2184Tables.Rdata')
load('/users/joshwondra/R-projects/Welch rule/v52nSSVSeed2184Tables.Rdata')
load('/users/joshwondra/R-projects/Welch rule/v21andhalfnBSVSeed2184Tables.Rdata')
load('/users/joshwondra/R-projects/Welch rule/v51andhalfnBSVSeed2184Tables.Rdata')
load('/users/joshwondra/R-projects/Welch rule/v22nBSVSeed2184Tables.Rdata')
load('/users/joshwondra/R-projects/Welch rule/v52nBSVSeed2184Tables.Rdata')

## Retrieve data for boxplots
load('/users/joshwondra/R-projects/Welch rule/veNeSeed2184-sim1.Rdata')
load('/users/joshwondra/R-projects/Welch rule/v2NeSeed2184-sim1.Rdata')
load('/users/joshwondra/R-projects/Welch rule/v5NeSeed2184-sim1.Rdata')
load('/users/joshwondra/R-projects/Welch rule/veN15Seed2184-sim1.Rdata')
load('/users/joshwondra/R-projects/Welch rule/v2N15ssvSeed2184-sim1.Rdata')
load('/users/joshwondra/R-projects/Welch rule/v2N15bsvSeed2184-sim1.Rdata')
load('/users/joshwondra/R-projects/Welch rule/v5N15ssvSeed2184-sim1.Rdata')
load('/users/joshwondra/R-projects/Welch rule/v5N15bsvSeed2184-sim1.Rdata')
load('/users/joshwondra/R-projects/Welch rule/veN2Seed2184-sim1.Rdata')
load('/users/joshwondra/R-projects/Welch rule/v2N2ssvSeed2184-sim1.Rdata')
load('/users/joshwondra/R-projects/Welch rule/v2N2bsvSeed2184-sim1.Rdata')
load('/users/joshwondra/R-projects/Welch rule/v5N2ssvSeed2184-sim1.Rdata')
load('/users/joshwondra/R-projects/Welch rule/v5N2bsvSeed2184-sim1.Rdata')

## Load packages
library(ggplot2)
library(grid)
library(gridExtra)

# Multiple plot function
# retrieved from http://www.cookbook-r.com/Graphs/Multiple_graphs_on_one_page_(ggplot2)/

# ggplot objects can be passed in ..., or to plotlist (as a list of ggplot objects)
# - cols:   Number of columns in layout
# - layout: A matrix specifying the layout. If present, 'cols' is ignored.

# If the layout is something like matrix(c(1,2,3,3), nrow=2, byrow=TRUE),
# then plot 1 will go in the upper left, 2 will go in the upper right, and
# 3 will go all the way across the bottom.

multiplot <- function(..., plotlist=NULL, file, cols=1, layout=NULL) {

  # Make a list from the ... arguments and plotlist
  plots <- c(list(...), plotlist)

  numPlots = length(plots)

  # If layout is NULL, then use 'cols' to determine layout
  if (is.null(layout)) {
    # Make the panel
    # ncol: Number of columns of plots
    # nrow: Number of rows needed, calculated from # of cols
    layout <- matrix(seq(1, cols * ceiling(numPlots/cols)),
                    ncol = cols, nrow = ceiling(numPlots/cols))
  }

 if (numPlots==1) {
    print(plots[[1]])

  } else {
    # Set up the page
    grid.newpage()
    pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout), heights=unit(rep(1,nrow(layout)),'null'))))

    # Make each plot, in the correct location
    for (i in 1:numPlots) {
      # Get the i,j matrix positions of the regions that contain this subplot
      matchidx <- as.data.frame(which(layout == i, arr.ind = TRUE))

      print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row, 
                                      layout.pos.col = matchidx$col))
    }
  }

}

@

\subsection{Visualizing Data with Boxplots} 
    One option for deciding whether the group variances are equal is to examine boxplots. Figure 1 displays boxplots from simulations of two groups with equal variances and Figure 2 displays boxplots from simulations of two groups when the variance of the group to the right is five times as large as the variance of the group to the left. The first row displays groups with $n$=20 in the second row displays groups with $n$=100. The population distributions are displayed at the top. When the sample sizes are smaller there is more variability in the boxplots. For example, in Figure 1 it appears as though the the third boxplot displays data from populations with unequal variances, whereas in Figure 2 it appears as though the third boxplot displays data from populations with equal variances. It would be difficult to decide that the boxplots provide evidence of unequal variances when $n$=20 unless the differences were quite extreme. In contrast, when sample sizes are larger there is more consistency in the boxplots. It would be easy to determine that differences in the visual variances of the boxplots point to different population variances differ when $n$=100. By examining several additional simulations it would be possible to see how much variability in the boxplots is normal when variances are equal or unequal.
   
\begin{figure}
<<varEqualBoxplots, echo=FALSE, collapse=TRUE>>=

x1 <- seq(0,12,.1)
y1 <- dnorm(x1, mean=6, sd=sqrt(2))
x2 <- seq(1.13,13.13,.1)
y2 <- dnorm(x2, mean=7.13, sd=sqrt(2))

x <- c(x1,x2)
y <- c(y1,y2)
group <- factor(c(rep(1,length(x1)), rep(2,length(x2))))

ve.distributions <- qplot(x=x, y=y, colour=group, geom='line', xlab=NULL, ylab=NULL) + geom_vline(xintercept=as.numeric(by(x,group,mean))) + theme(legend.position='none')

set.seed(2184)
ve.bplots <- c(list(ve.distributions), lapply(1:4, function(x){qplot(x=factor(rep(c(1,2), each=20)), y=c(rnorm(20,6,sqrt(2)),rnorm(20,7.13,sqrt(2))), geom='boxplot', colour=factor(rep(c(1,2), each=20)), xlab=NULL, ylab=NULL, ylim=c(-3,18)) + theme(legend.position='none', axis.text.x=element_blank())}), lapply(1:4, function(x){qplot(x=factor(rep(c(1,2), each=100)), y=c(rnorm(100,6,sqrt(2)),rnorm(100,7.13,sqrt(2))), geom='boxplot', colour=factor(rep(c(1,2), each=100)), xlab=NULL, ylab=NULL, ylim=c(-3,18)) + theme(legend.position='none', axis.text.x=element_blank())}))

layout <- matrix(c(0,1,1,0,2,3,4,5,6,7,8,9), nrow=3, byrow=TRUE)
multiplot(plotlist=ve.bplots, layout=layout)

@
\textit{Figure 1.} Boxplots for groups with equal variances. The first row displays groups with $n$=20 and the second row displays groups with $n$=100.
\end{figure}

\begin{figure}
<<varDifferentBoxplots, echo=FALSE>>=

x1 <- seq(0,12,.1)
y1 <- dnorm(x1, mean=6, sd=sqrt(2))
x2 <- seq(1.13,13.13,.1)
y2 <- dnorm(x2, mean=7.13, sd=sqrt(10))

x <- c(x1,x2)
y <- c(y1,y2)
group <- factor(c(rep(1,length(x1)), rep(2,length(x2))))

ve.distributions <- qplot(x=x, y=y, fill=group, colour=group, geom='line', xlab=NULL, ylab=NULL) + geom_vline(xintercept=as.numeric(by(x,group,mean))) + theme(legend.position='none')

set.seed(2184)
ve.bplots <- c(list(ve.distributions), lapply(1:4, function(x){qplot(x=factor(rep(c(1,2), each=20)), y=c(rnorm(20,6,sqrt(2)),rnorm(20,7.13,sqrt(10))), geom='boxplot', colour=factor(rep(c(1,2), each=20)), xlab=NULL, ylab=NULL, ylim=c(-3,18)) + theme(legend.position='none', axis.text.x=element_blank())}), lapply(1:4, function(x){qplot(x=factor(rep(c(1,2), each=100)), y=c(rnorm(100,6,sqrt(2)),rnorm(100,7.13,sqrt(10))), geom='boxplot', colour=factor(rep(c(1,2), each=100)), xlab=NULL, ylab=NULL, ylim=c(-3,18)) + theme(legend.position='none', axis.text.x=element_blank())}))

layout <- matrix(c(0,1,1,0,2,3,4,5,6,7,8,9), nrow=3, byrow=TRUE)
multiplot(plotlist=ve.bplots, layout=layout)

@
\textit{Figure 2.} Boxplots for groups with unequal variances. The first row displays groups with $n$=20 and the second row displays groups with $n$=100.
\end{figure}


\subsection{Does the df Ratio Help?}
    We examined whether looking at ratio of the Welch t test degrees of freedom to the Student t test degrees of freedom would provide a heuristic for deciding that the equal variances assumption does not hold. Rather than simulate the df ratio, we examined the analytical df ratio as a function of the variance ratio, as a function of the sample size ratio, and as a function of both. 

    Figure 3 displays the change in the df ratio as the variance ratio increases when sample sizes are equal. As expected, the ratio decreases as the difference in variances grows larger. When the variances are equal, the df ratio is equal to 1, though in real data the observed variances will rarely be exactly equal even if the population variances are equal. When once variance is twice the size of the other, the ratio drops to .9. A useful heuristic might to assume that the variances are unequal when the ratio falls below 96\%.
  
\begin{figure}
<< dfratiosDiffvars, echo=FALSE>>=

load('/users/joshwondra/R-projects/Welch rule/dfs.list.RData')

df.ratio.Ns <- function(n1, n2, var1, var2) {
    welch.num <- (var1/n1 + var2/n2)^2
    welch.denom <- var1^2/(n1^2*(n1-1)) + var2^2/(n2^2*(n2-1))
    classic.df <- n1+n2-2
    (welch.num/welch.denom)/classic.df
}

df.ratio.defaults <- list('n1'=50,'n2'=50,'var1'=2,'var2'=2)

partial.df <- function(var = 'a', params=df.ratio.defaults){
    params[[var]]=as.name('x')
    function(x)do.call(df.ratio.Ns, params)
}

ggplot(data.frame(x=seq(2,10,.1)), aes(x)) +
    stat_function(fun=partial.df(var='var2', params=list('n1'=50,'n2'=50,'var1'=2,'var2'=2))) + #NOTE: the sample sizes don't affect the df ratio
    labs(y=bquote(frac('df'['Welch'],'df'['Student'])), x=bquote(frac(sigma[1]^2,sigma[2]^2))) +
    scale_x_continuous(breaks=c(2,4,6,8,10), labels=c(2,4,6,8,10)/2)

@
\textit{Figure 3.} Degrees of freedom ratio when sample sizes are equal and variances are unequal.
\end{figure}

    But now look at what happens when the variances are equal and the sample size ratio changes (Figure 4). Here, too, the df ratio decreases as the difference in sample sizes grows larger, even though the variances stay the same. The 96\% heuristic would lead us astray and we would incorrectly conclude that the variances are unequal in many cases when only the sample sizes are unqual.

\begin{figure}
<<dfratiosDiffNratios, echo=FALSE>>=

ggplot(data.frame(x=seq(50,100,.1)), aes(x)) + 
    stat_function(fun=partial.df(var='n2', params=list('n1'=50,'n2'=50,'var1'=2,'var2'=2))) +
    labs(y=bquote(frac('df'['Welch'],'df'['Student'])), x=bquote(frac('n'[1],'n'[2]))) +
    scale_x_continuous(breaks=c(50,60,70,80,90,100), labels=c(50,60,70,80,90,100)/50)

@
\textit{Figure 4.} Degrees of freedom ratio when sample sizes are unequal and variances are equal.
\end{figure}

    The picture becomes even more complicated when both the sample sizes and variances are unequal (Figure 5). In this case, the effect of different variances depends on whether the larger group has the larger variance or the smaller variance. When the variance of the larger group is increasing, the move from equal to unequal variances actual counteracts the effect of the unequal sample sizes at first, and the df ratio initially begins to approach 1 before dropping again. Due to the difference in sample sizes, a 96\% heuristic would mislead us into concluding that variances are equal when they are actually three times different from each other. However, when the variance of the smaller group is increasing the immediate drop in the df ratio is quite dramatic before it begins to level off. 

\begin{figure}
<<dfratiosDiffvarsDiffNratios, echo=FALSE>>=
ggplot(data.frame(x=seq(2,10,.1)), aes(x)) + 
    stat_function(fun=partial.df(var='var2', params=list('n1'=75,'n2'=50,'var1'=2,'var2'=2)), aes(colour='n2')) +
    stat_function(fun=partial.df(var='var2', params=list('n1'=50,'n2'=75,'var1'=2,'var2'=2)), aes(colour='n1/2')) +
    scale_colour_manual(values=c('blue', 'red'), labels=c(bquote(frac('n'[1],'n'[2])~'='~frac(3,2)), bquote(frac('n'[1],'n'[2])~'= '~frac(2,3)))) +
    labs(y=bquote(frac('df'['Welch'],'df'['Student'])), x=bquote(frac(sigma[1]^2,sigma[2]^2))) +
    scale_x_continuous(breaks=c(2,4,6,8,10), labels=c(2,4,6,8,10)/2)
@
\textit{Figure 5.} Degrees of freedom ratio when sample sizes are unequal and variances are unequal.
\end{figure}
    In short, the usefulness of a heuristic based on the df ratio is limited to cases when the sample sizes are equal. 

\subsection{When Does Each Test Perform Best?}
    There did not seem to be a simple rule based on the degrees of freedom penalty to detect whether variances are unequal, so we decided to examine when the Welch and Student t tests would perform best based on the sample size, variance ratio, and sample size ratio. We examined how well each test balances the concerns about false positives, power, and estimation. Some prior research has examined the Type I error rate for the two tests \cite{Boneau1960, Zimmerman1993, Zimmerman2004, Zimmerman1996, Zimmerman2009} and some has also examined the power of the two tests, though not always representing the complete configuration of conditions that we examined in our simulations \cite{Neuhauser2002, Zimmerman1993}. Nevertheless, we believe that it will be informative to display the false positives and power of the two tests here. We also discuss implications for effect size estimation, which follows from the false positive results but has not, to our knowledge, been discussed explicitly in past research.

NOTE: Zimmerman1993 only reports power when Student's t test looks bad; Neuhauser only reports power when sample sizes are equal

NOTE: I excluded here citations that looked at ANOVA vs. Welch for now (Overall et al., 1995a, 1995b), but I could add them if we want.
    
\subsubsection{Type I Error Rates}
<<type1 setup, echo=FALSE>>=

##### Student's t test data
## Set up data
nullT.ve.ne <- reject.null.ve.ne[,1,1]
nullT.v2.ne <- reject.null.v2.ne[,1,1]
nullT.v5.ne <- reject.null.v5.ne[,1,1]
nullT.ve.1.5n <- reject.null.ve.1.5n[,1,1]
nullT.ve.2n <- reject.null.ve.2n[,1,1]
nullT.v2.1.5n.ssv <- reject.null.v2.1.5n.ssv[,1,1]
nullT.v2.1.5n.bsv <- reject.null.v2.1.5n.bsv[,1,1]
nullT.v2.2n.ssv <- reject.null.v2.2n.ssv[,1,1]
nullT.v2.2n.bsv <- reject.null.v2.2n.bsv[,1,1]
nullT.v5.1.5n.ssv <- reject.null.v5.1.5n.ssv[,1,1]
nullT.v5.1.5n.bsv <- reject.null.v5.1.5n.bsv[,1,1]
nullT.v5.2n.ssv <- reject.null.v5.2n.ssv[,1,1]
nullT.v5.2n.bsv <- reject.null.v5.2n.bsv[,1,1]

## Put it in a dataframe
type1.classic <- data.frame(
    type1.rate=c(nullT.ve.ne, nullT.v2.ne, nullT.v2.ne, nullT.v5.ne, nullT.v5.ne, nullT.ve.1.5n, nullT.ve.2n, nullT.v2.1.5n.ssv, nullT.v2.1.5n.bsv, nullT.v2.2n.ssv, nullT.v2.2n.bsv, nullT.v5.1.5n.ssv, nullT.v5.1.5n.bsv, nullT.v5.2n.ssv, nullT.v5.2n.bsv), #NOTE: doubled any with equal variance or equal ns because they don't have separate ssv and bsv versions
    Ns=factor(c('20,20','50,50','100,100','20,20','50,50','100,100','20,20','50,50','100,100','20,20','50,50','100,100','20,20','50,50','100,100','20,30','50,75','100,150','20,40','50,100','100,200','20,30','50,75','100,150','20,30','50,75','100,150','20,40','50,100','100,200','20,40','50,100','100,200','20,30','50,75','100,150','20,30','50,75','100,150','20,40','50,100','100,200','20,40','50,100','100,200'), levels=c('20,20','20,30','20,40','50,50','50,75','50,100','100,100','100,150','100,200')),
    var.ratio=factor(rep(c(0,-1,1,-2,2,0,0,-1,1, -1,1,-2,2,-2,2), each=3), levels=c(-2,-1,0,1,2), labels=c(.2,.5,1,2,5)))

## Plot it
T1classic <- ggplot(type1.classic, aes(x=as.numeric(var.ratio), y=type1.rate, shape=Ns, color=Ns, ymin=0, ymax=.13)) + 
    geom_point(size=4, solid=FALSE) + 
    scale_shape_manual(values=rep(c(1,2,0), 3)) +
    geom_line(size=.5) + 
    scale_color_manual(values=rep(c('deepskyblue4','darkgoldenrod1', 'darkgrey'), each=3)) + 
    scale_x_discrete(breaks=seq(-2:2), labels=c(.2,.5,1,2,5)) +
    scale_y_continuous(breaks=seq(from=0, to=.13, by=.01)) +
    xlab(bquote(frac(sigma[1]^2,sigma[2]^2))) + ylab('Type I Error Rate')


##### Welch's t test data
## Set up data
welch.nullT.ve.ne <- reject.null.ve.ne[,1,2]
welch.nullT.v2.ne <- reject.null.v2.ne[,1,2]
welch.nullT.v5.ne <- reject.null.v5.ne[,1,2]
welch.nullT.ve.1.5n <- reject.null.ve.1.5n[,1,2]
welch.nullT.ve.2n <- reject.null.ve.2n[,1,2]
welch.nullT.v2.1.5n.ssv <- reject.null.v2.1.5n.ssv[,1,2]
welch.nullT.v2.1.5n.bsv <- reject.null.v2.1.5n.bsv[,1,2]
welch.nullT.v2.2n.ssv <- reject.null.v2.2n.ssv[,1,2]
welch.nullT.v2.2n.bsv <- reject.null.v2.2n.bsv[,1,2]
welch.nullT.v5.1.5n.ssv <- reject.null.v5.1.5n.ssv[,1,2]
welch.nullT.v5.1.5n.bsv <- reject.null.v5.1.5n.bsv[,1,2]
welch.nullT.v5.2n.ssv <- reject.null.v5.2n.ssv[,1,2]
welch.nullT.v5.2n.bsv <- reject.null.v5.2n.bsv[,1,2]

## Put it in a dataframe
type1.welch <- data.frame(
    type1.rate=c(welch.nullT.ve.ne, welch.nullT.v2.ne, welch.nullT.v2.ne, welch.nullT.v5.ne, welch.nullT.v5.ne, welch.nullT.ve.1.5n, welch.nullT.ve.2n, welch.nullT.v2.1.5n.ssv, welch.nullT.v2.1.5n.bsv, welch.nullT.v2.2n.ssv, welch.nullT.v2.2n.bsv, welch.nullT.v5.1.5n.ssv, welch.nullT.v5.1.5n.bsv, welch.nullT.v5.2n.ssv, welch.nullT.v5.2n.bsv), #NOTE: doubled any with equal variance or equal ns because they don't have separate ssv and bsv versions
    Ns=factor(c('20,20','50,50','100,100','20,20','50,50','100,100','20,20','50,50','100,100','20,20','50,50','100,100','20,20','50,50','100,100','20,30','50,75','100,150','20,40','50,100','100,200','20,30','50,75','100,150','20,30','50,75','100,150','20,40','50,100','100,200','20,40','50,100','100,200','20,30','50,75','100,150','20,30','50,75','100,150','20,40','50,100','100,200','20,40','50,100','100,200'), levels=c('20,20','20,30','20,40','50,50','50,75','50,100','100,100','100,150','100,200')),                                                                                                                                                                                                                                                                                                                
    var.ratio=factor(rep(c(0,-1,1,-2,2,0,0,-1,1, -1,1,-2,2,-2,2), each=3), levels=c(-2,-1,0,1,2), labels=c(.2,.5,1,2,5)))
type1.welch <- type1.welch[order(type1.welch$Ns, type1.welch$var.ratio),]

## Plot it
T1welch <- ggplot(type1.welch, aes(x=as.numeric(var.ratio), y=type1.rate, shape=Ns, color=Ns, ymin=0, ymax=.13)) + 
    geom_point(size=4, solid=FALSE) + 
    scale_shape_manual(values=rep(c(1,2,0), 3)) +
    geom_line(size=.5) + 
    scale_color_manual(values=rep(c('deepskyblue4','darkgoldenrod1', 'darkgrey'), each=3)) + 
    scale_x_discrete(breaks=seq(-2:2), labels=c(.2,.5,1,2,5)) +
    scale_y_continuous(breaks=seq(from=0, to=.13, by=.01)) +
    xlab(bquote(frac(sigma[1]^2,sigma[2]^2))) + ylab('Type I Error Rate')

@

    The expected type I error rate is $\alpha = .05$. The observed type I error rate for Student's t test remained close to the expected .05 rate when either the sample size or the population variances were equal, but it varied widely when both population variances and sample sizes were unequal (see Figure 7). When the group with the larger sample size had the larger variance (the left side of Figure 7), the type I error rate dropped as low as about .01, but when the group with the larger sample size had the smaller variance (the right side of Figure 7), the type I error rate rose as high as .12, which is more than double the normally accepted false positive rate. In contrast, the observed type I error rate for Welch's t test remained close to the expected .05 rate across all conditions (see Figure 8). Overall, the Welch t test consistently behaved as expected when it comes to false positives. Student's t test did not.

\begin{figure}    
<<type1 classic plot, echo=FALSE>>=
T1classic
@
\textit{Figure 7.} Type I error rates for Student's t test.
\end{figure}


\begin{figure}
<<type1 Welch plot, echo=FALSE>>=
T1welch
@
\textit{Figure 8.} Type I error rates for Welch's t test.
\end{figure}

\subsubsection{Power}
<<PowerSetup, echo=FALSE>>=

## Common variables

power.Ns <- factor(c('20,20','50,50','100,100','20,20','50,50','100,100','20,20','50,50','100,100','20,20','50,50','100,100','20,20','50,50','100,100','20,30','50,75','100,150','20,40','50,100','100,200','20,30','50,75','100,150','20,30','50,75','100,150','20,40','50,100','100,200','20,40','50,100','100,200','20,30','50,75','100,150','20,30','50,75','100,150','20,40','50,100','100,200','20,40','50,100','100,200'), levels=c('20,20','20,30','20,40','50,50','50,75','50,100','100,100','100,150','100,200'))

power.var.ratio <- factor(rep(c(0,-1,1,-2,2,0,0,-1,1,-1,1,-2,2,-2,2), each=3), levels=c(-2,-1,0,1,2), labels=c(.2,.5,1,2,5))

## Set up data for observed power

# d=.2
smalld.ve.ne <- reject.null.ve.ne[,2,1]
smalld.v2.ne <- reject.null.v2.ne[,2,1]
smalld.v5.ne <- reject.null.v5.ne[,2,1]
smalld.ve.1.5n <- reject.null.ve.1.5n[,2,1]
smalld.ve.2n <- reject.null.ve.2n[,2,1]
smalld.v2.1.5n.ssv <- reject.null.v2.1.5n.ssv[,2,1]
smalld.v2.1.5n.bsv <- reject.null.v2.1.5n.bsv[,2,1]
smalld.v2.2n.ssv <- reject.null.v2.2n.ssv[,2,1]
smalld.v2.2n.bsv <- reject.null.v2.2n.bsv[,2,1]
smalld.v5.1.5n.ssv <- reject.null.v5.1.5n.ssv[,2,1]
smalld.v5.1.5n.bsv <- reject.null.v5.1.5n.bsv[,2,1]
smalld.v5.2n.ssv <- reject.null.v5.2n.ssv[,2,1]
smalld.v5.2n.bsv <- reject.null.v5.2n.bsv[,2,1]
power.smalld.student <- c(smalld.ve.ne, smalld.v2.ne, smalld.v2.ne, smalld.v5.ne, smalld.v5.ne, smalld.ve.1.5n, smalld.ve.2n, smalld.v2.1.5n.ssv, smalld.v2.1.5n.bsv, smalld.v2.2n.ssv, smalld.v2.2n.bsv, smalld.v5.1.5n.ssv, smalld.v5.1.5n.bsv, smalld.v5.2n.ssv, smalld.v5.2n.bsv) #NOTE: doubled any with equal variance or equal ns because they don't have separate ssv and bsv versions

smalld.student <- data.frame(power.smalld.student, power.Ns, power.var.ratio)
smalld.student <- smalld.student[order(smalld.student$power.Ns, smalld.student$power.var.ratio),]


# d=.5
midd.ve.ne <- reject.null.ve.ne[,3,1]
midd.v2.ne <- reject.null.v2.ne[,3,1]
midd.v5.ne <- reject.null.v5.ne[,3,1]
midd.ve.1.5n <- reject.null.ve.1.5n[,3,1]
midd.ve.2n <- reject.null.ve.2n[,3,1]
midd.v2.1.5n.ssv <- reject.null.v2.1.5n.ssv[,3,1]
midd.v2.1.5n.bsv <- reject.null.v2.1.5n.bsv[,3,1]
midd.v2.2n.ssv <- reject.null.v2.2n.ssv[,3,1]
midd.v2.2n.bsv <- reject.null.v2.2n.bsv[,3,1]
midd.v5.1.5n.ssv <- reject.null.v5.1.5n.ssv[,3,1]
midd.v5.1.5n.bsv <- reject.null.v5.1.5n.bsv[,3,1]
midd.v5.2n.ssv <- reject.null.v5.2n.ssv[,3,1]
midd.v5.2n.bsv <- reject.null.v5.2n.bsv[,3,1]
power.midd.student <- c(midd.ve.ne, midd.v2.ne, midd.v2.ne, midd.v5.ne, midd.v5.ne, midd.ve.1.5n, midd.ve.2n, midd.v2.1.5n.ssv, midd.v2.1.5n.bsv, midd.v2.2n.ssv, midd.v2.2n.bsv, midd.v5.1.5n.ssv, midd.v5.1.5n.bsv, midd.v5.2n.ssv, midd.v5.2n.bsv) #NOTE: doubled any with equal variance or equal ns because they don't have separate ssv and bsv versions

midd.student <- data.frame(power.midd.student, power.Ns, power.var.ratio)                                                                                                                                                                                                                                                                                                                
midd.student <- midd.student[order(midd.student$power.Ns, midd.student$power.var.ratio),]

# d=.8
bigd.ve.ne <- reject.null.ve.ne[,4,1]
bigd.v2.ne <- reject.null.v2.ne[,4,1]
bigd.v5.ne <- reject.null.v5.ne[,4,1]
bigd.ve.1.5n <- reject.null.ve.1.5n[,4,1]
bigd.ve.2n <- reject.null.ve.2n[,4,1]
bigd.v2.1.5n.ssv <- reject.null.v2.1.5n.ssv[,4,1]
bigd.v2.1.5n.bsv <- reject.null.v2.1.5n.bsv[,4,1]
bigd.v2.2n.ssv <- reject.null.v2.2n.ssv[,4,1]
bigd.v2.2n.bsv <- reject.null.v2.2n.bsv[,4,1]
bigd.v5.1.5n.ssv <- reject.null.v5.1.5n.ssv[,4,1]
bigd.v5.1.5n.bsv <- reject.null.v5.1.5n.bsv[,4,1]
bigd.v5.2n.ssv <- reject.null.v5.2n.ssv[,4,1]
bigd.v5.2n.bsv <- reject.null.v5.2n.bsv[,4,1]
power.bigd.student <- c(bigd.ve.ne, bigd.v2.ne, bigd.v2.ne, bigd.v5.ne, bigd.v5.ne, bigd.ve.1.5n, bigd.ve.2n, bigd.v2.1.5n.ssv, bigd.v2.1.5n.bsv, bigd.v2.2n.ssv, bigd.v2.2n.bsv, bigd.v5.1.5n.ssv, bigd.v5.1.5n.bsv, bigd.v5.2n.ssv, bigd.v5.2n.bsv) #NOTE: doubled any with equal variance or equal ns because they don't have separate ssv and bsv versions

bigd.student <- data.frame(power.bigd.student, power.Ns, power.var.ratio)
bigd.student <- bigd.student[order(bigd.student$power.Ns, bigd.student$power.var.ratio),]

## Student t, d=.2, power
## Plot using ggplot2
smalld.student.plot <- ggplot(smalld.student, aes(x=as.numeric(power.var.ratio), y=power.smalld.student, shape=power.Ns, color=power.Ns, ymin=0, ymax=1)) + 
    geom_point(size=3, solid=FALSE) + 
    scale_shape_manual(values=rep(c(1,2,0), 3)) +
    geom_line(size=.5) + 
    scale_color_manual(values=rep(c('deepskyblue4','darkgoldenrod1', 'darkgrey'), each=3)) + 
    scale_x_discrete(breaks=seq(-2:2), labels=c(.2,.5,1,2,5)) +
    scale_y_continuous(breaks=seq(from=0, to=1, by=.1)) +
    xlab(bquote(sigma[1]^2~'/'~sigma[2]^2)) + ylab('Small Effect') + ggtitle("Student's t Test")

## Student t, d=.5, power
## Set up Student t test plot of deviation from expected power
midd.student.plot <- ggplot(midd.student, aes(x=as.numeric(power.var.ratio), y=power.midd.student, shape=power.Ns, color=power.Ns, ymin=0, ymax=1)) + 
    geom_point(size=3, solid=FALSE) + 
    scale_shape_manual(values=rep(c(1,2,0), 3)) +
    geom_line(size=.5) + 
    scale_color_manual(values=rep(c('deepskyblue4','darkgoldenrod1', 'darkgrey'), each=3)) + 
    scale_x_discrete(breaks=seq(-2:2), labels=c(.2,.5,1,2,5)) +
    scale_y_continuous(breaks=seq(from=0, to=1, by=.1)) +
    xlab(bquote(sigma[1]^2~'/'~sigma[2]^2)) + ylab('Medium Effect') #+ ggtitle("Student's t Test")

## Student t, d=.8, power
bigd.student.plot <- ggplot(bigd.student, aes(x=as.numeric(power.var.ratio), y=power.bigd.student, shape=power.Ns, color=power.Ns, ymin=0, ymax=1)) + 
    geom_point(size=3, solid=FALSE) + 
    scale_shape_manual(values=rep(c(1,2,0), 3)) +
    geom_line(size=.5) + 
    scale_color_manual(values=rep(c('deepskyblue4','darkgoldenrod1', 'darkgrey'), each=3)) + 
    scale_x_discrete(breaks=seq(-2:2), labels=c(.2,.5,1,2,5)) +
    scale_y_continuous(breaks=seq(from=0, to=1, by=.1)) +
    xlab(bquote(sigma[1]^2~'/'~sigma[2]^2)) + ylab('Large Effect') #+ ggtitle("Student's t Test")

@

<<welch power setup, echo=FALSE>>=

## Set up data for observed power

# d=.2
smalld.ve.ne <- reject.null.ve.ne[,2,2]
smalld.v2.ne <- reject.null.v2.ne[,2,2]
smalld.v5.ne <- reject.null.v5.ne[,2,2]
smalld.ve.1.5n <- reject.null.ve.1.5n[,2,2]
smalld.ve.2n <- reject.null.ve.2n[,2,2]
smalld.v2.1.5n.ssv <- reject.null.v2.1.5n.ssv[,2,2]
smalld.v2.1.5n.bsv <- reject.null.v2.1.5n.bsv[,2,2]
smalld.v2.2n.ssv <- reject.null.v2.2n.ssv[,2,2]
smalld.v2.2n.bsv <- reject.null.v2.2n.bsv[,2,2]
smalld.v5.1.5n.ssv <- reject.null.v5.1.5n.ssv[,2,2]
smalld.v5.1.5n.bsv <- reject.null.v5.1.5n.bsv[,2,2]
smalld.v5.2n.ssv <- reject.null.v5.2n.ssv[,2,2]
smalld.v5.2n.bsv <- reject.null.v5.2n.bsv[,2,2]
power.smalld.welch <- c(smalld.ve.ne, smalld.v2.ne, smalld.v2.ne, smalld.v5.ne, smalld.v5.ne, smalld.ve.1.5n, smalld.ve.2n, smalld.v2.1.5n.ssv, smalld.v2.1.5n.bsv, smalld.v2.2n.ssv, smalld.v2.2n.bsv, smalld.v5.1.5n.ssv, smalld.v5.1.5n.bsv, smalld.v5.2n.ssv, smalld.v5.2n.bsv) #NOTE: doubled any with equal variance or equal ns because they don't have separate ssv and bsv versions

smalld.welch <- data.frame(power.smalld.welch, power.Ns, power.var.ratio)
smalld.welch <- smalld.welch[order(smalld.welch$power.Ns, smalld.welch$power.var.ratio),]

# d=.5
midd.ve.ne <- reject.null.ve.ne[,3,2]
midd.v2.ne <- reject.null.v2.ne[,3,2]
midd.v5.ne <- reject.null.v5.ne[,3,2]
midd.ve.1.5n <- reject.null.ve.1.5n[,3,2]
midd.ve.2n <- reject.null.ve.2n[,3,2]
midd.v2.1.5n.ssv <- reject.null.v2.1.5n.ssv[,3,2]
midd.v2.1.5n.bsv <- reject.null.v2.1.5n.bsv[,3,2]
midd.v2.2n.ssv <- reject.null.v2.2n.ssv[,3,2]
midd.v2.2n.bsv <- reject.null.v2.2n.bsv[,3,2]
midd.v5.1.5n.ssv <- reject.null.v5.1.5n.ssv[,3,2]
midd.v5.1.5n.bsv <- reject.null.v5.1.5n.bsv[,3,2]
midd.v5.2n.ssv <- reject.null.v5.2n.ssv[,3,2]
midd.v5.2n.bsv <- reject.null.v5.2n.bsv[,3,2]
power.midd.welch <- c(midd.ve.ne, midd.v2.ne, midd.v2.ne, midd.v5.ne, midd.v5.ne, midd.ve.1.5n, midd.ve.2n, midd.v2.1.5n.ssv, midd.v2.1.5n.bsv, midd.v2.2n.ssv, midd.v2.2n.bsv, midd.v5.1.5n.ssv, midd.v5.1.5n.bsv, midd.v5.2n.ssv, midd.v5.2n.bsv) #NOTE: doubled any with equal variance or equal ns because they don't have separate ssv and bsv versions

midd.welch <- data.frame(power.midd.welch, power.Ns, power.var.ratio)
midd.welch <- midd.welch[order(midd.welch$power.Ns, midd.welch$power.var.ratio),]

# d=.8
bigd.ve.ne <- reject.null.ve.ne[,4,2]
bigd.v2.ne <- reject.null.v2.ne[,4,2]
bigd.v5.ne <- reject.null.v5.ne[,4,2]
bigd.ve.1.5n <- reject.null.ve.1.5n[,4,2]
bigd.ve.2n <- reject.null.ve.2n[,4,2]
bigd.v2.1.5n.ssv <- reject.null.v2.1.5n.ssv[,4,2]
bigd.v2.1.5n.bsv <- reject.null.v2.1.5n.bsv[,4,2]
bigd.v2.2n.ssv <- reject.null.v2.2n.ssv[,4,2]
bigd.v2.2n.bsv <- reject.null.v2.2n.bsv[,4,2]
bigd.v5.1.5n.ssv <- reject.null.v5.1.5n.ssv[,4,2]
bigd.v5.1.5n.bsv <- reject.null.v5.1.5n.bsv[,4,2]
bigd.v5.2n.ssv <- reject.null.v5.2n.ssv[,4,2]
bigd.v5.2n.bsv <- reject.null.v5.2n.bsv[,4,2]
power.bigd.welch <- c(bigd.ve.ne, bigd.v2.ne, bigd.v2.ne, bigd.v5.ne, bigd.v5.ne, bigd.ve.1.5n, bigd.ve.2n, bigd.v2.1.5n.ssv, bigd.v2.1.5n.bsv, bigd.v2.2n.ssv, bigd.v2.2n.bsv, bigd.v5.1.5n.ssv, bigd.v5.1.5n.bsv, bigd.v5.2n.ssv, bigd.v5.2n.bsv) #NOTE: doubled any with equal variance or equal ns because they don't have separate ssv and bsv versions

bigd.welch <- data.frame(power.bigd.welch, power.Ns, power.var.ratio)
bigd.welch <- bigd.welch[order(bigd.welch$power.Ns, bigd.welch$power.var.ratio),]


## Welch t, d=.2, power
smalld.welch.plot <- ggplot(smalld.welch, aes(x=as.numeric(power.var.ratio), y=power.smalld.welch, shape=power.Ns, color=power.Ns, ymin=0, ymax=1)) + 
    geom_point(size=3, solid=FALSE) + 
    scale_shape_manual(values=rep(c(1,2,0), 3)) +
    geom_line(size=.5) + 
    scale_color_manual(values=rep(c('deepskyblue4','darkgoldenrod1', 'darkgrey'), each=3)) + 
    scale_x_discrete(breaks=seq(-2:2), labels=c(.2,.5,1,2,5)) +
    scale_y_continuous(breaks=seq(from=0, to=1, by=.1)) +
    xlab(bquote(sigma[1]^2~'/'~sigma[2]^2)) + ylab('Small Effect') + ggtitle("Welch's t Test")


## Welch t, d=.5, power
midd.welch.plot <- ggplot(midd.welch, aes(x=as.numeric(power.var.ratio), y=power.midd.welch, shape=power.Ns, color=power.Ns, ymin=0, ymax=1)) + 
    geom_point(size=3, solid=FALSE) + 
    scale_shape_manual(values=rep(c(1,2,0), 3)) +
    geom_line(size=.5) + 
    scale_color_manual(values=rep(c('deepskyblue4','darkgoldenrod1', 'darkgrey'), each=3)) + 
    scale_x_discrete(breaks=seq(-2:2), labels=c(.2,.5,1,2,5)) +
    scale_y_continuous(breaks=seq(from=0, to=1, by=.1)) +
    xlab(bquote(sigma[1]^2~'/'~sigma[2]^2)) + ylab('Medium Effect') #+ ggtitle("Welch's t Test")


## Welch t, d=.8, power
bigd.welch.plot <- ggplot(bigd.welch, aes(x=as.numeric(power.var.ratio), y=power.bigd.welch, shape=power.Ns, color=power.Ns, ymin=0, ymax=1)) + 
    geom_point(size=3, solid=FALSE) + 
    scale_shape_manual(values=rep(c(1,2,0), 3)) +
    geom_line(size=.5) + 
    scale_color_manual(values=rep(c('deepskyblue4','darkgoldenrod1', 'darkgrey'), each=3)) + 
    scale_x_discrete(breaks=seq(-2:2), labels=c(.2,.5,1,2,5)) +
    scale_y_continuous(breaks=seq(from=0, to=1, by=.1)) +
    xlab(bquote(sigma[1]^2~'/'~sigma[2]^2)) + ylab('Large Effect') #+ ggtitle("Welch's t Test")


@

<<diffPowerSetup, echo=FALSE>>=

power.smalld.diff <- power.smalld.student-power.smalld.welch
smalld.diff <- data.frame(power.smalld.diff, power.Ns, power.var.ratio)
smalld.diff <- smalld.diff[order(smalld.diff$power.Ns, smalld.diff$power.var.ratio),]

power.midd.diff <- power.midd.student-power.midd.welch
midd.diff <- data.frame(power.midd.diff, power.Ns, power.var.ratio)
midd.diff <- midd.diff[order(midd.diff$power.Ns, midd.diff$power.var.ratio),]

power.bigd.diff <- power.bigd.student-power.bigd.welch
bigd.diff <- data.frame(power.bigd.diff, power.Ns, power.var.ratio)
bigd.diff <- bigd.diff[order(bigd.diff$power.Ns, bigd.diff$power.var.ratio),]

smalld.diff.plot <- ggplot(smalld.diff, aes(x=as.numeric(power.var.ratio), y=power.smalld.diff, shape=power.Ns, color=power.Ns, ymin=-.3, ymax=.3)) + 
    geom_point(size=3, solid=FALSE) + 
    scale_shape_manual(values=rep(c(1,2,0), 3)) +
    geom_line(size=.5) + 
    scale_color_manual(values=rep(c('deepskyblue4','darkgoldenrod1', 'darkgrey'), each=3)) + 
    scale_x_discrete(breaks=seq(-2:2), labels=c(.2,.5,1,2,5)) +
    scale_y_continuous(breaks=round(seq(from=-.3, to=.3, by=.1), digits=2)) +
    xlab(bquote(sigma[1]^2~'/'~sigma[2]^2)) + ylab('Small Effect') + ggtitle(NULL)

midd.diff.plot <- ggplot(midd.diff, aes(x=as.numeric(power.var.ratio), y=power.midd.diff, shape=power.Ns, color=power.Ns, ymin=-.3, ymax=.3)) + 
    geom_point(size=3, solid=FALSE) + 
    scale_shape_manual(values=rep(c(1,2,0), 3)) +
    geom_line(size=.5) + 
    scale_color_manual(values=rep(c('deepskyblue4','darkgoldenrod1', 'darkgrey'), each=3)) + 
    scale_x_discrete(breaks=seq(-2:2), labels=c(.2,.5,1,2,5)) +
    scale_y_continuous(breaks=round(seq(from=-.3, to=.3, by=.1), digits=2)) +
    xlab(bquote(sigma[1]^2~'/'~sigma[2]^2)) + ylab('Medium Effect') + ggtitle(NULL)

bigd.diff.plot <- ggplot(bigd.diff, aes(x=as.numeric(power.var.ratio), y=power.bigd.diff, shape=power.Ns, color=power.Ns, ymin=-.3, ymax=.3)) + 
    geom_point(size=3, solid=FALSE) + 
    scale_shape_manual(values=rep(c(1,2,0), 3)) +
    geom_line(size=.5) + 
    scale_color_manual(values=rep(c('deepskyblue4','darkgoldenrod1', 'darkgrey'), each=3)) + 
    scale_x_discrete(breaks=seq(-2:2), labels=c(.2,.5,1,2,5)) +
    scale_y_continuous(breaks=round(seq(from=-.3, to=.3, by=.1), digits=2)) +
    xlab(bquote(sigma[1]^2~'/'~sigma[2]^2)) + ylab('Big Effect') + ggtitle(NULL)

@

    Is Welch's t test underpowered compared to Student's t test? Figure 9 displays the power of Student's t test and of Welch t tests to detect small, medium, and large effects under the different conditions. For both tests, power decreases as variances become unequal because we increased one group's variance; however, the power of each test decreases at a different rate depending on the sample size ratio. Figure 10 displays the difference in power between Student's t test and Welch's t test, with higher numbers indicating that Student's t test is more powerful. When the sample sizes or variances are equal, the power of the two tests is approximately equal. However, when both the sample sizes and variances are unequal, there are differences in power. Overall, Student's t test is more powerful when the large sample has the smaller variance, whereas the Welch t test is more powerful when the small sample has the smaller variance. These differences are the most dramatic when one sample is twice the size of the other. 
    
    The conditions in which Student's t test has the greatest power over Welch's t test, when one sample is twice the size of the other and the large sample has the small variance, are the same conditions in which Student's t test has a risk of doubling the false positive rate. In contrast, Welch's t test was more powerful than Student's t test under other conditions and never inflated the type I error rate beyond the expected rate.

    Taken together, the type I error rates and power favor the Welch t test over Student's t test as better balancing researchers' concerns.
\begin{figure}
<<plotPower, echo=FALSE>>=

layout <- matrix(c(1,2,3,4,5,6), nrow=3, byrow=TRUE)
multiplot(smalld.student.plot, smalld.welch.plot, midd.student.plot, midd.welch.plot, bigd.student.plot, bigd.welch.plot, layout=layout)

#multiplot(smalld.classic.power, smalld.classic.deviation, smalld.welch.power, smalld.welch.deviation, cols=2)

#multiplot(midd.classic.power, midd.classic.deviation, midd.welch.power, midd.welch.deviation, cols=2)

#multiplot(bigd.classic.power, bigd.classic.deviation, bigd.welch.power, bigd.welch.deviation, cols=2)

@
\textit{Figure 9.} Power of Student's and Welch's t tests.
\end{figure}

\begin{figure}
<<plotPowerDiff, echo=FALSE>>=

layout <- matrix(c(0,1,1,0,0,2,2,0,0,3,3,0), nrow=3, byrow=TRUE)
multiplot(smalld.diff.plot, midd.diff.plot, bigd.diff.plot, layout=layout)

@
\textit{Figure 10.} Difference in the Power of Student's and Welch's t tests (Student-Welch).
\end{figure}

\subsubsection{Coverage Probability}
We examined the coverage probability of 95\% confidence intervals, which is the proportion of confidence intervals that contain the true population value of the difference in group means, based on Student's t test and Welch's t test. Because the accuracy of a confidence interval is influenced by the variance and sample size, but not by the true effect size, we only show the coverage probability when the null hypothesis is true (the coverage probabilities are identical across all effect sizes). Additionally, when the null hypothesis is true and $\alpha = .05$, the coverage probability of 95\% confidence intervals has a simple relationship with the type I error rate--the coverage probability is the complement of the type I error rate. Table 10 displays the coverage probabilities of the two t tests. The coverage probability for Student's t test varies dramatically, just as the type I error rates did. When Student's test is the least powerful, it is the most accurate at estimating the difference in means. When it is the most powerful to find an effect, it is the least accurate, and what you would believe is a 95\% confidence interval drops as low as an 88\% confidence interval in reality. In contrast, the Welch t test performs as expected and the confidence interval contains the true effect 95\% of the time regardless of the variance and sample size ratios. 

<<classic coverage, echo=FALSE>>=

##### Plot classic results only

## Set up data
nullT.ve.ne <- obs.coverage.ve.ne[,1,1]
nullT.v2.ne <- obs.coverage.v2.ne[,1,1]
nullT.v5.ne <- obs.coverage.v5.ne[,1,1]
nullT.ve.1.5n <- obs.coverage.ve.1.5n[,1,1]
nullT.ve.2n <- obs.coverage.ve.2n[,1,1]
nullT.v2.1.5n.ssv <- obs.coverage.v2.1.5n.ssv[,1,1]
nullT.v2.1.5n.bsv <- obs.coverage.v2.1.5n.bsv[,1,1]
nullT.v2.2n.ssv <- obs.coverage.v2.2n.ssv[,1,1]
nullT.v2.2n.bsv <- obs.coverage.v2.2n.bsv[,1,1]
nullT.v5.1.5n.ssv <- obs.coverage.v5.1.5n.ssv[,1,1]
nullT.v5.1.5n.bsv <- obs.coverage.v5.1.5n.bsv[,1,1]
nullT.v5.2n.ssv <- obs.coverage.v5.2n.ssv[,1,1]
nullT.v5.2n.bsv <- obs.coverage.v5.2n.bsv[,1,1]
smalld.ve.ne <- obs.coverage.ve.ne[,2,1]
smalld.v2.ne <- obs.coverage.v2.ne[,2,1]
smalld.v5.ne <- obs.coverage.v5.ne[,2,1]
smalld.ve.1.5n <- obs.coverage.ve.1.5n[,2,1]
smalld.ve.2n <- obs.coverage.ve.2n[,2,1]
smalld.v2.1.5n.ssv <- obs.coverage.v2.1.5n.ssv[,2,1]
smalld.v2.1.5n.bsv <- obs.coverage.v2.1.5n.bsv[,2,1]
smalld.v2.2n.ssv <- obs.coverage.v2.2n.ssv[,2,1]
smalld.v2.2n.bsv <- obs.coverage.v2.2n.bsv[,2,1]
smalld.v5.1.5n.ssv <- obs.coverage.v5.1.5n.ssv[,2,1]
smalld.v5.1.5n.bsv <- obs.coverage.v5.1.5n.bsv[,2,1]
smalld.v5.2n.ssv <- obs.coverage.v5.2n.ssv[,2,1]
smalld.v5.2n.bsv <- obs.coverage.v5.2n.bsv[,2,1]
midd.ve.ne <- obs.coverage.ve.ne[,3,1]
midd.v2.ne <- obs.coverage.v2.ne[,3,1]
midd.v5.ne <- obs.coverage.v5.ne[,3,1]
midd.ve.1.5n <- obs.coverage.ve.1.5n[,3,1]
midd.ve.2n <- obs.coverage.ve.2n[,3,1]
midd.v2.1.5n.ssv <- obs.coverage.v2.1.5n.ssv[,3,1]
midd.v2.1.5n.bsv <- obs.coverage.v2.1.5n.bsv[,3,1]
midd.v2.2n.ssv <- obs.coverage.v2.2n.ssv[,3,1]
midd.v2.2n.bsv <- obs.coverage.v2.2n.bsv[,3,1]
midd.v5.1.5n.ssv <- obs.coverage.v5.1.5n.ssv[,3,1]
midd.v5.1.5n.bsv <- obs.coverage.v5.1.5n.bsv[,3,1]
midd.v5.2n.ssv <- obs.coverage.v5.2n.ssv[,3,1]
midd.v5.2n.bsv <- obs.coverage.v5.2n.bsv[,3,1]
bigd.ve.ne <- obs.coverage.ve.ne[,4,1]
bigd.v2.ne <- obs.coverage.v2.ne[,4,1]
bigd.v5.ne <- obs.coverage.v5.ne[,4,1]
bigd.ve.1.5n <- obs.coverage.ve.1.5n[,4,1]
bigd.ve.2n <- obs.coverage.ve.2n[,4,1]
bigd.v2.1.5n.ssv <- obs.coverage.v2.1.5n.ssv[,4,1]
bigd.v2.1.5n.bsv <- obs.coverage.v2.1.5n.bsv[,4,1]
bigd.v2.2n.ssv <- obs.coverage.v2.2n.ssv[,4,1]
bigd.v2.2n.bsv <- obs.coverage.v2.2n.bsv[,4,1]
bigd.v5.1.5n.ssv <- obs.coverage.v5.1.5n.ssv[,4,1]
bigd.v5.1.5n.bsv <- obs.coverage.v5.1.5n.bsv[,4,1]
bigd.v5.2n.ssv <- obs.coverage.v5.2n.ssv[,4,1]
bigd.v5.2n.bsv <- obs.coverage.v5.2n.bsv[,4,1]

## Put it in a dataframe
coverage.classic.nullT <- data.frame(
    coverage.rate=c(nullT.ve.ne, nullT.v2.ne, nullT.v2.ne, nullT.v5.ne, nullT.v5.ne, nullT.ve.1.5n, nullT.ve.2n, nullT.v2.1.5n.ssv, nullT.v2.1.5n.bsv, nullT.v2.2n.ssv, nullT.v2.2n.bsv, nullT.v5.1.5n.ssv, nullT.v5.1.5n.bsv, nullT.v5.2n.ssv, nullT.v5.2n.bsv), #NOTE: doubled any with equal variance or equal ns because they don't have separate ssv and bsv versions
    Ns=factor(c('20,20','50,50','100,100','20,20','50,50','100,100','20,20','50,50','100,100','20,20','50,50','100,100','20,20','50,50','100,100','20,30','50,75','100,150','20,40','50,100','100,200','20,30','50,75','100,150','20,30','50,75','100,150','20,40','50,100','100,200','20,40','50,100','100,200','20,30','50,75','100,150','20,30','50,75','100,150','20,40','50,100','100,200','20,40','50,100','100,200'), levels=c('20,20','20,30','20,40','50,50','50,75','50,100','100,100','100,150','100,200')),                                                                                                                                                                                                                                                                                                                
    var.ratio=factor(rep(c(0,-1,1,-2,2,0,0,-1,1,-1,1,-2,2,-2,2), each=3), levels=c(-2,-1,0,1,2), labels=c(.2,.5,1,2,5)))
coverage.classic.nullT <- coverage.classic.nullT[order(coverage.classic.nullT$Ns, coverage.classic.nullT$var.ratio),]

coverage.classic.smalld <- data.frame(
    coverage.rate=c(smalld.ve.ne, smalld.v2.ne, smalld.v2.ne, smalld.v5.ne, smalld.v5.ne, smalld.ve.1.5n, smalld.ve.2n, smalld.v2.1.5n.ssv, smalld.v2.1.5n.bsv, smalld.v2.2n.ssv, smalld.v2.2n.bsv, smalld.v5.1.5n.ssv, smalld.v5.1.5n.bsv, smalld.v5.2n.ssv, smalld.v5.2n.bsv), #NOTE: doubled any with equal variance or equal ns because they don't have separate ssv and bsv versions
    Ns=factor(c('20,20','50,50','100,100','20,20','50,50','100,100','20,20','50,50','100,100','20,20','50,50','100,100','20,20','50,50','100,100','20,30','50,75','100,150','20,40','50,100','100,200','20,30','50,75','100,150','20,30','50,75','100,150','20,40','50,100','100,200','20,40','50,100','100,200','20,30','50,75','100,150','20,30','50,75','100,150','20,40','50,100','100,200','20,40','50,100','100,200'), levels=c('20,20','20,30','20,40','50,50','50,75','50,100','100,100','100,150','100,200')),                                                                                                                                                                                                                                                                                                                
    var.ratio=factor(rep(c(0,-1,1,-2,2,0,0,-1,1,-1,1,-2,2,-2,2), each=3), levels=c(-2,-1,0,1,2), labels=c(.2,.5,1,2,5)))
coverage.classic.smalld <- coverage.classic.smalld[order(coverage.classic.smalld$Ns, coverage.classic.smalld$var.ratio),]

coverage.classic.midd <- data.frame(
    coverage.rate=c(midd.ve.ne, midd.v2.ne, midd.v2.ne, midd.v5.ne, midd.v5.ne, midd.ve.1.5n, midd.ve.2n, midd.v2.1.5n.ssv, midd.v2.1.5n.bsv, midd.v2.2n.ssv, midd.v2.2n.bsv, midd.v5.1.5n.ssv, midd.v5.1.5n.bsv, midd.v5.2n.ssv, midd.v5.2n.bsv), #NOTE: doubled any with equal variance or equal ns because they don't have separate ssv and bsv versions
    Ns=factor(c('20,20','50,50','100,100','20,20','50,50','100,100','20,20','50,50','100,100','20,20','50,50','100,100','20,20','50,50','100,100','20,30','50,75','100,150','20,40','50,100','100,200','20,30','50,75','100,150','20,30','50,75','100,150','20,40','50,100','100,200','20,40','50,100','100,200','20,30','50,75','100,150','20,30','50,75','100,150','20,40','50,100','100,200','20,40','50,100','100,200'), levels=c('20,20','20,30','20,40','50,50','50,75','50,100','100,100','100,150','100,200')),                                                                                                                                                                                                                                                                                                                
    var.ratio=factor(rep(c(0,-1,1,-2,2,0,0,-1,1,-1,1,-2,2,-2,2), each=3), levels=c(-2,-1,0,1,2), labels=c(.2,.5,1,2,5)))
coverage.classic.midd <- coverage.classic.midd[order(coverage.classic.midd$Ns, coverage.classic.midd$var.ratio),]

coverage.classic.bigd <- data.frame(
    coverage.rate=c(bigd.ve.ne, bigd.v2.ne, bigd.v2.ne, bigd.v5.ne, bigd.v5.ne, bigd.ve.1.5n, bigd.ve.2n, bigd.v2.1.5n.ssv, bigd.v2.1.5n.bsv, bigd.v2.2n.ssv, bigd.v2.2n.bsv, bigd.v5.1.5n.ssv, bigd.v5.1.5n.bsv, bigd.v5.2n.ssv, bigd.v5.2n.bsv), #NOTE: doubled any with equal variance or equal ns because they don't have separate ssv and bsv versions
    Ns=factor(c('20,20','50,50','100,100','20,20','50,50','100,100','20,20','50,50','100,100','20,20','50,50','100,100','20,20','50,50','100,100','20,30','50,75','100,150','20,40','50,100','100,200','20,30','50,75','100,150','20,30','50,75','100,150','20,40','50,100','100,200','20,40','50,100','100,200','20,30','50,75','100,150','20,30','50,75','100,150','20,40','50,100','100,200','20,40','50,100','100,200'), levels=c('20,20','20,30','20,40','50,50','50,75','50,100','100,100','100,150','100,200')),                                                                                                                                                                                                                                                                                                                
    var.ratio=factor(rep(c(0,-1,1,-2,2,0,0,-1,1,-1,1,-2,2,-2,2), each=3), levels=c(-2,-1,0,1,2), labels=c(.2,.5,1,2,5)))
coverage.classic.bigd <- coverage.classic.bigd[order(coverage.classic.bigd$Ns, coverage.classic.bigd$var.ratio),]


## Set up classic t test plot
CovClassic.nullT <- ggplot(coverage.classic.nullT, aes(x=as.numeric(var.ratio), y=coverage.rate, shape=Ns, color=Ns, ymin=.85, ymax=1)) + 
    geom_point(size=3, solid=FALSE) + 
    scale_shape_manual(values=rep(c(1,2,0), 3)) +
    geom_line(size=.5) + 
    scale_color_manual(values=rep(c('deepskyblue4','darkgoldenrod1', 'darkgrey'), each=3)) + 
    scale_x_discrete(breaks=seq(-2:2), labels=c(.2,.5,1,2,5)) +
    scale_y_continuous(breaks=seq(from=.85, to=1, length.out=16)) +
    xlab(bquote('s'[1]^2~'/'~'s'[2]^2)) + ylab('Coverage Probability') + ggtitle("Student's t Test")

CovClassic.smalld <- ggplot(coverage.classic.smalld, aes(x=as.numeric(var.ratio), y=coverage.rate, shape=Ns, color=Ns, ymin=.85, ymax=1)) + 
    geom_point(size=3, solid=FALSE) + 
    scale_shape_manual(values=rep(c(1,2,0), 3)) +
    geom_line(size=.5) + 
    scale_color_manual(values=rep(c('deepskyblue4','darkgoldenrod1', 'darkgrey'), each=3)) + 
    scale_x_discrete(breaks=seq(-2:2), labels=c(.2,.5,1,2,5)) +
    scale_y_continuous(breaks=seq(from=.85, to=1, length.out=16)) +
    xlab(bquote('s'[1]^2~'/'~'s'[2]^2)) + ylab('Coverage Probability') + ggtitle("Student's t Test")

CovClassic.midd <- ggplot(coverage.classic.midd, aes(x=as.numeric(var.ratio), y=coverage.rate, shape=Ns, color=Ns, ymin=.85, ymax=1)) + 
    geom_point(size=3, solid=FALSE) + 
    scale_shape_manual(values=rep(c(1,2,0), 3)) +
    geom_line(size=.5) + 
    scale_color_manual(values=rep(c('deepskyblue4','darkgoldenrod1', 'darkgrey'), each=3)) + 
    scale_x_discrete(breaks=seq(-2:2), labels=c(.2,.5,1,2,5)) +
    scale_y_continuous(breaks=seq(from=.85, to=1, length.out=16)) +
    xlab(bquote('s'[1]^2~'/'~'s'[2]^2)) + ylab('Coverage Probability') + ggtitle("Student's t Test")

CovClassic.bigd <- ggplot(coverage.classic.bigd, aes(x=as.numeric(var.ratio), y=coverage.rate, shape=Ns, color=Ns, ymin=.85, ymax=1)) + 
    geom_point(size=3, solid=FALSE) + 
    scale_shape_manual(values=rep(c(1,2,0), 3)) +
    geom_line(size=.5) + 
    scale_color_manual(values=rep(c('deepskyblue4','darkgoldenrod1', 'darkgrey'), each=3)) + 
    scale_x_discrete(breaks=seq(-2:2), labels=c(.2,.5,1,2,5)) +
    scale_y_continuous(breaks=seq(from=.85, to=1, length.out=16)) +
    xlab(bquote('s'[1]^2~'/'~'s'[2]^2)) + ylab('Coverage Probability') + ggtitle("Student's t Test")

@


<<welch coverage, echo=FALSE>>=

##### Plot Welch results only

## Set up data
nullT.ve.ne <- obs.coverage.ve.ne[,1,2]
nullT.v2.ne <- obs.coverage.v2.ne[,1,2]
nullT.v5.ne <- obs.coverage.v5.ne[,1,2]
nullT.ve.1.5n <- obs.coverage.ve.1.5n[,1,2]
nullT.ve.2n <- obs.coverage.ve.2n[,1,2]
nullT.v2.1.5n.ssv <- obs.coverage.v2.1.5n.ssv[,1,2]
nullT.v2.1.5n.bsv <- obs.coverage.v2.1.5n.bsv[,1,2]
nullT.v2.2n.ssv <- obs.coverage.v2.2n.ssv[,1,2]
nullT.v2.2n.bsv <- obs.coverage.v2.2n.bsv[,1,2]
nullT.v5.1.5n.ssv <- obs.coverage.v5.1.5n.ssv[,1,2]
nullT.v5.1.5n.bsv <- obs.coverage.v5.1.5n.bsv[,1,2]
nullT.v5.2n.ssv <- obs.coverage.v5.2n.ssv[,1,2]
nullT.v5.2n.bsv <- obs.coverage.v5.2n.bsv[,1,2]
smalld.ve.ne <- obs.coverage.ve.ne[,2,2]
smalld.v2.ne <- obs.coverage.v2.ne[,2,2]
smalld.v5.ne <- obs.coverage.v5.ne[,2,2]
smalld.ve.1.5n <- obs.coverage.ve.1.5n[,2,2]
smalld.ve.2n <- obs.coverage.ve.2n[,2,2]
smalld.v2.1.5n.ssv <- obs.coverage.v2.1.5n.ssv[,2,2]
smalld.v2.1.5n.bsv <- obs.coverage.v2.1.5n.bsv[,2,2]
smalld.v2.2n.ssv <- obs.coverage.v2.2n.ssv[,2,2]
smalld.v2.2n.bsv <- obs.coverage.v2.2n.bsv[,2,2]
smalld.v5.1.5n.ssv <- obs.coverage.v5.1.5n.ssv[,2,2]
smalld.v5.1.5n.bsv <- obs.coverage.v5.1.5n.bsv[,2,2]
smalld.v5.2n.ssv <- obs.coverage.v5.2n.ssv[,2,2]
smalld.v5.2n.bsv <- obs.coverage.v5.2n.bsv[,2,2]
midd.ve.ne <- obs.coverage.ve.ne[,3,2]
midd.v2.ne <- obs.coverage.v2.ne[,3,2]
midd.v5.ne <- obs.coverage.v5.ne[,3,2]
midd.ve.1.5n <- obs.coverage.ve.1.5n[,3,2]
midd.ve.2n <- obs.coverage.ve.2n[,3,2]
midd.v2.1.5n.ssv <- obs.coverage.v2.1.5n.ssv[,3,2]
midd.v2.1.5n.bsv <- obs.coverage.v2.1.5n.bsv[,3,2]
midd.v2.2n.ssv <- obs.coverage.v2.2n.ssv[,3,2]
midd.v2.2n.bsv <- obs.coverage.v2.2n.bsv[,3,2]
midd.v5.1.5n.ssv <- obs.coverage.v5.1.5n.ssv[,3,2]
midd.v5.1.5n.bsv <- obs.coverage.v5.1.5n.bsv[,3,2]
midd.v5.2n.ssv <- obs.coverage.v5.2n.ssv[,3,2]
midd.v5.2n.bsv <- obs.coverage.v5.2n.bsv[,3,2]
bigd.ve.ne <- obs.coverage.ve.ne[,4,2]
bigd.v2.ne <- obs.coverage.v2.ne[,4,2]
bigd.v5.ne <- obs.coverage.v5.ne[,4,2]
bigd.ve.1.5n <- obs.coverage.ve.1.5n[,4,2]
bigd.ve.2n <- obs.coverage.ve.2n[,4,2]
bigd.v2.1.5n.ssv <- obs.coverage.v2.1.5n.ssv[,4,2]
bigd.v2.1.5n.bsv <- obs.coverage.v2.1.5n.bsv[,4,2]
bigd.v2.2n.ssv <- obs.coverage.v2.2n.ssv[,4,2]
bigd.v2.2n.bsv <- obs.coverage.v2.2n.bsv[,4,2]
bigd.v5.1.5n.ssv <- obs.coverage.v5.1.5n.ssv[,4,2]
bigd.v5.1.5n.bsv <- obs.coverage.v5.1.5n.bsv[,4,2]
bigd.v5.2n.ssv <- obs.coverage.v5.2n.ssv[,4,2]
bigd.v5.2n.bsv <- obs.coverage.v5.2n.bsv[,4,2]

## Put it in a dataframe
coverage.welch.nullT <- data.frame(
    coverage.rate=c(nullT.ve.ne, nullT.v2.ne, nullT.v2.ne, nullT.v5.ne, nullT.v5.ne, nullT.ve.1.5n, nullT.ve.2n, nullT.v2.1.5n.ssv, nullT.v2.1.5n.bsv, nullT.v2.2n.ssv, nullT.v2.2n.bsv, nullT.v5.1.5n.ssv, nullT.v5.1.5n.bsv, nullT.v5.2n.ssv, nullT.v5.2n.bsv), #NOTE: doubled any with equal variance or equal ns because they don't have separate ssv and bsv versions
    Ns=factor(c('20,20','50,50','100,100','20,20','50,50','100,100','20,20','50,50','100,100','20,20','50,50','100,100','20,20','50,50','100,100','20,30','50,75','100,150','20,40','50,100','100,200','20,30','50,75','100,150','20,30','50,75','100,150','20,40','50,100','100,200','20,40','50,100','100,200','20,30','50,75','100,150','20,30','50,75','100,150','20,40','50,100','100,200','20,40','50,100','100,200'), levels=c('20,20','20,30','20,40','50,50','50,75','50,100','100,100','100,150','100,200')),                                                                                                                                                                                                                                                                                                                
    var.ratio=factor(rep(c(0,-1,1,-2,2,0,0,-1,1,-1,1,-2,2,-2,2), each=3), levels=c(-2,-1,0,1,2), labels=c(.2,.5,1,2,5)))
coverage.welch.nullT <- coverage.welch.nullT[order(coverage.welch.nullT$Ns, coverage.welch.nullT$var.ratio),]

coverage.welch.smalld <- data.frame(
    coverage.rate=c(smalld.ve.ne, smalld.v2.ne, smalld.v2.ne, smalld.v5.ne, smalld.v5.ne, smalld.ve.1.5n, smalld.ve.2n, smalld.v2.1.5n.ssv, smalld.v2.1.5n.bsv, smalld.v2.2n.ssv, smalld.v2.2n.bsv, smalld.v5.1.5n.ssv, smalld.v5.1.5n.bsv, smalld.v5.2n.ssv, smalld.v5.2n.bsv), #NOTE: doubled any with equal variance or equal ns because they don't have separate ssv and bsv versions
    Ns=factor(c('20,20','50,50','100,100','20,20','50,50','100,100','20,20','50,50','100,100','20,20','50,50','100,100','20,20','50,50','100,100','20,30','50,75','100,150','20,40','50,100','100,200','20,30','50,75','100,150','20,30','50,75','100,150','20,40','50,100','100,200','20,40','50,100','100,200','20,30','50,75','100,150','20,30','50,75','100,150','20,40','50,100','100,200','20,40','50,100','100,200'), levels=c('20,20','20,30','20,40','50,50','50,75','50,100','100,100','100,150','100,200')),                                                                                                                                                                                                                                                                                                                
    var.ratio=factor(rep(c(0,-1,1,-2,2,0,0,-1,1,-1,1,-2,2,-2,2), each=3), levels=c(-2,-1,0,1,2), labels=c(.2,.5,1,2,5)))
coverage.welch.smalld <- coverage.welch.smalld[order(coverage.welch.smalld$Ns, coverage.welch.smalld$var.ratio),]

coverage.welch.midd <- data.frame(
    coverage.rate=c(midd.ve.ne, midd.v2.ne, midd.v2.ne, midd.v5.ne, midd.v5.ne, midd.ve.1.5n, midd.ve.2n, midd.v2.1.5n.ssv, midd.v2.1.5n.bsv, midd.v2.2n.ssv, midd.v2.2n.bsv, midd.v5.1.5n.ssv, midd.v5.1.5n.bsv, midd.v5.2n.ssv, midd.v5.2n.bsv), #NOTE: doubled any with equal variance or equal ns because they don't have separate ssv and bsv versions
    Ns=factor(c('20,20','50,50','100,100','20,20','50,50','100,100','20,20','50,50','100,100','20,20','50,50','100,100','20,20','50,50','100,100','20,30','50,75','100,150','20,40','50,100','100,200','20,30','50,75','100,150','20,30','50,75','100,150','20,40','50,100','100,200','20,40','50,100','100,200','20,30','50,75','100,150','20,30','50,75','100,150','20,40','50,100','100,200','20,40','50,100','100,200'), levels=c('20,20','20,30','20,40','50,50','50,75','50,100','100,100','100,150','100,200')),                                                                                                                                                                                                                                                                                                                
    var.ratio=factor(rep(c(0,-1,1,-2,2,0,0,-1,1,-1,1,-2,2,-2,2), each=3), levels=c(-2,-1,0,1,2), labels=c(.2,.5,1,2,5)))
coverage.welch.midd <- coverage.welch.midd[order(coverage.welch.midd$Ns, coverage.welch.midd$var.ratio),]

coverage.welch.bigd <- data.frame(
    coverage.rate=c(bigd.ve.ne, bigd.v2.ne, bigd.v2.ne, bigd.v5.ne, bigd.v5.ne, bigd.ve.1.5n, bigd.ve.2n, bigd.v2.1.5n.ssv, bigd.v2.1.5n.bsv, bigd.v2.2n.ssv, bigd.v2.2n.bsv, bigd.v5.1.5n.ssv, bigd.v5.1.5n.bsv, bigd.v5.2n.ssv, bigd.v5.2n.bsv), #NOTE: doubled any with equal variance or equal ns because they don't have separate ssv and bsv versions
    Ns=factor(c('20,20','50,50','100,100','20,20','50,50','100,100','20,20','50,50','100,100','20,20','50,50','100,100','20,20','50,50','100,100','20,30','50,75','100,150','20,40','50,100','100,200','20,30','50,75','100,150','20,30','50,75','100,150','20,40','50,100','100,200','20,40','50,100','100,200','20,30','50,75','100,150','20,30','50,75','100,150','20,40','50,100','100,200','20,40','50,100','100,200'), levels=c('20,20','20,30','20,40','50,50','50,75','50,100','100,100','100,150','100,200')),                                                                                                                                                                                                                                                                                                                
    var.ratio=factor(rep(c(0,-1,1,-2,2,0,0,-1,1,-1,1,-2,2,-2,2), each=3), levels=c(-2,-1,0,1,2), labels=c(.2,.5,1,2,5)))
coverage.welch.bigd <- coverage.welch.bigd[order(coverage.welch.bigd$Ns, coverage.welch.bigd$var.ratio),]


## Set up welch t test plot
CovWelch.nullT <- ggplot(coverage.welch.nullT, aes(x=as.numeric(var.ratio), y=coverage.rate, shape=Ns, color=Ns, ymin=.85, ymax=1)) + 
    geom_point(size=3, solid=FALSE) + 
    scale_shape_manual(values=rep(c(1,2,0), 3)) +
    geom_line(size=.5) + 
    scale_color_manual(values=rep(c('deepskyblue4','darkgoldenrod1', 'darkgrey'), each=3)) + 
    scale_x_discrete(breaks=seq(-2:2), labels=c(.2,.5,1,2,5)) +
    scale_y_continuous(breaks=seq(from=.85, to=1, length.out=16)) +
    xlab(bquote('s'[1]^2~'/'~'s'[2]^2)) + ylab('Coverage Probability') + ggtitle("Welch's t Test")

CovWelch.smalld <- ggplot(coverage.welch.smalld, aes(x=as.numeric(var.ratio), y=coverage.rate, shape=Ns, color=Ns, ymin=.85, ymax=1)) + 
    geom_point(size=3, solid=FALSE) + 
    scale_shape_manual(values=rep(c(1,2,0), 3)) +
    geom_line(size=.5) + 
    scale_color_manual(values=rep(c('deepskyblue4','darkgoldenrod1', 'darkgrey'), each=3)) + 
    scale_x_discrete(breaks=seq(-2:2), labels=c(.2,.5,1,2,5)) +
    scale_y_continuous(breaks=seq(from=.85, to=1, length.out=16)) +
    xlab(bquote('s'[1]^2~'/'~'s'[2]^2)) + ylab('Coverage Probability') + ggtitle("Welch's t Test")

CovWelch.midd <- ggplot(coverage.welch.midd, aes(x=as.numeric(var.ratio), y=coverage.rate, shape=Ns, color=Ns, ymin=.85, ymax=1)) + 
    geom_point(size=3, solid=FALSE) + 
    scale_shape_manual(values=rep(c(1,2,0), 3)) +
    geom_line(size=.5) + 
    scale_color_manual(values=rep(c('deepskyblue4','darkgoldenrod1', 'darkgrey'), each=3)) + 
    scale_x_discrete(breaks=seq(-2:2), labels=c(.2,.5,1,2,5)) +
    scale_y_continuous(breaks=seq(from=.85, to=1, length.out=16)) +
    xlab(bquote('s'[1]^2~'/'~'s'[2]^2)) + ylab('Coverage Probability') + ggtitle("Welch's t Test")

CovWelch.bigd <- ggplot(coverage.welch.bigd, aes(x=as.numeric(var.ratio), y=coverage.rate, shape=Ns, color=Ns, ymin=.85, ymax=1)) + 
    geom_point(size=3, solid=FALSE) + 
    scale_shape_manual(values=rep(c(1,2,0), 3)) +
    geom_line(size=.5) + 
    scale_color_manual(values=rep(c('deepskyblue4','darkgoldenrod1', 'darkgrey'), each=3)) + 
    scale_x_discrete(breaks=seq(-2:2), labels=c(.2,.5,1,2,5)) +
    scale_y_continuous(breaks=seq(from=.85, to=1, length.out=16)) +
    xlab(bquote('s'[1]^2~'/'~'s'[2]^2)) + ylab('Coverage Probability') + ggtitle("Welch's t Test")

@

\begin{figure}
<<coverage plots, echo=FALSE>>=
layout <- matrix(c(1,2), nrow=1, byrow=TRUE)
multiplot(CovClassic.nullT, CovWelch.nullT, layout=layout)
@
\textit{Figure 10.} Coverage probabilities for Student's and Welch's t tests.
\end{figure}

\section{Discussion}
    We set out to find a simple rule to help researchers decide when to use Student's t test and when to use Welch's t test. We believe that the simplest rule is to always use Welch's t test to compare the means of independent groups.
    
    The results of our simulations demonstrated that when the population variances or sample sizes were equal, using Welch's t test instead of Student's t test didn't hurt. Figures 3 and 4 show that the Welch degrees of freedom could drop below 70\% the Student's t test degrees of freedom when only the variance ratio or the sample size ratio changed. Nevertheless, the false positive rates, power, and coverage probabilities of the two tests were almost identical under these conditions. It seems as though the difference in the degrees of freedom of the two tests was not very important at all. We illustrate this point in Figure 11, which shows two overlapping t distributions, one with half the degrees of freedom of the other. The tails of the distributions with the smaller degrees of freedom are a little larger, but not much. Given the same t-value, in the majority of cases these distributions would agree about whether or not to reject the null hypothesis. 
    
    More important than the degrees of freedom is the standard error, which affects the t-value itself. When either the variances or the sample sizes are equal, the pooled standard error of Student's t test and the separate variances standard error of Welch's t test are identical, and the two tests will generally agree with each other. However, when both the variances and the sample sizes are unequal, the pooled standard error of Student's t test gives more weight to the group with the larger sample size (CITATIONS); if that group has the larger variance, then Student's t test becomes more conservative, but if that group has the smaller variance, then Student's t test becomes more liberal. This can be seen in Figures 7 and 10, where the type I error rate and coverage probability of Student's t test vary widely. In contrast, Welch's t test was more stable, regardless of which sample had the larger variance. This can be seen in Figures 8 and 10, where the type I error rate and coverage probability of Welch's t test were exactly what you would expect.
    
    The biggest benefit of Student's t test was that it had more power than Welch's t test to detect true effects when the larger sample had the smaller variance--yet under these same conditions, it had an inflated type I error rate and the lowest coverage probability. Far from being underpowered, Welch's t test was more powerful than Student's t test when the larger sample had the larger variance, but under these same conditions it retained the expected type I error rate and coverage probability. Overall, Welch's t test did a better job of balancing researcher concerns about false positives, power, and estimation.
    
    We believe that researchers are more likely to use simple decision rules, and so we echo the recommendations of previous researchers to always use the Welch t test (CITATIONS). For researchers who have a reason to prefer Student's t test, there is some good news for those who are doing experimental work. With experiments, subjects are usually assigned evenly to conditions. In this case, the two tests will perform equally well. However, for researchers who are interested in comparing pre-existing groups, it might not be possible to have equal sample sizes. In this latter case, the Welch t test is likely to outperform Student's t test because the observed variances are unlikely to be precisely equal. If a researcher still prefers a complex rule where using either test is still possible, then one option is to look at boxplots to determine whether you can reasonably assume that the group variances are equal. This judgment will be easier if the sample sizes are large. Nevertheless, researchers should rest assured that they won't suffer from using the simple rule to always go with Welch's t test.
    
    We examined whether the ratio of the degrees of freedom from Welch's t test to the degrees of freedom from Student's t test could provide a good heuristic for determining whether or not the variances of the two groups are unequal. The rule had the greatest potential when it mattered the least--when the sample sizes of the two groups were equal. When the sample sizes were unequal the degrees of freedom ratio was not a reliable indicator of unequal variances. It could become very high as variances became unequal and it could drop very low when the variances were equal.

\begin{figure}  
<< tdist, echo=FALSE>>=

#plot t distributions with different degrees of freedom - this needs to go somewhere

x <- rep(seq(-4,4,.1), 2)
y <- c(dt(seq(-4,4,.1),df=38), dt(seq(-4,4,.1),df=38/2))
group <- factor(rep(c('df = 38', 'df = 19'), each=81))

ggplot(data.frame(x,y,group), aes(y=y, x=x, fill=group)) + geom_line(aes(colour=group)) + labs(x=NULL,y=NULL,colour=NULL)
@
\textit{Figure 11. Two t distributions with different degrees of freedom.}
\end{figure}

Make a note that our df ratio rule works best when it doesn't matter - when sample sizes are equal.
    
    In much experimental work, the choice between the tests is probably fine if the experimenter ensures that sample sizes are equal. This is generally not an option with pre-existing groups. 
    
    Notably, the two standard errors are equal when either the sample sizes or the variances of the two groups are identical, so the Welch t test could only be more powerful when both the sample sizes and variances are unequal. 
    
    Benefits of just using welch rule: simplifies the decision which makes it easier, puts researcher motivation in line with false positive preservation

    One of the concerns about using the Welch t test as an alternative to Student's t test is that the penalty on the degrees of freedom makes it difficult to find effects. Figures 3 and 4 show that the penalty is small when the variances and sample sizes are approximately equal, which is within the range that one might expect from normal sampling error when the true population variances are equal. However, the penalty to the degrees of freedom might not be very important in general. Figure 11 displays two t-distributions where one has half the degrees of freeom of the other. Despite the drastic difference in the degrees of freedom, the two distributions largely overlap and so the differences in standard errors of the two tests, which will affect the value of the t-test, might be more important.
    
    Cite Zimmerman 1996 when talking about how the pooled variance is the major cause of problems with Student's t test
    
    Cohen's d is a population standardized mean difference based on the standard deviation of the population - but what if there is no standard deviation of the population? This is exactly the situation we're faced with when lifting the equal variances assumption.
  



<< junk, echo=FALSE >>=

####################### boxplots of the simulated df ratios 

## Equal ns

# equal vars, equal ns
#Bplot.ve.ne.ns20 <- qplot(y=dfs.list$ve.ns20.ne.nullT, geom='boxplot', x=' ', ylab=bquote(frac('df'['Welch'],'df'['Student'])), xlab=bquote(frac('n'[1],'n'[2])~'='~frac(20,20)~', '~frac('var'[1],'var'[2])~'='~1), ylim=c(.5,1))
#Bplot.ve.ne.ns50 <- qplot(y=dfs.list$ve.ns50.ne.nullT, geom='boxplot', x=' ', ylab=bquote(frac('df'['Welch'],'df'['Student'])), xlab=bquote(frac('n'[1],'n'[2])~'='~frac(50,50)~', '~frac('var'[1],'var'[2])~'='~1), ylim=c(.5,1))
#Bplot.ve.ne.ns100 <- qplot(y=dfs.list$ve.ns100.ne.nullT, geom='boxplot', x=' ', ylab=bquote(frac('df'['Welch'],'df'['Student'])), xlab=bquote(frac('n'[1],'n'[2])~'='~frac(100,100)~', '~frac('var'[1],'var'[2])~'='~1), ylim=c(.5,1))

#grid.arrange(Bplot.ve.ne.ns20, Bplot.ve.ne.ns50, Bplot.ve.ne.ns100, ncol=3, heights=unit(.5, 'npc'), main='Figure 3. Degrees of Freedom Ratios with Increasing Sample Sizes')



# different vars, equal ns
#Bplot.ve.ne.ns50 <- qplot(y=dfs.list$ve.ns50.ne.nullT, geom='boxplot', x=' ', ylab=bquote(frac('df'['Welch'],'df'['Student'])), xlab=bquote(frac('n'[1],'n'[2])~'='~frac(50,50)~', '~frac('var'[1],'var'[2])~'='~1), ylim=c(.5,1))
#Bplot.v2.ne.ns50 <- qplot(y=dfs.list$v2.ns50.ne.nullT, geom='boxplot', x=' ', ylab=bquote(frac('df'['Welch'],'df'['Student'])), xlab=bquote(frac('n'[1],'n'[2])~'='~frac(50,50)~', '~frac('var'[1],'var'[2])~'='~frac(1,2)), ylim=c(.5,1))
#Bplot.v5.ne.ns50 <- qplot(y=dfs.list$v5.ns50.ne.nullT, geom='boxplot', x=' ', ylab=bquote(frac('df'['Welch'],'df'['Student'])), xlab=bquote(frac('n'[1],'n'[2])~'='~frac(50,50)~', '~frac('var'[1],'var'[2])~'='~frac(1,5)), ylim=c(.5,1))

#grid.arrange(Bplot.ve.ne.ns50, Bplot.v2.ne.ns50, Bplot.v5.ne.ns50, ncol=3, heights=unit(.5,'npc'), main='Figure 4. Degrees of Freedom Ratios with Changing Variance Ratios')




# equal vars, changing N ratios
#Bplot.ve.ne.ns50 <- qplot(y=dfs.list$ve.ns50.ne.nullT, geom='boxplot', x=' ', ylab=bquote(frac('df'['Welch'],'df'['Student'])), xlab=bquote(frac('n'[1],'n'[2])~'='~frac(50,50)~', '~frac('var'[1],'var'[2])~'='~1), ylim=c(.45,1))
#Bplot.ve.1.5n.ns50 <- qplot(y=dfs.list$ve.ns50.1.5n.nullT, geom='boxplot', x=' ', ylab=bquote(frac('df'['Welch'],'df'['Student'])), xlab=bquote(frac('n'[1],'n'[2])~'='~frac(50,75)~', '~frac('var'[1],'var'[2])~'='~1), ylim=c(.45,1))
#Bplot.ve.2n.ns50 <- qplot(y=dfs.list$ve.ns50.2n.nullT, geom='boxplot', x=' ', ylab=bquote(frac('df'['Welch'],'df'['Student'])), xlab=bquote(frac('n'[1],'n'[2])~'='~frac(50,100)~', '~frac('var'[1],'var'[2])~'='~1), ylim=c(.45,1))

#grid.arrange(Bplot.ve.ne.ns50, Bplot.ve.1.5n.ns50 , Bplot.ve.2n.ns50, ncol=3, heights=unit(.5,'npc'), main='Figure 5. Degrees of Freedom Ratios with Changing Sample Size Ratios')


# different vars, changing N ratios
#Bplot.ve.1.5n.ns50 <- qplot(y=dfs.list$ve.ns50.1.5n.nullT, geom='boxplot', x=' ', ylab=bquote(frac('df'['Welch'],'df'['Student'])), xlab=bquote(frac('n'[1],'n'[2])~'='~frac(50,75)~', '~frac('var'[1],'var'[2])~'='~1), ylim=c(.4,1))
#Bplot.v2.1.5n.ssv.ns50 <- qplot(y=dfs.list$v2.ns50.1.5n.ssv.nullT, geom='boxplot', x=' ', ylab=bquote(frac('df'['Welch'],'df'['Student'])), xlab=bquote(frac('n'[1],'n'[2])~'='~frac(50,75)~', '~frac('var'[1],'var'[2])~'='~frac(1,2)), ylim=c(.4,1))
#Bplot.v5.1.5n.ssv.ns50 <- qplot(y=dfs.list$v5.ns50.1.5n.ssv.nullT, geom='boxplot', x=' ', ylab=bquote(frac('df'['Welch'],'df'['Student'])), xlab=bquote(frac('n'[1],'n'[2])~'='~frac(50,75)~', '~frac('var'[1],'var'[2])~'='~frac(1,5)), ylim=c(.4,1))

#Bplot.ve.1.5n.ns50 <- qplot(y=dfs.list$ve.ns50.1.5n.nullT, geom='boxplot', x=' ', ylab=bquote(frac('df'['Welch'],'df'['Student'])), xlab=bquote(frac('n'[1],'n'[2])~'='~frac(50,75)~', '~frac('var'[1],'var'[2])~'='~1), ylim=c(.4,1))
#Bplot.v2.1.5n.bsv.ns50 <- qplot(y=dfs.list$v2.ns50.1.5n.bsv.nullT, geom='boxplot', x=' ', ylab=bquote(frac('df'['Welch'],'df'['Student'])), xlab=bquote(frac('n'[1],'n'[2])~'='~frac(50,75)~', '~frac('var'[1],'var'[2])~'='~2), ylim=c(.4,1))
#Bplot.v5.1.5n.bsv.ns50 <- qplot(y=dfs.list$v5.ns50.1.5n.bsv.nullT, geom='boxplot', x=' ', ylab=bquote(frac('df'['Welch'],'df'['Student'])), xlab=bquote(frac('n'[1],'n'[2])~'='~frac(50,75)~', '~frac('var'[1],'var'[2])~'='~5), ylim=c(.4,1))

#grid.arrange(Bplot.ve.1.5n.ns50, Bplot.v2.1.5n.ssv.ns50, Bplot.v5.1.5n.ssv.ns50, Bplot.ve.1.5n.ns50,  Bplot.v2.1.5n.bsv.ns50, Bplot.v5.1.5n.bsv.ns50, ncol=3, main='Figure 6. Degrees of Freedom Ratios with Changing Variance Ratios and Sample Size Ratios')




################## Boxplots of first simulations




### all null true

## Equal ns

# equal vars, equal ns
# 
# Bplot.ve.ne.ns20 <- ggplot(venesim1$ve.ns20.ne.nullT, aes(y=dv, x=factor(group, levels=c(1,2)))) +
#     geom_boxplot() +
#     scale_x_discrete(labels=c(bquote('n'[1]~'= 20'), bquote('n'[2]~'= 20'))) +
#     scale_y_continuous(limits=c(-2.5,15.5)) +
#     labs(title='Equal variances', x='', y='') +
#     theme(plot.margin=unit(c(0,0,0,0),'picas'))
# 
# Bplot.ve.ne.ns50 <- ggplot(venesim1$ve.ns50.ne.nullT, aes(y=dv, x=factor(group, levels=c(1,2)))) +
#     geom_boxplot() +
#     scale_x_discrete(labels=c(bquote('n'[1]~'= 50'), bquote('n'[2]~'= 50'))) +
#     scale_y_continuous(limits=c(-2.5,15.5)) +
#     labs(x='', y='') +
#     theme(plot.margin=unit(c(0,0,0,0),'picas'))
# 
# Bplot.ve.ne.ns100 <- ggplot(venesim1$ve.ns100.ne.nullT, aes(y=dv, x=factor(group, levels=c(1,2)))) +
#     geom_boxplot() +
#     scale_x_discrete(labels=c(bquote('n'[1]~'= 100'), bquote('n'[2]~'= 100'))) +
#     scale_y_continuous(limits=c(-2.5,15.5)) +
#     labs(x='', y='') +
#     theme(plot.margin=unit(c(0,0,0,0),'picas'))
# 
# # double vars, equal ns
# Bplot.v2.ne.ns20 <- ggplot(v2nesim1$v2.ns20.ne.nullT, aes(y=dv, x=factor(group, levels=c(1,2)))) +
#     geom_boxplot() +
#     scale_x_discrete(labels=c(bquote('n'[1]~'= 20'), bquote('n'[2]~'= 20'))) +
#     scale_y_continuous(limits=c(-2.5,15.5)) +
#     labs(title='Double variances', x='', y='') +
#     theme(plot.margin=unit(c(0,0,0,0),'picas'))
# 
# Bplot.v2.ne.ns50 <- ggplot(v2nesim1$v2.ns50.ne.nullT, aes(y=dv, x=factor(group, levels=c(1,2)))) +
#     geom_boxplot() +
#     scale_x_discrete(labels=c(bquote('n'[1]~'= 50'), bquote('n'[2]~'= 50'))) +
#     scale_y_continuous(limits=c(-2.5,15.5)) +
#     labs(x='', y='') +
#     theme(plot.margin=unit(c(0,0,0,0),'picas'))
# 
# Bplot.v2.ne.ns100 <- ggplot(v2nesim1$v2.ns100.ne.nullT, aes(y=dv, x=factor(group, levels=c(1,2)))) +
#     geom_boxplot() +
#     scale_x_discrete(labels=c(bquote('n'[1]~'= 100'), bquote('n'[2]~'= 100'))) +
#     scale_y_continuous(limits=c(-2.5,15.5)) +
#     labs(x='', y='') +
#     theme(plot.margin=unit(c(0,0,0,0),'picas'))
# 
# # 5x vars, equal ns
# Bplot.v5.ne.ns20 <- ggplot(v5nesim1$v5.ns20.ne.nullT, aes(y=dv, x=factor(group, levels=c(1,2)))) +
#     geom_boxplot() +
#     scale_x_discrete(labels=c(bquote('n'[1]~'= 20'), bquote('n'[2]~'= 20'))) +
#     scale_y_continuous(limits=c(-2.5,15.5)) +
#     labs(title='5x variances', x='', y='') +
#     theme(plot.margin=unit(c(0,0,0,0),'picas'))
# 
# Bplot.v5.ne.ns50 <- ggplot(v5nesim1$v5.ns50.ne.nullT, aes(y=dv, x=factor(group, levels=c(1,2)))) +
#     geom_boxplot() +
#     scale_x_discrete(labels=c(bquote('n'[1]~'= 50'), bquote('n'[2]~'= 50'))) +
#     scale_y_continuous(limits=c(-2.5,15.5)) +
#     labs(x='', y='') +
#     theme(plot.margin=unit(c(0,0,0,0),'picas'))
#     
# Bplot.v5.ne.ns100 <- ggplot(v5nesim1$v5.ns100.ne.nullT, aes(y=dv, x=factor(group, levels=c(1,2)))) +
#     geom_boxplot() +
#     scale_x_discrete(labels=c(bquote('n'[1]~'= 100'), bquote('n'[2]~'= 100'))) +
#     scale_y_continuous(limits=c(-2.5,15.5)) +
#     labs(x='', y='') +
#     theme(plot.margin=unit(c(0,0,0,0),'picas'))
# 
# # Plot equal ns
# grid.arrange(Bplot.ve.ne.ns20, Bplot.v2.ne.ns20, Bplot.v5.ne.ns20, Bplot.ve.ne.ns50, Bplot.v2.ne.ns50, Bplot.v5.ne.ns50, Bplot.ve.ne.ns100, Bplot.v2.ne.ns100, Bplot.v5.ne.ns100, nrow=3, ncol=3, main='Figure 1. Boxplots of First Simulations When Sample Sizes are Equal')
# 
# 
# 
# ## We'll only use it when the small N = 50 as an example
# 
# # Variances equal
# # NOTE: it's not really ssv when variances are equal
# Bplot.ve.n2.ssv <- ggplot(ven2sim1$ve.ns50.2n.nullT, aes(y=dv, x=factor(group, levels=c(1,2)))) +
#     geom_boxplot() +
#     scale_x_discrete(labels=c(bquote('n'[1]~'= 50'), bquote('n'[2]~'= 100'))) +
#     scale_y_continuous(limits=c(-3.5,16)) +
#     labs(title='Equal variances', x='', y='') +
#     theme(plot.margin=unit(c(0,0,0,0),'picas'))
# 
# Bplot.ve.n1.5.ssv <- ggplot(ven15sim1$ve.ns50.1.5n.nullT, aes(y=dv, x=factor(group, levels=c(1,2)))) +
#     geom_boxplot() +
#     scale_x_discrete(labels=c(bquote('n'[1]~'= 50'), bquote('n'[2]~'= 75'))) +
#     scale_y_continuous(limits=c(-3.5,16)) +
#     labs(x='', y='') +
#     theme(plot.margin=unit(c(0,0,0,0),'picas'))
# 
# Bplot.ve.n1.5.bsv <- ggplot(ven15sim1$ve.ns50.1.5n.nullT, aes(y=dv, x=factor(group, levels=c(2,1)))) +
#     geom_boxplot() +
#     scale_x_discrete(labels=c(bquote('n'[1]~'= 75'), bquote('n'[2]~'= 50'))) +
#     scale_y_continuous(limits=c(-3.5,16)) +
#     labs(x='', y='') +
#     theme(plot.margin=unit(c(0,0,0,0),'picas'))
# 
# Bplot.ve.n2.bsv <- ggplot(ven2sim1$ve.ns50.2n.nullT, aes(y=dv, x=factor(group, levels=c(2,1)))) +
#     geom_boxplot() +
#     scale_x_discrete(labels=c(bquote('n'[1]~'= 100'), bquote('n'[2]~'= 50'))) +
#     scale_y_continuous(limits=c(-3.5,16)) +
#     labs(x='', y='') +
#     theme(plot.margin=unit(c(0,0,0,0),'picas'))
# 
# 
# # Variances double
# # NOTE: it's not really ssv when variances are equal
# Bplot.v2.n2.ssv <- ggplot(v2n2ssvsim1$v2.ns50.2n.ssv.nullT, aes(y=dv, x=factor(group, levels=c(1,2)))) +
#     geom_boxplot() +
#     scale_x_discrete(labels=c(bquote('n'[1]~'= 50'), bquote('n'[2]~'= 100'))) +
#     scale_y_continuous(limits=c(-3.5,16)) +
#     labs(title='Double variances', x='', y='') +
#     theme(plot.margin=unit(c(0,0,0,0),'picas'))
# 
# Bplot.v2.n1.5.ssv <- ggplot(v2n15ssvsim1$v2.ns50.1.5n.ssv.nullT, aes(y=dv, x=factor(group, levels=c(1,2)))) +
#     geom_boxplot() +
#     scale_x_discrete(labels=c(bquote('n'[1]~'= 50'), bquote('n'[2]~'= 75'))) +
#     scale_y_continuous(limits=c(-3.5,16)) +
#     labs(x='', y='') +
#     theme(plot.margin=unit(c(0,0,0,0),'picas'))
# 
# Bplot.v2.n1.5.bsv <- ggplot(v2n15bsvsim1$v2.ns50.1.5n.bsv.nullT, aes(y=dv, x=factor(group, levels=c(2,1)))) +
#     geom_boxplot() +
#     scale_x_discrete(labels=c(bquote('n'[1]~'= 75'), bquote('n'[2]~'= 50'))) +
#     scale_y_continuous(limits=c(-3.5,16)) +
#     labs(x='', y='') +
#     theme(plot.margin=unit(c(0,0,0,0),'picas'))
# 
# Bplot.v2.n2.bsv <- ggplot(v2n2bsvsim1$v2.ns50.2n.bsv.nullT, aes(y=dv, x=factor(group, levels=c(2,1)))) +
#     geom_boxplot() +
#     scale_x_discrete(labels=c(bquote('n'[1]~'= 100'), bquote('n'[2]~'= 50'))) +
#     scale_y_continuous(limits=c(-3.5,16)) +
#     labs(x='', y='') +
#     theme(plot.margin=unit(c(0,0,0,0),'picas'))
# 
# # Variances 5x
# # NOTE: it's not really ssv when variances are equal
# Bplot.v5.n2.ssv <- ggplot(v5n2ssvsim1$v5.ns50.2n.ssv.nullT, aes(y=dv, x=factor(group, levels=c(1,2)))) +
#     geom_boxplot() +
#     scale_x_discrete(labels=c(bquote('n'[1]~'= 50'), bquote('n'[2]~'= 100'))) +
#     scale_y_continuous(limits=c(-3.5,16)) +
#     labs(title='5x variances', x='', y='') +
#     theme(plot.margin=unit(c(0,0,0,0),'picas'))
# 
# Bplot.v5.n1.5.ssv <- ggplot(v5n15ssvsim1$v5.ns50.1.5n.ssv.nullT, aes(y=dv, x=factor(group, levels=c(1,2)))) +
#     geom_boxplot() +
#     scale_x_discrete(labels=c(bquote('n'[1]~'= 50'), bquote('n'[2]~'= 75'))) +
#     scale_y_continuous(limits=c(-3.5,16)) +
#     labs(x='', y='') +
#     theme(plot.margin=unit(c(0,0,0,0),'picas'))
# 
# Bplot.v5.n1.5.bsv <- ggplot(v5n15bsvsim1$v5.ns50.1.5n.bsv.nullT, aes(y=dv, x=factor(group, levels=c(2,1)))) +
#     geom_boxplot() +
#     scale_x_discrete(labels=c(bquote('n'[1]~'= 75'), bquote('n'[2]~'= 50'))) +
#     scale_y_continuous(limits=c(-3.5,16)) +
#     labs(x='', y='') +
#     theme(plot.margin=unit(c(0,0,0,0),'picas'))
# 
# Bplot.v5.n2.bsv <- ggplot(v5n2bsvsim1$v5.ns50.2n.bsv.nullT, aes(y=dv, x=factor(group, levels=c(2,1)))) +
#     geom_boxplot() +
#     scale_x_discrete(labels=c(bquote('n'[1]~'= 100'), bquote('n'[2]~'= 50'))) +
#     scale_y_continuous(limits=c(-3.5,16)) +
#     labs(x='', y='') +
#     theme(plot.margin=unit(c(0,0,0,0),'picas'))
# 
# # Plot n = 20, different vars, different ns
# grid.arrange(Bplot.ve.n2.ssv, Bplot.v2.n2.ssv, Bplot.v5.n2.ssv, Bplot.ve.n1.5.ssv, Bplot.v2.n1.5.ssv, Bplot.v5.n1.5.ssv, Bplot.ve.n1.5.bsv, Bplot.v2.n1.5.bsv, Bplot.v5.n1.5.bsv, Bplot.ve.n2.bsv, Bplot.v2.n2.bsv, Bplot.v5.n2.bsv, nrow=4, main='Figure 2. Boxplots of First Simulations When Sample Sizes are Unequal')

@

\bibliography{bibliography}
\bibliographystyle{apacite}

\end{document}