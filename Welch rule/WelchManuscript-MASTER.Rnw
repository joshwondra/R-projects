%floatsintext can be used in man to have figs appear where they are called
\documentclass[man, noextraspace, apacite, floatsintext]{apa6}
%following looks like a journal printout; need to comment out previous line
%\documentclass[jou,noextraspace,apacite]{apa6}
\usepackage{apacite}

\title{Use Welch's \textit{t} Test to Compare the Means of Independent Groups}
\shorttitle{Welch t Test}
\author{Joshua D. Wondra and Richard Gonzalez}
\affiliation{Facebook, University of Michigan}

\abstract{When testing whether the means of independent groups are 
different from each other, researchers typically use Student's t test, which 
assumes that the population variances of the two groups are equal. If there is 
reason to believe that the group variances are unequal, then researchers can 
use Welch's t test, which does not assume equal variances, as an alternative. 
We were interested in finding a simple rule that could be used to 
decide when to use Student's vs. Welch's t test. We used Monte Carlo 
simulations to compare the false positive rate, power, and coverage probability 
of Student's and Welch's t tests across different variance ratios, sample size ratios, sample sizes, 
and effect sizes. We examined the simple case of a difference in the means of 
two groups and the more complex case of interactions in a 
2~$\times$~2 factorial design. We recommend the following rule: 
For normally distributed data always use Welch's t test to 
compare the means of independent groups.}
\keywords{t test, new statistics, Welch}

\authornote{Joshua D. Wondra, Facebook.

Richard Gonzalez, Department of Psychology, University of Michigan.

This research was conducted while Josh Wondra was a graduate student at the University of Michigan.

Correspondence concerning this article should be addressed to Josh Wondra, 
Facebook, 770 Broadway, New York, NY, 10003.

Contact: jdwondra@umich.edu}


\begin{document}
\SweaveOpts{concordance=TRUE}
\maketitle
<<setup, echo=FALSE>>=

# Load packages ----
library(tidyverse)
library(cowplot)


# Import simulation results ----

# two groups
load('veNeSeed2184Tables.Rdata')
load('v2NeSeed2184Tables.Rdata')
load('v5NeSeed2184Tables.Rdata')
load('ve1andhalfnSeed2184Tables.Rdata')
load('ve2nSeed2184Tables.Rdata')
load('v21andhalfnSSVSeed2184Tables.Rdata')
load('v51andhalfnSSVSeed2184Tables.Rdata')
load('v22nSSVSeed2184Tables.Rdata')
load('v52nSSVSeed2184Tables.Rdata')
load('v21andhalfnBSVSeed2184Tables.Rdata')
load('v51andhalfnBSVSeed2184Tables.Rdata')
load('v22nBSVSeed2184Tables.Rdata')
load('v52nBSVSeed2184Tables.Rdata')

## 2 x 2 interaction 
load('interaction2x2-veNeSeed2184Tables.Rdata')
load('interaction2x2-v2bigvarNeSeed2184Tables.Rdata')
load('interaction2x2-v5bigvarNeSeed2184Tables.Rdata')
load('interaction2x2-v2smallvarNeSeed2184Tables.Rdata')
load('interaction2x2-v5smallvarNeSeed2184Tables.Rdata')
load('interaction2x2-vebign1.5nSeed2184Tables.Rdata')
load('interaction2x2-vebign2nSeed2184Tables.Rdata')
load('interaction2x2-vesmalln1.5nSeed2184Tables.Rdata')
load('interaction2x2-vesmalln2nSeed2184Tables.Rdata')
load('interaction2x2-v2bigvarbign1.5nSeed2184Tables.Rdata')
load('interaction2x2-v2bigvarbign2nSeed2184Tables.Rdata')
load('interaction2x2-v2bigvarsmalln1.5nSeed2184Tables.Rdata')
load('interaction2x2-v2bigvarsmalln2nSeed2184Tables.Rdata')
load('interaction2x2-v5bigvarbign1.5nSeed2184Tables.Rdata')
load('interaction2x2-v5bigvarbign2nSeed2184Tables.Rdata')
load('interaction2x2-v5bigvarsmalln1.5nSeed2184Tables.Rdata')
load('interaction2x2-v5bigvarsmalln2nSeed2184Tables.Rdata')
load('interaction2x2-v2smallvarbign1.5nSeed2184Tables.Rdata')
load('interaction2x2-v2smallvarbign2nSeed2184Tables.Rdata')
load('interaction2x2-v2smallvarsmalln1.5nSeed2184Tables.Rdata')
load('interaction2x2-v2smallvarsmalln2nSeed2184Tables.Rdata')
load('interaction2x2-v5smallvarbign1.5nSeed2184Tables.Rdata')
load('interaction2x2-v5smallvarbign2nSeed2184Tables.Rdata')
load('interaction2x2-v5smallvarsmalln1.5nSeed2184Tables.Rdata')
load('interaction2x2-v5smallvarsmalln2nSeed2184Tables.Rdata')




# Prep data (two groups) ----
two_groups <- bind_rows(
  as.data.frame.table(reject.null.ve.ne),
  as.data.frame.table(reject.null.v2.ne),
  as.data.frame.table(reject.null.v2.ne), # duplicate symmetric conditions
  as.data.frame.table(reject.null.v5.ne),
  as.data.frame.table(reject.null.v5.ne), # duplicate symmetric conditions
  as.data.frame.table(reject.null.ve.1.5n),
  as.data.frame.table(reject.null.ve.2n),
  as.data.frame.table(reject.null.v2.1.5n.ssv),
  as.data.frame.table(reject.null.v2.1.5n.bsv),
  as.data.frame.table(reject.null.v2.2n.ssv),
  as.data.frame.table(reject.null.v2.2n.bsv),
  as.data.frame.table(reject.null.v5.1.5n.ssv),
  as.data.frame.table(reject.null.v5.1.5n.bsv),
  as.data.frame.table(reject.null.v5.2n.ssv),
  as.data.frame.table(reject.null.v5.2n.bsv)
) %>% 
  mutate(
    var.ratio = rep(c(1, 2, 1/2, 5, 1/5, 1, 1, 1/2, 2, 1/2, 2, 1/5, 5, 1/5, 5), each = 24),
    n.ratio = rep(c(1, 1, 1, 1, 1, 1.5, 2, 1.5, 1.5, 2, 2, 1.5, 1.5, 2, 2), each = 24),
    case = 'Two groups'
  ) %>% 
  rename(
    small_n = Var1,
    effect = Var2,
    test = Var3,
    rejects = Freq
  ) %>% 
  mutate(
    Ns = map_chr(small_n, ~ str_extract_all(., '[0-9]+', simplify = TRUE) %>% 
               paste(collapse = ', ') %>% 
               ifelse(str_detect(., ','), ., paste(., ., sep = ', '))
             ) %>% 
      factor(levels = c('20, 20', '20, 30', '20, 40', '50, 50', '50, 75', '50, 100', '100, 100', '100, 150', '100, 200')),
    small_ns = str_extract(Ns, '[0-9]+'),
    test = recode(test, classic = "Student's t test", welch = "Welch's t test")
  )

## Set up data - interaction

cross_int <- bind_rows(
  as.data.frame.table(int.reject.null.ve.ne),
  as.data.frame.table(int.reject.null.ve.1.5n.smalln),
  as.data.frame.table(int.reject.null.ve.1.5n.bign),
  as.data.frame.table(int.reject.null.ve.2n.smalln),
  as.data.frame.table(int.reject.null.ve.2n.bign),
  as.data.frame.table(int.reject.null.v2.ne.bigvar),
  as.data.frame.table(int.reject.null.v2.ne.smallvar),
  as.data.frame.table(int.reject.null.v5.ne.bigvar),
  as.data.frame.table(int.reject.null.v5.ne.smallvar),
  as.data.frame.table(int.reject.null.v2.1.5n.bign.bigvar),
  as.data.frame.table(int.reject.null.v2.1.5n.bign.smallvar),
  as.data.frame.table(int.reject.null.v2.1.5n.smalln.bigvar),
  as.data.frame.table(int.reject.null.v2.1.5n.smalln.smallvar),
  as.data.frame.table(int.reject.null.v2.2n.bign.bigvar),
  as.data.frame.table(int.reject.null.v2.2n.bign.smallvar),
  as.data.frame.table(int.reject.null.v2.2n.smalln.bigvar),
  as.data.frame.table(int.reject.null.v2.2n.smalln.smallvar),
  as.data.frame.table(int.reject.null.v5.1.5n.bign.bigvar),
  as.data.frame.table(int.reject.null.v5.1.5n.bign.smallvar),
  as.data.frame.table(int.reject.null.v5.1.5n.smalln.bigvar),
  as.data.frame.table(int.reject.null.v5.1.5n.smalln.smallvar),
  as.data.frame.table(int.reject.null.v5.2n.bign.bigvar),
  as.data.frame.table(int.reject.null.v5.2n.bign.smallvar),
  as.data.frame.table(int.reject.null.v5.2n.smalln.bigvar),
  as.data.frame.table(int.reject.null.v5.2n.smalln.smallvar)
) %>% 
  rename(
    small_n = Var1,
    effect = Var2,
    test = Var3,
    rejects = Freq
  ) %>% 
  mutate(
    var.ratio = rep(c(1, 1, 1, 1, 1, 2, 1/2, 5, 1/5, 2, 1/2, 2, 1/2, 2, 1/2, 2, 1/2, 5, 1/5, 5, 1/5, 5, 1/5, 5, 1/5), each = 12),
    n.ratio = rep(c(1, 1/1.5, 1.5, 1/2, 2, 1, 1, 1, 1, 1.5, 1.5, 1/1.5, 1/1.5, 2, 2, 1/2, 1/2, 1.5, 1.5, 1/1.5, 1/1.5, 2, 2, 1/2, 1/2), each = 12),
    Ns = case_when(
      n.ratio == 1 & small_n == 'N=20' ~ '20, 20, 20, 20',
      n.ratio == 1 & small_n == 'N=50' ~ '50, 50, 50, 50',
      n.ratio == 1 & small_n == 'N=100' ~ '100, 100, 100, 100',
      n.ratio == 1.5 & small_n == 'N=20' ~ '30, 20, 20, 20',
      n.ratio == 1.5 & small_n == 'N=50' ~ '75, 50, 50, 50',
      n.ratio == 1.5 & small_n == 'N=100' ~ '150, 100, 100, 100',
      n.ratio == 1/1.5 & small_n == 'N=20' ~ '20, 30, 30, 30',
      n.ratio == 1/1.5 & small_n == 'N=50' ~ '50, 75, 75, 75',
      n.ratio == 1/1.5 & small_n == 'N=100' ~ '100, 150, 150, 150',
      n.ratio == 2 & small_n == 'N=20' ~ '40, 20, 20, 20',
      n.ratio == 2 & small_n == 'N=50' ~ '100, 50, 50, 50',
      n.ratio == 2 & small_n == 'N=100' ~ '200, 100, 100, 100',
      n.ratio == 1/2 & small_n == 'N=20' ~ '20, 40, 40, 40',
      n.ratio == 1/2 & small_n == 'N=50' ~ '50, 100, 100, 100',
      n.ratio == 1/2 & small_n == 'N=100' ~ '100, 200, 200, 200'
    ) %>% 
      factor(levels = c('20, 20, 20, 20',
                        '30, 20, 20, 20',
                        '40, 20, 20, 20',
                        '20, 30, 30, 30',
                        '20, 40, 40, 40',
                        '50, 50, 50, 50',
                        '75, 50, 50, 50',
                        '100, 50, 50, 50',
                        '50, 75, 75, 75',
                        '50, 100, 100, 100',
                        '100, 100, 100, 100',
                        '150, 100, 100, 100',
                        '200, 100, 100, 100',
                        '100, 150, 150, 150',
                        '100, 200, 200, 200')),
    case = 'Crossover interaction',
    small_ns = str_extract(Ns, '[0-9]+'),
    test = recode(test, classic = "Student's t test", welch = "Welch's t test")
  )

# Set ggplot theme ----
theme_set(theme_bw())

@
    Data analysis involves decisions about which statistical test answers the research question, whether the test is 
suitable to the data, and whether there is a more appropriate test. Recent discussions of research practices in psychology 
\cite<e.g.,>{Fiedler2012, Simmons2011, 
Wagenmakers2012} highlight the tension between two valued outcomes of these 
decisions. On the one hand, researchers want to avoid mistakenly 
claiming there is a true effect where none exists, which involves a 
concern with false positives. On the other hand, researchers want to find 
effects where they do exist, which involves a concern with power. In addition,
there is a growing concern with estimating effect sizes 
\cite{Bakker2012, Cumming2014, Ioannidis2008}. 
Researchers must find a way to balance these concerns as they make their decisions. 

The recent push for pre-registration forces researchers to make these decisions prior to data collection by specifying how they plan to test their hypothesis (e.g., a two-sample t test). But they should also address the assumptions of those tests (e.g., equal variances), how they will check if those assumptions are violated (e.g., a significance test of two variances), and what they will do in that case (e.g., use a different test).  Such planning can be good for science, but it can easily become overwhelming to write a long list of conditional statements: ``if I see this, then I'll do that.''  Preregistration can be simplified if the researcher commits to a statistical procedure that is robust to violations of assumptions, and all the better if the procedure doesn't introduce new costs, such as lower power. We propose that when researchers compare the means of independent groups, they should commit to Welch's t test, which is robust when variances are unequal, instead of Student's t test or ANOVAs. 

\subsection{The Comparison of Means from Independent Groups}
    One of the first decisions many researchers learn is how to compare 
the means of two independent groups---they run a t test. But even this basic 
comparison involves a choice between the classic Student's t test 
\cite{Student1908} or the alternative Welch-Satterthwaite test \cite{Welch1938, 
Satterthwaite1946}. When you use Student's t test to compare 
the means of independent groups, you assume the populations follow a normal distribution,
observations are independent of each other, and the populations have the same variance.  
    
The t-value is the 
difference in group means divided by the standard error of that 
difference:   
    \begin{equation}
    t = \frac{\hat{\mu}_1-\hat{\mu}_2}{\hat{\sigma}_{\mu_1-\mu_2}}
    \end{equation}
    The p-value then depends on the degrees of 
freedom (df), $df=n_1+n_2-2$. You are more likely to reject the null 
hypothesis and conclude there is a difference between the means
as the t-value and 
df get larger---when 
the sample size gets larger (which increases the df), when the 
difference in means gets larger (which increases the numerator of t), or 
when the standard error gets smaller (which decreases the denominator of t). 
    
    To compute the standard error, you need to find the 
common variance of the groups which is 
equal to the population variance of each group 
if the equal variances assumption is true. 
In practice, you rarely know the population variances of the groups, and 
instead you estimate them from the sample variances. 
Unfortunately, even if the population variances are equal, the sample 
variances are rarely equal due to sampling error. Student's t test deals with this 
problem by pooling the sample variances of the groups into a weighted 
average to estimate the common variance. 
    \begin{equation}
    s_p^2 = \frac{(n_1 - 1)s_1^2 + (n_2 - 1)s_2^2}{n_1 + n_2 - 2}
    \end{equation}
    \begin{equation}
    s_{\overline{x}_1-\overline{x}_2} = \sqrt{\frac{s_p^2}{n_1}  + \frac{s_p^2}{n_2}} = s_p\sqrt{\frac{1}{n_1}  + \frac{1}{n_2}}
    \end{equation}
    with $s_1^2$ and $s_2^2$ being the two sample variances.
In the pooling equation (EQ 2), if the sample sizes are different, the variance of the group 
with the larger sample size is given more weight. So the standard error is larger when the larger group has the larger 
variance, and smaller when the larger group has the 
smaller variance.

    If the data or study design suggests the assumptions of 
normality, independence, and equal variances do not hold, then Student's t test 
is not the right choice. If the group 
variances are unequal, then the Welch-Satterthwaite t test (
Welch's t test for the sake of brevity) is a good alternative.  In SPSS, Welch's t test is in the ``Equal 
variances not assumed'' row when you run an independent samples t 
test. In R, it's the default when you use the
\texttt{t.test()} function, and you can get Student's t test by setting 
the \texttt{var.equal} argument to \texttt{TRUE}.
    
        Like Student's t test, Welch's t test assumes normality and 
independence; however, it does not assume equal variances. The standard error is based on separate group variances instead of a 
common variance:
    \begin{equation}
    s_{\overline{x}_1-\overline{x}_2} = \sqrt{\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2}}
    \end{equation}
Additionally, Welch's t test uses a complicated formula to compute the df: 
    \begin{equation}
    df = \frac{(\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2})^2}{\frac{(s_1^2)^2}{n_1^2(n_1-1)} + \frac{(s_2^2)^2}{n_2^2(n_2-1)}}
    \end{equation}
The important thing to know is the formula decreases the df to the extent that the group
variances differ. It can also produce a df value that is not an integer.

Due to these differences in the standard errors and df, Student's and Welch's t tests can disagree about whether the group means are different. Because Welch's t test decreases the df, it might be more conservative. If Student's t test finds 
too many false positives when variances are unequal, this could make Welch's t test the better choice. But if Welch's t test is not powerful enough to detect true 
effects, this could 
make it the worse choice. 
        
    However, Welch's t test might not always be more conservative. The power of 
the two tests is not only based on the df, but also on the 
standard error, and Welch's t test could be more powerful than 
Student's when its standard error is smaller.
    
    How do you decide which test to use? The typical approach is to use 
Student's t test unless you have evidence that the variances are unequal. The challenge is how to find that evidence. 
Our goal is to find a simple strategy for researchers to decide which test to use. We discuss two decision strategies in the main text---using a statistical test of the equal variances assumption and visually examining the data with boxplots---and one decision strategy in the supplemental materials---comparing the df of the two t tests. Then we use Monte Carlo simulations to empirically determine when each test performs best.

    Before we proceed, we note that aside from Welch's t test, there have been Bayesian approaches 
to comparing groups with equal or unequal variances \cite{Box1973, Kruschke2013}. Our purpose is to inform decisions about how to use 
frequentist approaches, so we do not discuss
Bayesian approaches further.

\subsection{Testing Assumptions with Another Test}
    One way to decide which test to use
is to run another test of the null hypothesis that the group variances 
are equal. If the test retains the null, the variances are equal and you can use Student's t test. If the test rejects the null, the variances are unequal and you should use Welch's t test.

One example of this approach is Levene's test for homogeneity \cite{Levene1960}, which appears in SPSS by default when you run an independent samples t test.
In its original formulation, the test finds the absolute value of the deviations of each observation from its group mean, then it uses a oneway ANOVA to test whether the average deviations are different between the groups.
Later formulations use deviations from the group median or the trimmed mean, which work better when the raw data are not normally distributed \cite{Brown1974}.

Consider the logic of using Levene's test to decide whether to use Student's or Welch's t 
test. You use an ANOVA to test 
whether the mean deviations are different.
When there are only two groups, this is equivalent to a two sample t test, which means you have to make another choice
between Student and Welch, which means you must decide whether the
group \textit{deviations} have equal variances.
This doesn't solve the original problem, it just hides it in a different test. 

Additionally, tests of assumptions,
like other tests, are sensitive 
to sample size \cite{Gonzalez2008}. If your sample is too small, you won't have enough power to 
detect true differences in the variances. If your sample is 
large, even minute differences will be statistically significant. 

Overall, using tests of equal variances is an ineffective decision strategy, as confirmed by simulation 
studies \cite{Zimmerman1996,Zimmerman2004}. 

\subsection{Visualizing Data with Boxplots} 

    A third way to decide which test to use is to visualize the data using boxplots and make a judgment 
about whether the variances appear to differ.  With smaller samples, you 
can tolerate larger apparent differences and still conclude the variances are equal. You can enhance this strategy by 
simulating data that have the sample sample sizes as the real data, changing whether the variances are equal or not, 
and checking if the boxplots from the real data look like the boxplots of 
simulations with equal variances. 

    Figure 1 displays boxplots from groups with equal 
variances (top) and where one group has five times the variance of the other (bottom), and where each group has $n$ = 20 (first rows) or $n$ = 
100 (second rows). 

  When the 
sample sizes are smaller it's normal to see 
variability in the spread of the groups. When the population 
variances are equal,
sometimes the spread of the groups appears almost equal (as it should), 
and other times it appears to differ. When the population variances are unequal, the group to the right is generally more spread out, but sometimes the spread of the groups looks similar.

In contrast, when sample sizes are larger there is more 
consistency in the boxplots Smaller differences in the spread of the groups 
might be a sign that the population variances differ. Using boxplots
to decide which test to use
is a viable strategy, though it requires some subjective judgment,
especially when sample sizes are small.

   
\begin{figure}[!ht]
<<boxplots, echo = FALSE, fig = TRUE>>=

# Boxplots (equal variances) ----

# distributions
x1 <- seq(0, 12, .1)
x2 <- seq(1.13, 13.13, .1)

ve_dist_data <- data.frame(
  x = c(x1, x2),
  y = c(dnorm(x1, mean = 6, sd = sqrt(2)), dnorm(x2, mean = 7.13, sd = sqrt(2))),
  group = factor(rep(c(1, 2), each = length(x1)))
)

ve_distributions <- ggplot(ve_dist_data, aes(x = x, y = y, linetype = group)) +
  geom_line(show.legend = FALSE) +
  labs(y = NULL, x = NULL) +
  theme(
    axis.text = element_blank(),
    axis.ticks = element_blank(),
    panel.grid = element_blank()
  )

# boxplots
set.seed(2184)
ve_bplots_n20 <- lapply(1:4, function(x) {
                 plot_data <- data.frame(
                   group = factor(rep(c(1, 2), each = 20)),
                   y = c(
                     rnorm(n = 20, mean = 6, sd = sqrt(2)),
                     rnorm(n = 20, mean = 7.13, sd = sqrt(2))
                   )
                 )
                 
                 ggplot(plot_data, aes(x = group, y = y, linetype = group)) +
                   geom_boxplot(show.legend = FALSE) +
                   labs(x = NULL, y = NULL) +
                   theme(
                     axis.text = element_blank(),
                     axis.ticks = element_blank(),
                     panel.grid = element_blank()
                   )
})

ve_bplots_n100 <- lapply(1:4, function(x) {
                 plot_data <- data.frame(
                   group = factor(rep(c(1, 2), each = 100)),
                   y = c(
                     rnorm(n = 100, mean = 6, sd = sqrt(2)),
                     rnorm(n = 100, mean = 7.13, sd = sqrt(2))
                   )
                 )
                 
                 ggplot(plot_data, aes(x = group, y = y, linetype = group)) +
                   geom_boxplot(show.legend = FALSE) +
                   labs(x = NULL, y = NULL) +
                   theme(
                     axis.text = element_blank(),
                     axis.ticks = element_blank(),
                     panel.grid = element_blank()
                   )
})



# Boxplots (unequal variances) ----

# distributions
x1 <- seq(0, 12, .1)
x2 <- seq(1.13, 13.13, .1)

vun_dist_data <- data.frame(
  x = c(x1, x2),
  y = c(dnorm(x1, mean = 6, sd = sqrt(2)), dnorm(x2, mean = 7.13, sd = sqrt(10))),
  group = factor(rep(c(1, 2), each = length(x1)))
)

vun_distributions <- ggplot(vun_dist_data, aes(x = x, y = y, linetype = group)) +
  geom_line(show.legend = FALSE) +
  labs(y = NULL, x = NULL) +
  theme(
    axis.text = element_blank(),
    axis.ticks = element_blank(),
    panel.grid = element_blank()
  )

# boxplots
set.seed(2184)
vun_bplots_n20 <- lapply(1:4, function(x) {
                 plot_data <- data.frame(
                   group = factor(rep(c(1, 2), each = 20)),
                   y = c(
                     rnorm(n = 20, mean = 6, sd = sqrt(2)),
                     rnorm(n = 20, mean = 7.13, sd = sqrt(10))
                   )
                 )
                 
                 ggplot(plot_data, aes(x = group, y = y, linetype = group)) +
                   geom_boxplot(show.legend = FALSE) +
                   labs(x = NULL, y = NULL) +
                   theme(
                     axis.text = element_blank(),
                     axis.ticks = element_blank()
                   )
})

vun_bplots_n100 <- lapply(1:4, function(x) {
                 plot_data <- data.frame(
                   group = factor(rep(c(1, 2), each = 100)),
                   y = c(
                     rnorm(n = 100, mean = 6, sd = sqrt(2)),
                     rnorm(n = 100, mean = 7.13, sd = sqrt(10))
                   )
                 )
                 
                 ggplot(plot_data, aes(x = group, y = y, linetype = group)) +
                   geom_boxplot(show.legend = FALSE) +
                   labs(x = NULL, y = NULL) +
                   theme(
                     axis.text = element_blank(),
                     axis.ticks = element_blank()
                   )
})

# Print boxplots ---- 

# prep for printing
ve_distributions <- plot_grid(NULL, ve_distributions, NULL, rel_widths = c(1, 2, 1), nrow = 1)
ve_bplots <- plot_grid(plotlist = c(ve_bplots_n20, ve_bplots_n100), nrow = 2)
vun_distributions <- plot_grid(NULL, vun_distributions, NULL, rel_widths = c(1, 2, 1), nrow = 1)
vun_bplots <- plot_grid(plotlist = c(vun_bplots_n20, vun_bplots_n100), nrow = 2)

# combine into one grid
full_bplots <- plot_grid(ve_distributions, ve_bplots, vun_distributions, vun_bplots, nrow = 4)

full_bplots
@

\textit{Figure 1.} Boxplots for groups with equal variances (top) and unequal 
variances (bottom). For each set of boxplots, the first row displays groups 
with $n$=20 and the second row displays groups with $n$=100.
\end{figure}

\subsection{When Does Each Test Perform Best?}

    So far, we have been following the standard procedure---use Student's t test 
by default and  switch to Welch's t test if the variances 
are unequal. However, this approach might be
unwarranted. When the variances and sample sizes are equal, the two tests are equivalent. If Welch's t test generally comes to similar or better 
decisions than Student's, whether the variances and sample sizes are equal or not, then 
it would be better to always use 
Welch's t test. 

To investigate this possibility, we ran simulations of two independent groups 
with normally distributed data.  But the use of t tests can also be expanded to study designs with more than two groups, where they can be used to test main effects, interactions, or other specific contrasts. So we also examined the more complex case of a 2~$\times$~2 between-subjects design with a crossover action but no main effects.

We examined how well each test balances concerns with 
false positives, power, and effect size estimation under 
different variance ratios, sample sizes, sample size ratios, and effect sizes. 
For the 2~$\times$~2 design we varied the variance and sample size of just one group. 
Sample size did not affect the comparative performance of the two tests, so in the main text we only show cases where the smallest sample was 50. The supplemental materials have cases where the smallest sample was 20 or 100 as well.  
Some prior research has examined the 
false positive rate \cite{Boneau1960, Zimmerman1993, 
Zimmerman2004, Zimmerman1996, Zimmerman2009} and
power of the two tests when comparing two groups, though not with the complete configuration of 
conditions that we examined \cite{Neuhauser2002, 
Zimmerman1993}. Nevertheless, it will be informative to display 
the false positives and power of the two tests here. We also discuss 
implications for estimating effect sizes, which follows from the false positive
results but has not, to our knowledge, been discussed explicitly in past 
research.

    
\subsubsection{False Positive Rates}

    The expected false positive rate is $\alpha = .05$. The observed false positive 
rate for Student's t test remained close to the expected rate when either 
the population variances or sample sizes were equal, but it varied widely 
when both the variances and sample sizes were unequal (see Figure 2). 
When the larger group(s) had the larger variance, the false positive rate dropped as low as 
about .01, but when it had the smaller 
variance, the false positive rate 
rose as high as .13, more than double the expected rate. 
In contrast, the observed false positive rate for Welch's t test 
remained close to the expected rate across all conditions. Overall, Welch's 
t test consistently behaved as expected when it came to false positives. 
Student's t test did not.

<<false positive plots, echo = FALSE>>=

# False pos plot (two groups) ----
falsepos_two <- ggplot(filter(two_groups, effect == 'd=0', small_ns == 50), 
       aes(x = as.factor(var.ratio), y = rejects, ymin=0, ymax=.13, group = Ns, shape = Ns, linetype = Ns)) + 
    geom_point(size = 4) + 
    scale_shape_manual(name = 'Sample sizes \nGroup 1, 2',
                       labels = c('50, 50', '50, 75', '50, 100'),
                       values = rep(c(15, 16, 17), times = 3)) +
    geom_line(size = .5) + 
    scale_linetype_manual(name = 'Sample sizes \nGroup 1, 2',
                          labels = c('50, 50', '50, 75', '50, 100'),
                          values = rep('solid', each = 9)
                          ) +
    scale_x_discrete(labels = factor(c('1/5', '1/2', '1', '2', '5'))) +
    scale_y_continuous(breaks = c(0, .05, .10), minor_breaks = seq(0, .13, .01)) +
    labs(
      x = bquote(frac(sigma[1]^2, sigma[2]^2)),
      y = 'False Positive Rate',
      title = 'Two groups'
      ) +
    facet_grid(. ~ test) +
    theme(
      panel.grid = element_blank(),
#      panel.grid.minor.y = element_line(color = 'gray90'),
      panel.grid.major.y = element_line(color = 'gray80')
      )


# False pos plot (interaction, one group has small n) ----
falsepos_int_smalln <- ggplot(filter(cross_int, effect == 'd=0', !(n.ratio %in% c(1.5, 2)), small_ns == 50), 
       aes(x = as.factor(var.ratio), y = rejects, ymin=0, ymax=.13, group = Ns, shape = Ns, linetype = Ns)) + 
    geom_point(size = 4) + 
    scale_shape_manual(name = 'Sample sizes \nGroup 1, 2, 3, 4',
                       labels = c(
                                  '50, 50, 50, 50',
                                  '50, 75, 75, 75',
                                  '50, 100, 100, 100'
),
                       values = rep(c(15, 16, 17), times = 3)
                       ) +
    geom_line(size = .5) + 
    scale_linetype_manual(name = 'Sample sizes \nGroup 1, 2, 3, 4',
                          labels = c(
                                  '50, 50, 50, 50',
                                  '50, 75, 75, 75',
                                  '50, 100, 100, 100'
),
                          values = rep('solid', each = 9)
                          ) +
    scale_x_discrete(labels = factor(c('1/5', '1/2', '1', '2', '5'))) +
    scale_y_continuous(breaks = c(0, .05, .10), minor_breaks = seq(0, .13, .01)) +
    labs(
      x = bquote(frac(sigma[1]^2,{sigma[2]^2 == sigma[3]^2} == sigma[4]^2)),
      y = 'False Positive Rate',
      title = 'Crossover interaction - Group 1 has small sample'
      ) +
    facet_grid(. ~ test) +
    theme(
      panel.grid = element_blank(),
#      panel.grid.minor.y = element_line(color = 'gray90'),
      panel.grid.major.y = element_line(color = 'gray80')
      )

# False pos plot (interaction, one group has large n) ----
falsepos_int_bign <- ggplot(filter(cross_int, effect == 'd=0', !(n.ratio %in% c(1/1.5, 1/2)), small_n == 'N=50'), 
       aes(x = as.factor(var.ratio), y = rejects, ymin=0, ymax=.13, group = Ns, shape = Ns, linetype = Ns)) + 
    geom_point(size = 4) + 
    scale_shape_manual(name = 'Sample sizes \nGroup 1, 2, 3, 4',
                       labels = c(
                                  '50, 50, 50, 50',
                                  '75, 50, 50, 50',
                                  '100, 50, 50, 50'
                                  ),
                       values = rep(c(15, 16, 17), times = 3)
                       ) +
    geom_line(size = .5) + 
    scale_linetype_manual(name = 'Sample sizes \nGroup 1, 2, 3, 4',
                          labels = c(
                                  '50, 50, 50, 50',
                                  '75, 50, 50, 50',
                                  '100, 50, 50, 50'
                                  ),
                          values = rep('solid', each = 9)
                          ) +
    scale_x_discrete(labels = factor(c('1/5', '1/2', '1', '2', '5'))) +
    scale_y_continuous(breaks = c(0, .05, .10), minor_breaks = seq(0, .13, .01)) +
    labs(
      x = bquote(frac(sigma[1]^2,{sigma[2]^2 == sigma[3]^2} == sigma[4]^2)),
      y = 'False Positive Rate',
      title = 'Crossover interaction - Group 1 has large sample'
      ) +
    facet_grid(. ~ test) +
    theme(
      panel.grid = element_blank(),
#      panel.grid.minor.y = element_line(color = 'gray90'),
      panel.grid.major.y = element_line(color = 'gray80')
      )


# Print false pos plots ----
plottype1 <- plot_grid(falsepos_two, falsepos_int_smalln, falsepos_int_bign, nrow = 3)


@

\begin{figure}[!ht]  

<<FalsePositivePlots, echo = FALSE, collapse = TRUE, fig = TRUE, width = 8, height = 11>>=
plottype1
@

\textit{Figure 2.} False positive rates for Student's t test and Welch's t test 
as the variance ratio, sample size ratio, and sample sizes vary.
\end{figure}


\subsubsection{Power}

<<power plots, echo = FALSE>>=

# Power plot (two groups) ----
power_two <- ggplot(filter(two_groups, effect == 'd=.5', small_ns == 50), 
       aes(x = as.factor(var.ratio), y = rejects, ymin = 0, ymax = 1, group = Ns, shape = Ns, linetype = Ns)) + 
    geom_point(size = 4) + 
    scale_shape_manual(name = 'Sample sizes \nGroup 1, 2',
                       labels = c('20, 20', '20, 30', '20, 40',
                                     '50, 50', '50, 75', '50, 100',
                                     '100, 100', '100, 150', '100, 200'),
                       values = rep(c(15, 16, 17), times = 3)) +
    geom_line(size = .5) + 
    scale_linetype_manual(name = 'Sample sizes \nGroup 1, 2',
                          labels = c('20, 20', '20, 30', '20, 40',
                                     '50, 50', '50, 75', '50, 100',
                                     '100, 100', '100, 150', '100, 200'),
                          values=rep(c('solid'), each = 3)
                          ) +
    scale_x_discrete(labels = factor(c('1/5', '1/2', '1', '2', '5'))) +
    scale_y_continuous(breaks = seq(0, 1, .10), minor_breaks = seq(0, 1, .01)) +
    labs(
      x = bquote(frac(sigma[1]^2, sigma[2]^2)),
      y = 'Power',
      title = 'Two groups'
      ) +
    facet_grid(. ~ test) +
    theme(
      panel.grid = element_blank(),
#      panel.grid.minor.y = element_line(color = 'gray90'),
      panel.grid.major.y = element_line(color = 'gray80')
      )


# Power plot (interaction, one group has small n) ----
power_int_smalln <- ggplot(filter(cross_int, effect == 'd=.5', !(n.ratio %in% c(1.5, 2)), small_n == 'N=50'), 
       aes(x = as.factor(var.ratio), y = rejects, ymin = 0, ymax = 1, group = Ns, shape = Ns, linetype = Ns)) + 
    geom_point(size = 4) + 
    scale_shape_manual(name = 'Sample sizes \nGroup 1, 2, 3, 4',
                       labels = c('20, 20, 20, 20',
                                  '20, 30, 30, 30',
                                  '20, 40, 40, 40',
                                  '50, 50, 50, 50',
                                  '50, 75, 75, 75',
                                  '50, 100, 100, 100',
                                  '100, 100, 100, 100',
                                  '100, 150, 150, 150',
                                  '100, 200, 200, 200'),
                       values = rep(c(15, 16, 17), times = 3)
                       ) +
    geom_line(size = .5) + 
    scale_linetype_manual(name = 'Sample sizes \nGroup 1, 2, 3, 4',
                          labels = c('20, 20, 20, 20',
                                  '20, 30, 30, 30',
                                  '20, 40, 40, 40',
                                  '50, 50, 50, 50',
                                  '50, 75, 75, 75',
                                  '50, 100, 100, 100',
                                  '100, 100, 100, 100',
                                  '100, 150, 150, 150',
                                  '100, 200, 200, 200'),
                          values = rep(c('solid'), each = 3)
                          ) +
    scale_x_discrete(labels = factor(c('1/5', '1/2', '1', '2', '5'))) +
    scale_y_continuous(breaks = seq(0, 1, .10), minor_breaks = seq(0, 1, .01)) +
    labs(
      x = bquote(frac(sigma[1]^2,{sigma[2]^2 == sigma[3]^2} == sigma[4]^2)),
      y = 'Power',
      title = 'Crossover interaction - Group 1 has small sample'
      ) +
    facet_grid(. ~ test) +
    theme(
      panel.grid = element_blank(),
#      panel.grid.minor.y = element_line(color = 'gray90'),
      panel.grid.major.y = element_line(color = 'gray80')
      )

# Power plot (interaction, one group has large n) ----
power_int_bign <- ggplot(filter(cross_int, effect == 'd=.5', !(n.ratio %in% c(1/1.5, 1/2)), small_n == 'N=50'), 
       aes(x = as.factor(var.ratio), y = rejects, ymin = 0, ymax = 1, group = Ns, shape = Ns, linetype = Ns)) + 
    geom_point(size = 4) + 
    scale_shape_manual(name = 'Sample sizes \nGroup 1, 2, 3, 4',
                       labels = c('20, 20, 20, 20',
                                  '30, 20, 20, 20',
                                  '40, 20, 20, 20',
                                  '50, 50, 50, 50',
                                  '75, 50, 50, 50',
                                  '100, 50, 50, 50',
                                  '100, 100, 100, 100',
                                  '150, 100, 100, 100',
                                  '200, 100, 100, 100'),
                       values = rep(c(15, 16, 17), times = 3)
                       ) +
    geom_line(size = .5) + 
    scale_linetype_manual(name = 'Sample sizes \nGroup 1, 2, 3, 4',
                          labels = c('20, 20, 20, 20',
                                  '30, 20, 20, 20',
                                  '40, 20, 20, 20',
                                  '50, 50, 50, 50',
                                  '75, 50, 50, 50',
                                  '100, 50, 50, 50',
                                  '100, 100, 100, 100',
                                  '150, 100, 100, 100',
                                  '200, 100, 100, 100'),
                          values = rep(c('solid'), each = 3)
                          ) +
    scale_x_discrete(labels = factor(c('1/5', '1/2', '1', '2', '5'))) +
    scale_y_continuous(breaks = seq(0, 1, .10), minor_breaks = seq(0, 1, .01)) +
    labs(
      x = bquote(frac(sigma[1]^2,{sigma[2]^2 == sigma[3]^2} == sigma[4]^2)),
      y = 'Power',
      title = 'Crossover interaction - Group 1 has large sample'
      ) +
    facet_grid(. ~ test) +
    theme(
      panel.grid = element_blank(),
#      panel.grid.minor.y = element_line(color = 'gray90'),
      panel.grid.major.y = element_line(color = 'gray80')
      )

@

<<power difference, echo = FALSE>>=

# Power difference data (two groups) ----
classic_rejects_two <- two_groups %>% 
  filter(test == "Student's t test") %>% 
  select(-test) %>% 
  rename(classic_rejects = rejects)

welch_rejects_two <- two_groups %>% 
  filter(test == "Welch's t test") %>% 
  select(-test) %>% 
  rename(welch_rejects = rejects)

power_diff_two <- full_join(x = classic_rejects_two, y = welch_rejects_two) %>% 
  filter(effect != 'd=0') %>% 
  mutate(
    effect = str_replace(effect, '=', ' = '),
    diff_rejects = classic_rejects - welch_rejects,
    test = 'Student - Welch'
    )

# Power difference data (interaction) ----
classic_rejects_int <- cross_int %>% 
  filter(test == "Student's t test") %>% 
  select(-test) %>% 
  rename(classic_rejects = rejects)

welch_rejects_int <- cross_int %>% 
  filter(test == "Welch's t test") %>% 
  select(-test) %>% 
  rename(welch_rejects = rejects)

power_diff_int <- full_join(x = classic_rejects_int, y = welch_rejects_int) %>% 
  filter(effect != 'd=0') %>% 
  mutate(
    effect = str_replace(effect, '=', ' = '),
    diff_rejects = classic_rejects - welch_rejects,
    test = 'Student - Welch'
    )

# Power difference plot (two groups) ----
diff_plot_two <- ggplot(filter(power_diff_two, effect == 'd = .5', small_ns == 50), 
       aes(x = as.factor(var.ratio), y = diff_rejects, ymin = -.3, ymax = .3, group = Ns, shape = Ns, linetype = Ns)) + 
    geom_point(size = 4) + 
    scale_shape_manual(name = 'Sample sizes \nGroup 1, 2',
                       labels = c('50, 50', '50, 75', '50, 100'),
                       values = rep(c(15, 16, 17), times = 3)) +
    geom_line(size = .5) + 
    scale_linetype_manual(name = 'Sample sizes \nGroup 1, 2',
                          labels = c('50, 50', '50, 75', '50, 100'),
                          values=rep(c('solid'), each = 3)
                          ) +
    scale_x_discrete(labels = factor(c('1/5', '1/2', '1', '2', '5'))) +
    scale_y_continuous(breaks = seq(-.30, .30, .10), minor_breaks = seq(-.30, .30, .01), labels = round(seq(-.30, .30, .10), digits = 2)) +
    labs(
      x = bquote(frac(sigma[1]^2, sigma[2]^2)),
      y = 'Difference in \nPower',
      title = ' '
      ) +
    facet_grid(. ~ test) +
    theme(
      panel.grid = element_blank(),
#      panel.grid.minor.y = element_line(color = 'gray90'),
      panel.grid.major.y = element_line(color = 'gray80')
      )


# Power difference plot (interaction, one group has small n) ----
diff_plot_int_smalln <- ggplot(filter(power_diff_int, effect == 'd = .5', !(n.ratio %in% c(1.5, 2)), small_n == 'N=50'), 
       aes(x = as.factor(var.ratio), y = diff_rejects, ymin = -.3, ymax = .3, group = Ns, shape = Ns, linetype = Ns)) + 
    geom_point(size = 4) + 
    scale_shape_manual(name = 'Sample sizes \nGroup 1, 2, 3, 4',
                       labels = c(
                                  '50, 50, 50, 50',
                                  '50, 75, 75, 75',
                                  '50, 100, 100, 100'
                                  ),
                       values = rep(c(15, 16, 17), times = 3)
                       ) +
    geom_line(size = .5) + 
    scale_linetype_manual(name = 'Sample sizes \nGroup 1, 2, 3, 4',
                          labels = c(
                                  '50, 50, 50, 50',
                                  '50, 75, 75, 75',
                                  '50, 100, 100, 100'
                                  ),
                          values = rep(c('solid'), each = 3)
                          ) +
    scale_x_discrete(labels = factor(c('1/5', '1/2', '1', '2', '5'))) +
    scale_y_continuous(breaks = seq(-.30, .30, .10), minor_breaks = seq(-.30, .30, .01), labels = round(seq(-.30, .30, .10), digits = 2)) +
    labs(
      x = bquote(frac(sigma[1]^2, sigma[2]^2)),
      y = 'Difference in \nPower',
      title = ' '
      ) +
    facet_grid(. ~ test) +
    theme(
      panel.grid = element_blank(),
#      panel.grid.minor.y = element_line(color = 'gray90'),
      panel.grid.major.y = element_line(color = 'gray80')
      )



# Power difference plot (interaction, one group has small n) ----
diff_plot_int_bign <- ggplot(filter(power_diff_int, effect == 'd = .5', !(n.ratio %in% c(1/1.5, 1/2)), small_n == 'N=50'), 
       aes(x = as.factor(var.ratio), y = diff_rejects, ymin = -.3, ymax = .3, group = Ns, shape = Ns, linetype = Ns)) + 
    geom_point(size = 4) + 
    scale_shape_manual(name = 'Sample sizes \nGroup 1, 2, 3, 4',
                       labels = c(
                                  '50, 50, 50, 50',
                                  '75, 50, 50, 50',
                                  '100, 50, 50, 50'
                                  ),
                       values = rep(c(15, 16, 17), times = 3)
                       ) +
    geom_line(size = .5) + 
    scale_linetype_manual(name = 'Sample sizes \nGroup 1, 2, 3, 4',
                          labels = c(
                                  '50, 50, 50, 50',
                                  '75, 50, 50, 50',
                                  '100, 50, 50, 50'
                                  ),
                          values = rep(c('solid'), each = 3)
                          ) +
    scale_x_discrete(labels = factor(c('1/5', '1/2', '1', '2', '5'))) +
    scale_y_continuous(breaks = seq(-.30, .30, .10), minor_breaks = seq(-.30, .30, .01), labels = round(seq(-.30, .30, .10), digits = 2)) +
    labs(
      x = bquote(frac(sigma[1]^2, sigma[2]^2)),
      y = 'Difference in \nPower',
      title = ' '
      ) +
    facet_grid(. ~ test) +
    theme(
      panel.grid = element_blank(),
#      panel.grid.minor.y = element_line(color = 'gray90'),
      panel.grid.major.y = element_line(color = 'gray80')
      )

@

\begin{figure}[!ht]
<<PowerPlots, echo = FALSE,  collapse = TRUE, fig = TRUE, width = 8, height = 11>>=

power_plots <- plot_grid(
  power_two + 
    guides(shape = FALSE, linetype = FALSE) +
    theme(axis.title.x = element_text(hjust = .9)), 
  diff_plot_two +
    theme(axis.title.x = element_text(color = 'white')), 

  power_int_smalln + 
    guides(shape = FALSE, linetype = FALSE) +
    theme(axis.title.x = element_text(hjust = 1)), 
  diff_plot_int_smalln +
    theme(axis.title.x = element_text(color = 'white')), 

  power_int_bign + guides(shape = FALSE, linetype = FALSE) +
    theme(axis.title.x = element_text(hjust = 1)), 
  diff_plot_int_bign +
    theme(axis.title.x = element_text(color = 'white')), 
  nrow = 3
  )

power_plots

@

\textit{Figure 3.} Power of Student's and Welch's t tests to detect a medium effect.
\end{figure}

    Is Welch's t test underpowered compared to Student's t test? Figure 3 
displays the power of the two tests to detect a medium (d = .5) effect
under the different conditions, as well as the difference in power, with higher numbers meaning that Student's t test is more poweful (results for small and large effect sizes are available in the supplemental materials). For both tests, power decreased as variances 
became unequal because we increased one or more groups' variances; however, 
it decreased at different rates depending on the sample size and 
variance ratios. When the sample sizes or variances were equal, power was approximately equal. However, when both the sample sizes and 
variances were unequal, there were differences. Student's t 
test was more powerful when the large sample(s) had the smaller variance, whereas 
Welch's t test was more powerful when the small sample(s) had the smaller variance. 
    
    Student's t test had the greatest power over 
Welch's when one sample size was twice the other and the large 
group(s) had the small variance. These are the same conditions where Student's t 
test more than doubled the expected false positive rate. So the advantage of Student's t test
in power is undermined by its inflated false 
positive rate. In contrast, Welch's t test was more 
powerful when the small group had the smaller variance, but it never inflated the false positive rate. 
These joint results, on the false positive rate and power, support the decision to always use Welch's t test. By using 
Welch's t test, you sometimes have greater power to detect true effects, but 
you don't have to worry about sacrificing concerns with false positives.  


\subsubsection{Coverage Probability}
We used 95\% confidence intervals constructed using Student's and Welch's t 
tests to find their relative coverage probability, the proportion of confidence intervals 
that contain the population value of the estimated parameter, which is the 
difference in group means in this case. By 
definition,  the expected coverage probability of 95\% confidence intervals  is .95. 
% Because the coverage 
% probability of a confidence interval is not influenced by the effect size, we 
% only show the coverage probabilities when the null hypothesis is true. 
Additionally, when the null hypothesis is true and $\alpha = .05$, the coverage 
probability of 95\% confidence intervals has a simple relationship with the 
false positive rate---it's the complement of the false positive rate. 

Figure 4 displays the coverage probabilities of the two t tests. 
The coverage probability for Student's t test varies dramatically, just as the 
false positive rate did.  When Student's t test is the most 
powerful, it is also the least accurate at estimating the difference in means, and what you would believe is a 95\% 
confidence interval drops to as low as an 87\% confidence interval in reality. 
In contrast, Welch's t test performs as expected and the confidence interval 
contains the true effect 95\% of the time regardless of the variance and sample 
size ratios. 

<<ClassicCoverage, echo=FALSE>>=

# Prep data (two groups) ----
two_groups <- bind_rows(
  as.data.frame.table(obs.coverage.ve.ne),
  as.data.frame.table(obs.coverage.v2.ne),
  as.data.frame.table(obs.coverage.v2.ne), # duplicate symmetric conditions
  as.data.frame.table(obs.coverage.v5.ne),
  as.data.frame.table(obs.coverage.v5.ne), # duplicate symmetric conditions
  as.data.frame.table(obs.coverage.ve.1.5n),
  as.data.frame.table(obs.coverage.ve.2n),
  as.data.frame.table(obs.coverage.v2.1.5n.ssv),
  as.data.frame.table(obs.coverage.v2.1.5n.bsv),
  as.data.frame.table(obs.coverage.v2.2n.ssv),
  as.data.frame.table(obs.coverage.v2.2n.bsv),
  as.data.frame.table(obs.coverage.v5.1.5n.ssv),
  as.data.frame.table(obs.coverage.v5.1.5n.bsv),
  as.data.frame.table(obs.coverage.v5.2n.ssv),
  as.data.frame.table(obs.coverage.v5.2n.bsv)
) %>% 
  mutate(
    var.ratio = rep(c(1, 2, 1/2, 5, 1/5, 1, 1, 1/2, 2, 1/2, 2, 1/5, 5, 1/5, 5), each = 24),
    n.ratio = rep(c(1, 1, 1, 1, 1, 1.5, 2, 1.5, 1.5, 2, 2, 1.5, 1.5, 2, 2), each = 24),
    case = 'Two groups'
  ) %>% 
  rename(
    small_n = Var1,
    effect = Var2,
    test = Var3,
    rejects = Freq
  ) %>% 
  mutate(
    Ns = map_chr(small_n, ~ str_extract_all(., '[0-9]+', simplify = TRUE) %>% 
               paste(collapse = ', ') %>% 
               ifelse(str_detect(., ','), ., paste(., ., sep = ', '))
             ) %>% 
      factor(levels = c('20, 20', '20, 30', '20, 40', '50, 50', '50, 75', '50, 100', '100, 100', '100, 150', '100, 200')),
    small_ns = str_extract(Ns, '[0-9]+'),
    test = recode(test, classic = "Student's t test", welch = "Welch's t test")
  )

# Prep data (interaction) ----

cross_int <- bind_rows(
  as.data.frame.table(int.obs.coverage.ve.ne),
  as.data.frame.table(int.obs.coverage.ve.1.5n.smalln),
  as.data.frame.table(int.obs.coverage.ve.1.5n.bign),
  as.data.frame.table(int.obs.coverage.ve.2n.smalln),
  as.data.frame.table(int.obs.coverage.ve.2n.bign),
  as.data.frame.table(int.obs.coverage.v2.ne.bigvar),
  as.data.frame.table(int.obs.coverage.v2.ne.smallvar),
  as.data.frame.table(int.obs.coverage.v5.ne.bigvar),
  as.data.frame.table(int.obs.coverage.v5.ne.smallvar),
  as.data.frame.table(int.obs.coverage.v2.1.5n.bign.bigvar),
  as.data.frame.table(int.obs.coverage.v2.1.5n.bign.smallvar),
  as.data.frame.table(int.obs.coverage.v2.1.5n.smalln.bigvar),
  as.data.frame.table(int.obs.coverage.v2.1.5n.smalln.smallvar),
  as.data.frame.table(int.obs.coverage.v2.2n.bign.bigvar),
  as.data.frame.table(int.obs.coverage.v2.2n.bign.smallvar),
  as.data.frame.table(int.obs.coverage.v2.2n.smalln.bigvar),
  as.data.frame.table(int.obs.coverage.v2.2n.smalln.smallvar),
  as.data.frame.table(int.obs.coverage.v5.1.5n.bign.bigvar),
  as.data.frame.table(int.obs.coverage.v5.1.5n.bign.smallvar),
  as.data.frame.table(int.obs.coverage.v5.1.5n.smalln.bigvar),
  as.data.frame.table(int.obs.coverage.v5.1.5n.smalln.smallvar),
  as.data.frame.table(int.obs.coverage.v5.2n.bign.bigvar),
  as.data.frame.table(int.obs.coverage.v5.2n.bign.smallvar),
  as.data.frame.table(int.obs.coverage.v5.2n.smalln.bigvar),
  as.data.frame.table(int.obs.coverage.v5.2n.smalln.smallvar)
) %>% 
  rename(
    small_n = Var1,
    effect = Var2,
    test = Var3,
    rejects = Freq
  ) %>% 
  mutate(
    var.ratio = rep(c(1, 1, 1, 1, 1, 2, 1/2, 5, 1/5, 2, 1/2, 2, 1/2, 2, 1/2, 2, 1/2, 5, 1/5, 5, 1/5, 5, 1/5, 5, 1/5), each = 12),
    n.ratio = rep(c(1, 1/1.5, 1.5, 1/2, 2, 1, 1, 1, 1, 1.5, 1.5, 1/1.5, 1/1.5, 2, 2, 1/2, 1/2, 1.5, 1.5, 1/1.5, 1/1.5, 2, 2, 1/2, 1/2), each = 12),
    Ns = case_when(
      n.ratio == 1 & small_n == 'N=20' ~ '20, 20, 20, 20',
      n.ratio == 1 & small_n == 'N=50' ~ '50, 50, 50, 50',
      n.ratio == 1 & small_n == 'N=100' ~ '100, 100, 100, 100',
      n.ratio == 1.5 & small_n == 'N=20' ~ '30, 20, 20, 20',
      n.ratio == 1.5 & small_n == 'N=50' ~ '75, 50, 50, 50',
      n.ratio == 1.5 & small_n == 'N=100' ~ '150, 100, 100, 100',
      n.ratio == 1/1.5 & small_n == 'N=20' ~ '20, 30, 30, 30',
      n.ratio == 1/1.5 & small_n == 'N=50' ~ '50, 75, 75, 75',
      n.ratio == 1/1.5 & small_n == 'N=100' ~ '100, 150, 150, 150',
      n.ratio == 2 & small_n == 'N=20' ~ '40, 20, 20, 20',
      n.ratio == 2 & small_n == 'N=50' ~ '100, 50, 50, 50',
      n.ratio == 2 & small_n == 'N=100' ~ '200, 100, 100, 100',
      n.ratio == 1/2 & small_n == 'N=20' ~ '20, 40, 40, 40',
      n.ratio == 1/2 & small_n == 'N=50' ~ '50, 100, 100, 100',
      n.ratio == 1/2 & small_n == 'N=100' ~ '100, 200, 200, 200'
    ) %>% 
      factor(levels = c('20, 20, 20, 20',
                        '30, 20, 20, 20',
                        '40, 20, 20, 20',
                        '20, 30, 30, 30',
                        '20, 40, 40, 40',
                        '50, 50, 50, 50',
                        '75, 50, 50, 50',
                        '100, 50, 50, 50',
                        '50, 75, 75, 75',
                        '50, 100, 100, 100',
                        '100, 100, 100, 100',
                        '150, 100, 100, 100',
                        '200, 100, 100, 100',
                        '100, 150, 150, 150',
                        '100, 200, 200, 200')),
    case = 'Crossover interaction',
    small_ns = str_extract(Ns, '[0-9]+'),
    test = recode(test, classic = "Student's t test", welch = "Welch's t test")
  )



@

\begin{figure}[!ht]
<<CoveragePlots, echo = FALSE, fig = TRUE, width = 8, height = 11>>=

# Coverage plot (two groups) ----
cov_two <- ggplot(filter(two_groups, effect == 'd=0', small_ns == 50), 
       aes(x = as.factor(var.ratio), y = rejects, ymin=.8, ymax=1, group = Ns, shape = Ns, linetype = Ns)) + 
    geom_point(size = 4) + 
    scale_shape_manual(name = 'Sample sizes \nGroup 1, 2',
                       labels = c('50, 50', '50, 75', '50, 100'),
                       values = rep(c(15, 16, 17), times = 3)) +
    geom_line(size = .5) + 
    scale_linetype_manual(name = 'Sample sizes \nGroup 1, 2',
                          labels = c('50, 50', '50, 75', '50, 100'),
                          values=rep(c('solid'), each = 3)
                          ) +
    scale_x_discrete(labels = factor(c('1/5', '1/2', '1', '2', '5'))) +
    scale_y_continuous(breaks = seq(0, 1, .05), minor_breaks = seq(0, 1, .01)) +
    labs(
      x = bquote(frac(sigma[1]^2, sigma[2]^2)),
      y = 'Coverage Rate',
      title = 'Two groups'
      ) +
    facet_grid(. ~ test) +
    theme(
      panel.grid = element_blank(),
#      panel.grid.minor.y = element_line(color = 'gray90'),
      panel.grid.major.y = element_line(color = 'gray80')
      )


# Coverage plot (interaction, one group has small n) ----
cov_int_smalln <- ggplot(filter(cross_int, effect == 'd=0', !(n.ratio %in% c(1.5, 2)), small_n == 'N=50'), 
       aes(x = as.factor(var.ratio), y = rejects, ymin=.8, ymax=1, group = Ns, shape = Ns, linetype = Ns)) + 
    geom_point(size = 4) + 
    scale_shape_manual(name = 'Sample sizes \nGroup 1, 2, 3, 4',
                       labels = c(
                                  '50, 50, 50, 50',
                                  '50, 75, 75, 75',
                                  '50, 100, 100, 100'
                                  ),
                       values = rep(c(15, 16, 17), times = 3)
                       ) +
    geom_line(size = .5) + 
    scale_linetype_manual(name = 'Sample sizes \nGroup 1, 2, 3, 4',
                          labels = c(
                                  '50, 50, 50, 50',
                                  '50, 75, 75, 75',
                                  '50, 100, 100, 100'
                                  ),
                          values = rep(c('solid'), each = 3)
                          ) +
    scale_x_discrete(labels = factor(c('1/5', '1/2', '1', '2', '5'))) +
    scale_y_continuous(breaks = seq(.8, 1, .05), minor_breaks = seq(.8, 1, .01)) +
    labs(
      x = bquote(frac(sigma[1]^2,{sigma[2]^2 == sigma[3]^2} == sigma[4]^2)),
      y = 'Coverage Rate',
      title = 'Crossover interaction - Group 1 has small sample'
      ) +
    facet_grid(. ~ test) +
    theme(
      panel.grid = element_blank(),
#      panel.grid.minor.y = element_line(color = 'gray90'),
      panel.grid.major.y = element_line(color = 'gray80')
      )

# Coverage plot (interaction, one group has large n) ----
cov_int_bign <- ggplot(filter(cross_int, effect == 'd=0', !(n.ratio %in% c(1/1.5, 1/2)), small_n == 'N=50'), 
       aes(x = as.factor(var.ratio), y = rejects, ymin=.8, ymax=1, group = Ns, shape = Ns, linetype = Ns)) + 
    geom_point(size = 4) + 
    scale_shape_manual(name = 'Sample sizes \nGroup 1, 2, 3, 4',
                       labels = c(
                                  '50, 50, 50, 50',
                                  '75, 50, 50, 50',
                                  '100, 50, 50, 50'
                                  ),
                       values = rep(c(15, 16, 17), times = 3)
                       ) +
    geom_line(size = .5) + 
    scale_linetype_manual(name = 'Sample sizes \nGroup 1, 2, 3, 4',
                          labels = c(
                                  '50, 50, 50, 50',
                                  '75, 50, 50, 50',
                                  '100, 50, 50, 50'
                                  ),
                          values = rep(c('solid'), each = 3)
                          ) +
    scale_x_discrete(labels = factor(c('1/5', '1/2', '1', '2', '5'))) +
    scale_y_continuous(breaks = seq(.8, 1, .05), minor_breaks = seq(.8, 1, .01)) +
    labs(
      x = bquote(frac(sigma[1]^2,{sigma[2]^2 == sigma[3]^2} == sigma[4]^2)),
      y = 'Coverage Rate',
      title = 'Crossover interaction - Group 1 has large sample'
      ) +
    facet_grid(. ~ test) +
    theme(
      panel.grid = element_blank(),
#      panel.grid.minor.y = element_line(color = 'gray90'),
      panel.grid.major.y = element_line(color = 'gray80')
      )


# Print false pos plots ----
plot_coverage <- plot_grid(cov_two, cov_int_smalln, cov_int_bign, nrow = 3)
plot_coverage

@

\textit{Figure 6.} Coverage probabilities for Student's and Welch's t tests.
\end{figure}




\section{Discussion}
    We set out to find a simple rule to help researchers decide when to use 
Student's t or Welch's t test. We believe the simplest 
rule is to always use Welch's t test to compare the means of independent groups, whether there are two groups or more.
    
    The simulations demonstrated that when the population 
variances or sample sizes were equal, using Welch's t test instead of Student's 
didn't hurt. Figure 1 shows the Welch's t test df could 
drop below 70\% of Student's when only the variances 
ratio or only the sample sizes were unequal, but the sumulated false positive rates, 
power, and coverage probabilities of the two tests were almost identical under 
these conditions. The difference in the df had a 
negligible effect on the outcomes. 
    
    More important was the standard error, which 
affects the t-value itself. When either the variances or sample sizes are 
equal, the pooled standard error of Student's t test and the separate variances 
standard error of Welch's t test are identical, and the two tests will 
generally agree with each other. However, when both the variances and the 
sample sizes are unequal, the pooled standard error of Student's t test gives 
more weight to the larger group \cite<e.g.,>{Coombs1996,Zimmerman2009}; if that group has the larger 
variance, Student's t test becomes more conservative, but if it
has the smaller variance, Student's t test becomes more liberal. In 
contrast, Welch's t test was more stable, regardless of which group had the 
larger variance. This is why the false positive 
rate and coverage probability varied widely for Student's t test.
    
    The biggest benefit of Student's t test was that it had more power when the larger sample had the smaller 
variance---yet under these same conditions, it had an inflated false positive
rate and the lowest coverage probability. Far from being underpowered, Welch's 
t test was more powerful when the larger sample had the 
larger variance, but it retained the expected false positive
rate and coverage probability. Overall, Welch's t test did a better job 
of balancing concerns with false positives, power, and estimation.
    
    We believe researchers prefer simple decision rules, 
and so we echo others' recommendations to always use 
Welch's t test \cite{Zimmerman1996,Moser1992,Moser1989}. For researchers who 
insist on using Student's t test, there is some 
good news. If they run experiments, subjects are usually assigned evenly to 
conditions, and when sample sizes are equal the two tests perform equally well. 
However, if their research compares pre-existing groups, it 
might not be possible to have equal sample sizes. Welch's t test is likely to 
outperform Student's unless the variances are 
equal. One could use boxplots to 
determine whether it is reasonable to assume equal variances, which will be easier if the sample sizes are large. 
Nevertheless, researchers should rest assured that they won't suffer from choosing 
Welch's t test as the default.
    
    Prior simulations that compared Student's and Welch's t tests 
focused on null hypothesis significance testing by emphasizing false positives
and power \cite{Boneau1960, Neuhauser2002, Zimmerman1993, Zimmerman2004, 
Zimmerman1996, Zimmerman2009}, but there are important implications of this work
for effect size estimation. Some have called for researchers to report 
effect sizes and confidence intervals to address limitations of 
merely reporting significance tests. It is important to remember 
that effect size estimation also involves assumptions about group variances.
We found that using Student's t test to find the confidence
interval for the difference in group means, which assumes equal variances, led to 
 unstable estimates. Under some conditions, the confidence intervals were 
less accurate than expected because they were too narrow, and under other 
conditions, they were more accurate than expected because they were too wide. 
Using Welch's t test, which does not assume equal variances, led 
to more stable estimates across conditions.
    
    Cohen's d \cite{Cohen1992} is the most popular effect size for reporting 
the difference in two group means, but it assumes equal variances. Cohen's d standardizes the difference in means based on a 
common standard deviation of the population. This common standard deviation
is just the square root of the pooled variance from Student's t test.
But when the group variances are unequal, there is no common standard deviation. 
Cohen's d will suffer from the same problems as Student's t test. Given the same difference in group 
means, if the sample sizes and variances both differ, then Cohen's d will give more 
weight to the larger sample when pooling the variance. If the larger sample has the larger variance, the 
standardized effect size will be smaller than if the larger sample 
has the smaller variance. In reality, either estimate is misleading
because there is no common standard deviation, so there can be no traditional Cohen's d.
    
    Standardized effect sizes such as Cohen's d are often desirable for their 
use in meta-analysis. But the equal variances 
problem applies to meta-analysis as well. Using Cohen's d as the basis for a 
meta-analysis involves an assumption that the group variances across 
the body of research are equal, an assumption which might be untenable. Differences in variances might not be just a nuisance, but rather an interesting part of the effect for meta-analysts to examine. The effect of the independent variable may be on the variances and not merely the means.
    
    The good news is that raw difference in means are also effect sizes
\cite{Cumming2014, Kelley2012}, and you can find confidence intervals around raw differences 
in means without assuming equal variances. In fact, 95\% confidence intervals
based on Welch's t test appear in the 
default output of programs such as SPSS and R. Reporting 
descriptive statistics in their original scale might be a better practice than 
reporting only standardized effect sizes anyway. First, unlike Cohen's d, reporting raw descriptive statistics does not require the researcher to commit to an 
equal variances assumption. Second, it provides all of the necessary 
information for others who want to assume equal variances to find Cohen's d or its alternatives \cite{Peng2013, 
Grissom2001}. Third, it allows other researchers to examine whether differences 
in group variances are a consistent part of an effect, which would be lost by 
just reporting the standardized difference.

    We suspect that most statistics courses in psychology thoroughly teach Student's t 
test and only briefly touch on Welch's t test, if they teach it at 
all. Indeed, we have heard colleagues complain that when they use Welch's t test in a 
manuscript, reviewers are suspicious of the 
degrees of freedom with decimals. These reviewers must not have learned that 
degrees of freedom with decimals are the norm for Welch's t test and related methods such as corrections used for assumptions in repeated measures ANOVA \cite{Gonzalez2008}. The emphasis 
on Student's t test in teaching is consistent with the strategy of assuming equal variances and only using Welch's t test if it appears the assumption has been violated. But why should we spend so much 
time on the equal variances assumption in the first place? Why not teach 
Welch's t test at the outset without imposing 
restrictive assumptions? Student's t test could be taught briefly so  
students understand the existing literature, but we believe it would be 
beneficial to emphasize Welch's t test as the default approach. As demonstrated 
in our simulations, this approach will lead to better decision-making when 
it comes to analyzing data. 

Our discussion began with the relatively simple case of testing the means from two independent groups with normally distributed data.
But we expanded this discussion to demonstrate that the implications generalize to more complex designs, such as interactions in a 2 x 2 factorial design.
The simulations show the pattern is not straightforward and depends on combinations of which group has the larger sample size and which group has the larger variance.  For between-groups contrasts, we arrived at a simple conclusion: on balance, when considering the false positive rate, power, and effect size estimation, an efficient strategy is to always uses the Welch t test. As the study design becomes more complicated, such as with repeated measures, random effect models, or non-normally distributed data, the story will likely be more complicated. As a field we should move toward analysis strategies that have fewer moving parts so they can be described easily in a scientific report or preregistration plan, and they provides robust estimates.



\bibliography{bibliography}
\bibliographystyle{apacite}

\end{document}